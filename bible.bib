
@inproceedings{van_den_bogaert_binaural_2007,
	title = {Binaural cue preservation for hearing aids using an interaural transfer function multichannel {Wiener} filter},
	volume = {4},
	booktitle = {Acoustics, {Speech} and {Signal} {Processing}, 2007. {ICASSP} 2007. {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Van den Bogaert, Tim and Wouters, Jan and Doclo, Simon and Moonen, Marc},
	year = {2007},
	pages = {IV--565},
	file = {bogaert2007.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\J293ER25\\bogaert2007.pdf:application/pdf},
}

@article{van_den_bogaert_effect_2008,
	title = {The effect of multimicrophone noise reduction systems on sound source localization by users of binaural hearing aids},
	volume = {124},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2931962},
	doi = {10.1121/1.2931962},
	language = {en},
	number = {1},
	urldate = {2017-10-26},
	journal = {The Journal of the Acoustical Society of America},
	author = {Van den Bogaert, Tim and Doclo, Simon and Wouters, Jan and Moonen, Marc},
	month = jul,
	year = {2008},
	pages = {484--497},
	file = {bogaert2008.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\53FN3B6A\\bogaert2008.pdf:application/pdf},
}

@article{van_den_bogaert_speech_2009,
	title = {Speech enhancement with multichannel {Wiener} filter techniques in multimicrophone binaural hearing aids},
	volume = {125},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3023069},
	doi = {10.1121/1.3023069},
	language = {en},
	number = {1},
	urldate = {2017-10-26},
	journal = {The Journal of the Acoustical Society of America},
	author = {Van den Bogaert, Tim and Doclo, Simon and Wouters, Jan and Moonen, Marc},
	month = jan,
	year = {2009},
	pages = {360--371},
	file = {bogaert2009.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\P6REBLJA\\bogaert2009.pdf:application/pdf},
}

@article{cheung_realignment_2009,
	title = {Realignment of {Interaural} {Cortical} {Maps} in {Asymmetric} {Hearing} {Loss}},
	volume = {29},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.6072-08.2009},
	doi = {10.1523/JNEUROSCI.6072-08.2009},
	language = {en},
	number = {21},
	urldate = {2017-10-26},
	journal = {Journal of Neuroscience},
	author = {Cheung, S. W. and Bonham, B. H. and Schreiner, C. E. and Godey, B. and Copenhaver, D. A.},
	month = may,
	year = {2009},
	pages = {7065--7078},
	file = {cheung2009.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TW2QRQIE\\cheung2009.pdf:application/pdf},
}

@article{ching_binaural_2004,
	title = {Binaural {Benefits} for {Adults} {Who} {Use} {Hearing} {Aids} and {Cochlear} {Implants} in {Opposite} {Ears}:},
	volume = {25},
	issn = {0196-0202},
	shorttitle = {Binaural {Benefits} for {Adults} {Who} {Use} {Hearing} {Aids} and {Cochlear} {Implants} in {Opposite} {Ears}},
	url = {http://content.wkhealth.com/linkback/openurl?sid=WKPTLP:landingpage&an=00003446-200402000-00002},
	doi = {10.1097/01.AUD.0000111261.84611.C8},
	language = {en},
	number = {1},
	urldate = {2017-10-26},
	journal = {Ear and Hearing},
	author = {Ching, Teresa Y. C. and Incerti, Paula and Hill, Mandy},
	month = feb,
	year = {2004},
	pages = {9--21},
	file = {ching2004.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UK5CG7AB\\ching2004.pdf:application/pdf},
}

@article{day_benefit_1988,
	title = {Benefit from binaural hearing aids in individuals with a severe hearing impairment},
	volume = {22},
	issn = {0300-5364},
	url = {http://www.tandfonline.com/doi/full/10.3109/03005368809076464},
	doi = {10.3109/03005368809076464},
	language = {en},
	number = {4},
	urldate = {2017-10-26},
	journal = {British Journal of Audiology},
	author = {Day, Graham A. and Browning, George G. and Gatehouse, Stuart},
	month = jan,
	year = {1988},
	pages = {273--277},
	file = {day1988.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GMPZJCYV\\day1988.pdf:application/pdf},
}

@article{desloge_microphone-array_1997,
	title = {Microphone-array hearing aids with binaural output. {I}. {Fixed}-processing systems},
	volume = {5},
	abstract = {This work is aimed at developing a design for the use of a microphone array with binaural hearing aids. The goal of such a hearing aid is to provide both the spatial-filtering benefits of the array and the natural benefits to sound localization and speech intelligibility that accrue from binaural listening. The present study examines two types of designs for fixed-processing systems: one in which independent arrays provide outputs to the two ears, and another in which the binaural outputs are derived from a single array. For the latter, various methods are used to merge array processing with binaural listening. In one approach, filters are designed to satisfy a frequency-dependent trade between directionality and binaural cue fidelity. In another, the microphone signals are filtered into low- and high-frequency components with the lowpass signals providing binaural cues and the highpass signal being the single output of the array processor. Acoustic and behavioral measurements were made in an anechoic chamber and in a moderately reverberant room to evaluate example systems. Theoretical performance was calculated for model arrays mounted on an idealized spherical head. Results show that both single- and dual-array systems provided target- intelligibility enhancements (2–4 dB improvements in speech reception threshold) relative to binaural cardioid microphones. In addition, the binaural-output systems provided cues that assist in sound localization, with resulting performance depending directly upon the cue fidelity. Finally, the sphere-based calculations accurately reflected the major features of the actual head-mounted array results, both in terms of directional sensitivity and output binaural cues.},
	number = {6},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Desloge, Joseph G. and Rabinowitz, William M. and Zurek, Patrick M.},
	year = {1997},
	pages = {529--542},
	file = {desloge1997.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XVHCBD62\\desloge1997.pdf:application/pdf},
}

@article{godey_functional_2005,
	title = {Functional {Organization} of {Squirrel} {Monkey} {Primary} {Auditory} {Cortex}: {Responses} to {Frequency}-{Modulation} {Sweeps}},
	volume = {94},
	issn = {0022-3077, 1522-1598},
	shorttitle = {Functional {Organization} of {Squirrel} {Monkey} {Primary} {Auditory} {Cortex}},
	url = {http://jn.physiology.org/cgi/doi/10.1152/jn.00950.2004},
	doi = {10.1152/jn.00950.2004},
	language = {en},
	number = {2},
	urldate = {2017-10-26},
	journal = {Journal of Neurophysiology},
	author = {Godey, B.},
	month = may,
	year = {2005},
	pages = {1299--1311},
	file = {godey2005.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PARMYDFE\\godey2005.pdf:application/pdf},
}

@article{hawkins_signal--noise_1984,
	title = {Signal-to-noise ratio advantage of binaural hearing aids and directional microphones under different levels of reverberation},
	volume = {49},
	number = {3},
	journal = {Journal of Speech and Hearing Disorders},
	author = {Hawkins, David B. and Yacullo, William S.},
	year = {1984},
	pages = {278--286},
	file = {hawkins1984.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\K9WKMMBJ\\hawkins1984.pdf:application/pdf},
}

@article{klasen_binaural_2007,
	title = {Binaural {Noise} {Reduction} {Algorithms} for {Hearing} {Aids} {That} {Preserve} {Interaural} {Time} {Delay} {Cues}},
	volume = {55},
	issn = {1053-587X},
	url = {http://ieeexplore.ieee.org/document/4133039/},
	doi = {10.1109/TSP.2006.888897},
	number = {4},
	urldate = {2017-10-26},
	journal = {IEEE Transactions on Signal Processing},
	author = {Klasen, Thomas J. and Van den Bogaert, Tim and Moonen, Marc and Wouters, Jan},
	month = apr,
	year = {2007},
	pages = {1579--1585},
	file = {klasen2007.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\C3T4ZHNB\\klasen2007.pdf:application/pdf},
}

@article{mosnier_speech_2009,
	title = {Speech {Performance} and {Sound} {Localization} in a {Complex} {Noisy} {Environment} in {Bilaterally} {Implanted} {Adult} {Patients}},
	volume = {14},
	issn = {1421-9700, 1420-3030},
	url = {https://www.karger.com/Article/FullText/159121},
	doi = {10.1159/000159121},
	language = {en},
	number = {2},
	urldate = {2017-10-26},
	journal = {Audiology and Neurotology},
	author = {Mosnier, Isabelle and Sterkers, Olivier and Bebear, Jean-Pierre and Godey, Benoit and Robier, Alain and Deguine, Olivier and Fraysse, Bernard and Bordure, Philippe and Mondain, Michel and Bouccara, Didier and Bozorg-Grayeli, Alexis and Borel, St\&eacute;phanie and Ambert-Dahan, Emmanu\&egrave;le and Ferrary, Evelyne},
	year = {2009},
	pages = {106--114},
	file = {mosnier2009.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CZQU2SAN\\mosnier2009.pdf:application/pdf},
}

@article{nabelek_monaural_1974,
	title = {Monaural and binaural speech perception through hearing aids under noise and reverberation with normal and hearing-impaired listeners},
	volume = {17},
	number = {4},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Nabelek, Anna K. and Pickett, James M.},
	year = {1974},
	pages = {724--739},
	file = {nabelek1974.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2WQKE24C\\nabelek1974.pdf:application/pdf},
}

@article{plomp_auditory_1978,
	title = {Auditory handicap of hearing impairment and the limited benefit of hearing aids},
	volume = {63},
	number = {2},
	journal = {The Journal of the Acoustical Society of America},
	author = {Plomp, Reinier},
	year = {1978},
	pages = {533--549},
	file = {plomp1978.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7L3LDLAM\\plomp1978.pdf:application/pdf},
}

@article{welker_microphone-array_1997,
	title = {Microphone-array hearing aids with binaural output. {II}. {A} two-microphone adaptive system},
	volume = {5},
	number = {6},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Welker, Daniel P. and Greenberg, Julie E. and Desloge, Joseph G. and Zurek, Patrick M.},
	year = {1997},
	pages = {543--551},
	file = {welker1997.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YEHY6Z8G\\welker1997.pdf:application/pdf},
}

@article{wittkop_strategy-selective_2003,
	title = {Strategy-selective noise reduction for binaural digital hearing aids},
	volume = {39},
	number = {1},
	journal = {Speech Communication},
	author = {Wittkop, Thomas and Hohmann, Volker},
	year = {2003},
	pages = {111--138},
	file = {wittkop2003.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XEVSEGJD\\wittkop2003.pdf:application/pdf},
}

@article{lopez-enrique_binaural_2016,
	title = {A {Binaural} {Cochlear} {Implant} {Sound} {Coding} {Strategy} {Inspired} by the {Contralateral} {Medial} {Olivocochlear} {Reflex}},
	volume = {37},
	abstract = {Objectives: In natural hearing, cochlear mechanical compression is dynamically adjusted via the efferent medial olivocochlear re ex (MOCR). These adjustments probably help understanding speech in noisy envi- ronments and are not available to the users of current cochlear implants (CIs). The aims of the present study are to: (1) present a binaural CI sound processing strategy inspired by the control of cochlear com- pression provided by the contralateral MOCR in natural hearing; and (2) assess the bene ts of the new strategy for understanding speech presented in competition with steady noise with a speech-like spectrum in various spatial con gurations of the speech and noise sources.

Design: Pairs of CI sound processors (one per ear) were constructed to mimic or not mimic the effects of the contralateral MOCR on compres- sion. For the nonmimicking condition (standard strategy or STD), the two processors in a pair functioned similarly to standard clinical processors (i.e., with  xed back-end compression and independently of each other). When con gured to mimic the effects of the MOCR (MOC strategy), the two processors communicated with each other and the amount of back- end compression in a given frequency channel of each processor in the pair decreased/increased dynamically (so that output levels dropped/ increased) with increases/decreases in the output energy from the corre- sponding frequency channel in the contralateral processor. Speech recep- tion thresholds in speech-shaped noise were measured for 3 bilateral CI users and 2 single-sided deaf unilateral CI users. Thresholds were com- pared for the STD and MOC strategies in unilateral and bilateral listening conditions and for three spatial con gurations of the speech and noise sources in simulated free- eld conditions: speech and noise sources colo- cated in front of the listener, speech on the left ear with noise in front of the listener, and speech on the left ear with noise on the right ear. In both bilateral and unilateral listening, the electrical stimulus delivered to the test ear(s) was always calculated as if the listeners were wearing bilateral processors.

Results: In both unilateral and bilateral listening conditions, mean speech reception thresholds were comparable with the two strategies for colocated speech and noise sources, but were at least 2 dB lower (better) with the MOC than with the STD strategy for spatially separated speech and noise sources. In unilateral listening conditions, mean thresholds improved with increasing the spatial separation between the speech and noise sources regardless of the strategy but the improvement was sig- ni cantly greater with the MOC strategy. In bilateral listening conditions,
1Instituto de Neurociencias de Castilla y León, 2Instituto de Investigación Biomédica de Salamanca, 3Departamento de Cirugía, Facultad de Medicina, Universidad de Salamanca, Salamanca, Spain; 4USA Laboratory, MED-EL GmbH, Innsbruck, Austria; 5Department of Electrical and Computer Engineering, Duke University, Durham, North Carolina, USA; 6Institute of Mechatronics, University of Innsbruck, Innsbruck, Austria; 7Department of Surgery, Duke University Medical Center, Durham, North Carolina, USA; 8Department of Biomedical Engineering, Duke University, Durham, North Carolina, USA.
This is an open-access article distributed under the terms of the Creative Commons Attribution-Non Commercial-No Derivatives License 4.0 (CCBY- NC-ND), where it is permissible to download and share the work provided it is properly cited. The work cannot be changed in any way or used commercially.
thresholds improved signi cantly with increasing the speech-noise spa- tial separation only with the MOC strategy.

Conclusions: The MOC strategy (1) signi cantly improved the intelligi- bility of speech presented in competition with a spatially separated noise source, both in unilateral and bilateral listening conditions; (2) produced signi cant spatial release from masking in bilateral listening conditions, something that did not occur with  xed compression; and (3) enhanced spatial release from masking in unilateral listening conditions. The MOC strategy as implemented here, or a modi ed version of it, may be use- fully applied in CIs and in hearing aids.},
	number = {3},
	journal = {Ear and Hearing},
	author = {Lopez-Enrique, Enrique A. and Eustaquio-Martín, Almudena and Stohl, Joshua S. and Wolford, Robert D. and Schatzer, Reinhold and Wilson, Blake S.},
	month = jun,
	year = {2016},
	pages = {138--48},
	file = {Lopez-Poveda2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9E5IYYKJ\\Lopez-Poveda2016.pdf:application/pdf},
}

@article{balfour_comparison_1992,
	title = {A {Comparison} of {Sound} {Quality} {Judgments} for {Monaural} and {Binaural} {Hearing} {Aid} {Processed} {Stimuli}},
	volume = {13},
	abstract = {Fifteen adults with bilaterally symmetrical mild and/or moderate sensorineural hearing loss completed a paired-comparison task designed to elicit sound quality preference judgments for monaural/binaural hearing aid processed signals. Three stimuli (speech-in-quiet, speech-in-noise, and music) were recorded separately in three listening environments (audiometric test booth, living room, and a music/lecture hall) through hearing aids placed on a Knowles Electronics Manikin for Acoustics Research. Judgments were made on eight separate sound quality dimensions (brightness, clarity, fullness, loudness, nearness, overall impression, smoothness, and spaciousness) for each of the three stimuli in three listening environments. Results revealed a distinct binaural preference for all eight sound quality dimensions independent of listening environment. Binaural preferences were strongest for overall impression, fullness, and spaciousness. Stimulus type effect was significant only for fullness and spaciousness, where binaural preferences were strongest for speech-in-quiet. After binaural preference data were obtained, subjects ranked each sound quality dimension with respect to its importance for binaural listening relative to monaural. Clarity was ranked highest in importance and brightness was ranked least important. The key to demonstration of improved binaural hearing aid sound quality may be the use of a paired-comparison format.},
	number = {5},
	journal = {Ear and Hearing},
	author = {Balfour, Patricia B. and Hawkins, David B.},
	month = oct,
	year = {1992},
	pages = {331--339},
	file = {balfour1992.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XS2AHQB6\\balfour1992.pdf:application/pdf},
}

@article{kollmeier_real-time_1993,
	title = {Real-time multiband dynamic compression and noise reduction for binaural hearing aids},
	volume = {30},
	abstract = {A multi-signal-processor set-up is introduced that is used for real-time implementation of digital hearing aid algorithms that operate on stereophonic (i.e., binaural) input signals and perform signal processing in the frequency domain. A multiband dynamic compression algorithm was implemented which operates in 24 critical band filter channels, allows for interaction between frequency bands and stereo channels, and is fitted to the hearing of the individual patient by a loudness scaling method. In addition, a binaural noise reduction algorithm was implemented that amplifies sound emanating from the front and suppresses lateral noise sources as well as reverberation. These algorithms were optimized with respect to their processing parameters and by minimizing the processing artifacts. Different versions of the algorithms were tested in six listeners with sensorineural hearing impairment using both subjective quality assessment methods and speech intelligibility measurements in different acoustical situations. For most subjects, linear frequency shaping was subjectively assessed to be negative, although it improved speech intelligibility in noise. Additional compression was assessed to be positive and did not deteriorate speech intelligibility as long as the processing parameters were fitted carefully. All noise reduction strategies employed here were subjectively assessed to be positive. Although the suppression of reverberation only slightly improved speech intelligibility, a combination of directional filtering and dereverberation provided a substantial improvement in speech intelligibility for most subjects and for a certain range of signal-to-noise ratios. The real-time implementation was very helpful in optimizing and testing the algorithms, and the overall results indicate that carefully designed and fitted binaural hearing aids might be very beneficial for a large number of patients. 

Real-time multiband dynamic compression and noise reduction for binaural hearing aids.},
	number = {1},
	journal = {Journal of Rehabilitation Research and Development},
	author = {Kollmeier, Birger and Peissig, Jürgen and Hohmann, Volker},
	year = {1993},
	pages = {82--94},
	file = {kollmeier1993.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QCNMHG4W\\kollmeier1993.pdf:application/pdf},
}

@article{silman_late-onset_1984,
	title = {Late-onset auditory deprivation: {Effects} of monaural versus binaural hearing aids},
	volume = {76},
	abstract = {Performance on tests of pure-tone thresholds, speech-recognition thresholds, and speech-recognition scores for the two ears of each subject were evaluated in two groups of adults with bilateral hearing losses. One group was composed of individuals fitted with binaural hearing aids, and the other group included persons with monaural hearing aids. Performance prior to the use of hearing aids was compared to performance after 4-5 years of hearing aid use in order to determine whether the unaided ear would show effects of auditory deprivation. There were no differences over time for pure-tone thresholds or speech-recognition thresholds for both ears of both groups. Nevertheless, the results revealed that the speech-recognition difference scores of the binaurally fitted subjects remained stable over time whereas they increased for the monaurally fitted subjects. The findings reveal an auditory deprivation effect for the unfitted ears of the subjects with monaural hearing aids.},
	number = {5},
	journal = {The Journal of Acoustical Society of America},
	author = {Silman, Shlomo and Gelfand, Stanley A. and Silverman, Carol Ann},
	month = nov,
	year = {1984},
	pages = {1357--1362},
	file = {silman1984.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\DUFVM4N6\\silman1984.pdf:application/pdf},
}

@phdthesis{kamkar-parsi_signal_2009,
	title = {Signal estimators and noise reduction schemes for high-end binaural hearing aids},
	school = {University of Ottawa (Canada)},
	author = {Kamkar-Parsi, A. Homayoun},
	year = {2009},
	file = {kamkarparsi2005_phd.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QBQD46QV\\kamkarparsi2005_phd.pdf:application/pdf},
}

@article{kamkar-parsi_improved_2009,
	title = {Improved {Noise} {Power} {Spectrum} {Density} {Estimation} for {Binaural} {Hearing} {Aids} {Operating} in a {Diffuse} {Noise} {Field} {Environment}},
	volume = {17},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/4802171/},
	doi = {10.1109/TASL.2008.2009017},
	number = {4},
	urldate = {2017-10-26},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Kamkar-Parsi, A.H. and Bouchard, M.},
	month = may,
	year = {2009},
	pages = {521--533},
	file = {kamkarparsi2009.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MQMUR92V\\kamkarparsi2009.pdf:application/pdf},
}

@article{kamkar-parsi_instantaneous_2011,
	title = {Instantaneous {Binaural} {Target} {PSD} {Estimation} for {Hearing} {Aid} {Noise} {Reduction} in {Complex} {Acoustic} {Environments}},
	volume = {60},
	issn = {0018-9456, 1557-9662},
	url = {http://ieeexplore.ieee.org/document/5613936/},
	doi = {10.1109/TIM.2010.2084690},
	number = {4},
	urldate = {2017-10-26},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Kamkar-Parsi, A. Homayoun and Bouchard, Martin},
	month = apr,
	year = {2011},
	pages = {1141--1154},
	file = {kamkarparsi2011.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\A8JZXXW8\\kamkarparsi2011.pdf:application/pdf},
}

@article{lockwood_performance_2004,
	title = {Performance of time- and frequency-domain binaural beamformers based on recorded signals from real rooms},
	volume = {115},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1624064},
	doi = {10.1121/1.1624064},
	language = {en},
	number = {1},
	urldate = {2017-10-26},
	journal = {The Journal of the Acoustical Society of America},
	author = {Lockwood, Michael E. and Jones, Douglas L. and Bilger, Robert C. and Lansing, Charissa R. and O’Brien, William D. and Wheeler, Bruce C. and Feng, Albert S.},
	month = jan,
	year = {2004},
	pages = {379--391},
	file = {lockwood2004.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XKGTPEDG\\lockwood2004.pdf:application/pdf},
}

@article{picou_potential_2014,
	title = {Potential benefits and limitations of three types of directional processing in hearing aids},
	volume = {35},
	number = {3},
	journal = {Ear and hearing},
	author = {Picou, Erin M. and Aspell, Elizabeth and Ricketts, Todd A.},
	year = {2014},
	pages = {339--352},
	file = {picou2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AG5TFTLR\\picou2014.pdf:application/pdf},
}

@article{kollmeier_development_1997,
	title = {Development and evaluation of a {German} sentence test for objective and subjective speech intelligibility assessment},
	volume = {102},
	number = {4},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kollmeier, Birger and Wesselkamp, Matthias},
	year = {1997},
	pages = {2412--2421},
	file = {kollmeier1997.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KRCWS5V2\\kollmeier1997.pdf:application/pdf},
}

@article{luts_multicenter_2010,
	title = {Multicenter evaluation of signal enhancement algorithms for hearing aids},
	volume = {127},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3299168},
	doi = {10.1121/1.3299168},
	language = {en},
	number = {3},
	urldate = {2017-10-26},
	journal = {The Journal of the Acoustical Society of America},
	author = {Luts, Heleen and Eneman, Koen and Wouters, Jan and Schulte, Michael and Vormann, Matthias and Buechler, Michael and Dillier, Norbert and Houben, Rolph and Dreschler, Wouter A. and Froehlich, Matthias and Puder, Henning and Grimm, Giso and Hohmann, Volker and Leijon, Arne and Lombard, Anthony and Mauler, Dirk and Spriet, Ann},
	month = mar,
	year = {2010},
	pages = {1491--1505},
	file = {Luts et al. - 2010 - Multicenter evaluation of signal enhancement algor.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CQ2QLZXQ\\Luts et al. - 2010 - Multicenter evaluation of signal enhancement algor.pdf:application/pdf;luts2010.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\SSNJZMIY\\luts2010.pdf:application/pdf},
}

@inproceedings{yee_speech_2016,
	title = {A speech enhancement system using binaural hearing aids and an external microphone},
	booktitle = {Acoustics, {Speech} and {Signal} {Processing} ({ICASSP}), 2016 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Yee, Dianna and Kamkar-Parsi, Homayoun and Puder, Henning and Martin, Rainer},
	year = {2016},
	pages = {246--250},
	file = {yee2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RIMI6Z6L\\yee2016.pdf:application/pdf},
}

@article{nilsson_development_1994,
	title = {Development of the {Hearing} {In} {Noise} {Test} for the measurement of speech reception thresholds in quiet and in noise},
	volume = {95},
	abstract = {A large set of sentence materials, chosen for their uniformity in length and representation of natural speech, has been developed for the measurement of sentence speech reception thresholds (sSRTs). The mean‐squared level of each digitally recorded sentence was adjusted to equate intelligibility when presented in spectrally matched noise to normal‐hearing listeners. These materials were cast into 25 phonemically balanced lists of ten sentences for adaptive measurement of sentence sSRTs. The 95\% confidence interval for these measurements is ±2.98 dB for sSRTs in quiet and ±2.41 dB for sSRTs in noise, as defined by the variability of repeated measures with different lists. Average sSRTs in quiet were 23.91 dB(A). Average sSRTs in 72 dB(A) noise were 69.08 dB(A), or −2.92 dB signal/noise ratio. Low‐pass filtering increased sSRTs slightly in quiet and noise as the 4‐ and 8‐kHz octave bands were eliminated. Much larger increases in SRT occurred when the 2‐kHz octave band was eliminated, and bandwidth dropped below 2.5 kHz. Reliability was not degraded substantially until bandwidth dropped below 2.5 kHz. The statistical reliability and efficiency of the test suit it to practical applications in which measures of speech intelligibility are required.},
	number = {2},
	journal = {The Journal of the Acoustical Society of America},
	author = {Nilsson, Michael and Soli, Sigfrid D. and Sullivan, Jean A.},
	month = feb,
	year = {1994},
	pages = {1085--1099},
	file = {nilsson1994.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\C846SVMX\\nilsson1994.pdf:application/pdf},
}

@article{powers_three-microphone_2002,
	title = {Three-microphone instrument is designed to extend benefits of directionality},
	volume = {55},
	number = {10},
	journal = {The Hearing Journal},
	author = {Powers, Thomas A. and Hamacher, Volkmar},
	month = oct,
	year = {2002},
	pages = {38--45},
	file = {powers2002.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NHCB8CHX\\powers2002.pdf:application/pdf},
}

@article{vliegen_influence_2004,
	title = {The influence of duration and level on human sound localization},
	volume = {115},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1687423},
	doi = {10.1121/1.1687423},
	language = {en},
	number = {4},
	urldate = {2017-10-26},
	journal = {The Journal of the Acoustical Society of America},
	author = {Vliegen, Joyce and Van Opstal, A. John},
	month = apr,
	year = {2004},
	pages = {1705--1713},
	file = {2004.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IJT5KY9Q\\2004.pdf:application/pdf;Vliegen et Van Opstal - 2004 - The influence of duration and level on human sound.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JDGBD8DU\\Vliegen et Van Opstal - 2004 - The influence of duration and level on human sound.pdf:application/pdf},
}

@inproceedings{duda_adaptable_1999,
	title = {An adaptable ellipsoidal head model for the interaural time difference},
	volume = {2},
	booktitle = {Acoustics, {Speech}, and {Signal} {Processing}, 1999. {Proceedings}., 1999 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Duda, Richard O. and Avendano, Carlos and Algazi, V. Ralph},
	year = {1999},
	pages = {965--968},
	file = {1999.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2R9HELLA\\1999.pdf:application/pdf},
}

@article{daniel_representation_2000,
	title = {Représentation de champs acoustiques, application à la transmission et à la reproduction de scènes sonores complexes dans un contexte multimédia},
	author = {Daniel, Jérôme},
	year = {2000},
	file = {Daniel2001_PhD.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\22MR3YHJ\\Daniel2001_PhD.pdf:application/pdf},
}

@inproceedings{daniel_spatial_2003,
	title = {Spatial sound encoding including near field effect: {Introducing} distance coding filters and a viable, new ambisonic format},
	shorttitle = {Spatial sound encoding including near field effect},
	booktitle = {Audio {Engineering} {Society} {Conference}: 23rd {International} {Conference}: {Signal} {Processing} in {Audio} {Recording} and {Reproduction}},
	publisher = {Audio Engineering Society},
	author = {Daniel, Jérôme},
	year = {2003},
	file = {Daniel2003.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\P4WNHYT3\\Daniel2003.pdf:application/pdf},
}

@article{bronkhorst_cocktail-party_2015,
	title = {The cocktail-party problem revisited: early processing and selection of multi-talker speech},
	volume = {77},
	shorttitle = {The cocktail-party problem revisited},
	language = {en},
	urldate = {2017-10-27},
	journal = {Atten Percept Psychophys},
	author = {Bronkhorst, Adelbert W.},
	year = {2015},
	keywords = {Cocktail-party problem, Informational masking, Speech perception, Auditory scene analysis, Attention},
	pages = {1465--1487},
	file = {Bronkhorst2015CPeffectAPP.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KNGVNSMD\\Bronkhorst2015CPeffectAPP.pdf:application/pdf},
}

@article{dubno_spectral_2002,
	title = {Spectral contributions to the benefit from spatial separation of speech and noise},
	volume = {45},
	number = {6},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Dubno, Judy R. and Ahlstrom, Jayne B. and Horwitz, Amy R.},
	year = {2002},
	pages = {1297--1310},
	file = {dubno2002.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2FNK88A2\\dubno2002.pdf:application/pdf},
}

@article{cornelis_performance_2011,
	title = {Performance {Analysis} of {Multichannel} {Wiener} {Filter}-{Based} {Noise} {Reduction} in {Hearing} {Aids} {Under} {Second} {Order} {Statistics} {Estimation} {Errors}},
	volume = {19},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5617258/},
	doi = {10.1109/TASL.2010.2090519},
	number = {5},
	urldate = {2017-10-27},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Cornelis, B and Moonen, M and Wouters, J},
	month = jul,
	year = {2011},
	pages = {1368--1381},
	file = {cornelis2011.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HLXDZ6U3\\cornelis2011.pdf:application/pdf;cornelis2011.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CQUWHTN2\\cornelis2011.pdf:application/pdf},
}

@article{bronkhorst_effect_1992,
	title = {Effect of multiple speechlike maskers on binaural speech recognition in normal and impaired hearing},
	volume = {96},
	number = {6},
	journal = {The Journal of the Acoustical Society of America},
	author = {Bronkhorst, A. Homayoun and Plomp, Reinier},
	month = dec,
	year = {1992},
	pages = {3132--3139},
	file = {Bronkhorst1992.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Q85FA8XP\\Bronkhorst1992.pdf:application/pdf},
}

@article{bronkhorst_binaural_1989,
	title = {Binaural speech intelligibility in noise for hearing-impaired listeners},
	volume = {86},
	number = {4},
	journal = {The Journal of the Acoustical Society of America},
	author = {Bronkhorst, A. Homayoun and Plomp, Reinier},
	month = oct,
	year = {1989},
	pages = {1374--1383},
	file = {bronkhorst1989.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NPBPRNDX\\bronkhorst1989.pdf:application/pdf},
}

@article{cherry_experiments_1953,
	title = {Some {Experiments} on the {Recogntition} of {Speech}, with {One} and with {Two} {Ears}},
	volume = {25},
	abstract = {This paper describes a number of objective experiments on recogntition, concerning particularly the relation between the messages received by the two ears. Rather than use steady tones or clicks (frequency or time-point signals- continuous speech is used, and the results interpreted in the main statistically.
Two types of test are reported: (a) the behavior of a listener when presented with two speech signals simultaneously (statistical filtering problem) and (b) behavior when different speech signals are presented to his two ears.},
	number = {5},
	journal = {The Journal of the Acoustical Society of America},
	author = {Cherry, E. Colin},
	month = sep,
	year = {1953},
	keywords = {Cocktail-party problem, Speech perception},
	pages = {975--979},
	file = {Cherry53-cpe.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5EHBAYC3\\Cherry53-cpe.pdf:application/pdf},
}

@article{brungart_effects_2002,
	title = {The effects of spatial separation in distance on the informational and energetic masking of a nearby speech signal},
	volume = {112},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1490592},
	doi = {10.1121/1.1490592},
	language = {en},
	number = {2},
	urldate = {2017-10-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Brungart, Douglas S. and Simpson, Brian D.},
	month = aug,
	year = {2002},
	pages = {664--676},
	file = {Brungart2002.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\57IV348A\\Brungart2002.pdf:application/pdf},
}

@article{kates_using_1992,
	title = {On using coherence to measure distortion in hearing aids},
	volume = {91},
	abstract = {Coherence is a frequency-domain measure of the degree to which the output of a system is linearly related to the system input. The signal-to-distortion ratio (SDR), where the distortion term includes all nonlinear effects and noise in the system, can be computed from the coherence. The coherence estimate, however, is subject to sources of variance and bias and their effects on distortion measurements are presented. New procedures for reducing the variance and bias effects are described, and the processing effectiveness is demonstrated for a simulated hearing-aid response.},
	number = {4},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kates, James M.},
	month = apr,
	year = {1992},
	keywords = {hearing aids, SDR, distortion, coherence},
	pages = {2236--2244},
	file = {kates1992.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\F4ERSG9X\\kates1992.pdf:application/pdf},
}

@article{jutten_separation_2001,
	title = {De la séparation de sourcea l’analyse en composantes indépendantes},
	journal = {Actes de l’école d’été de Printemps, Villard-de-Lans (Isere)},
	author = {Jutten, Chritian and Guérin-Dugué, Alain},
	year = {2001},
	file = {IC2_djafari.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6VLC6UKA\\IC2_djafari.pdf:application/pdf},
}

@inproceedings{nadal_analyse_1997,
	title = {Analyse en composantes indépendantes et séparation de sources: approches basées sur la théorie de l'information et recherche de conditions minimales},
	shorttitle = {Analyse en composantes indépendantes et séparation de sources},
	booktitle = {16° {Colloque} sur le traitement du signal et des images, {FRA}, 1997},
	publisher = {GRETSI, Groupe d’Etudes du Traitement du Signal et des Images},
	author = {Nadal, Jean-Pierre and Parga, Nestor},
	year = {1997},
	file = {001_1181.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UZIFUYDQ\\001_1181.pdf:application/pdf},
}

@article{deshpande_blind_2017,
	title = {Blind localization and segregation of two sources including a binaural head movement model},
	volume = {142},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4986800},
	doi = {10.1121/1.4986800},
	language = {en},
	number = {1},
	urldate = {2017-10-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Deshpande, Nikhil and Braasch, Jonas},
	month = jul,
	year = {2017},
	pages = {EL113--EL117},
	file = {deshpande2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EK9P44GA\\deshpande2017.pdf:application/pdf},
}

@article{ewert_binaural_2017,
	title = {Binaural masking release in symmetric listening conditions with spectro-temporally modulated maskers},
	volume = {142},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4990019},
	doi = {10.1121/1.4990019},
	language = {en},
	number = {1},
	urldate = {2017-10-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Ewert, Stephan D. and Schubotz, Wiebke and Brand, Thomas and Kollmeier, Birger},
	month = jul,
	year = {2017},
	pages = {12--28},
	file = {ewert2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CD3EWKCP\\ewert2017.pdf:application/pdf},
}

@article{josupeit_modeling_2017,
	title = {Modeling speech localization, talker identification, and word recognition in a multi-talker setting},
	volume = {142},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4990375},
	doi = {10.1121/1.4990375},
	language = {en},
	number = {1},
	urldate = {2017-10-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Josupeit, Angela and Hohmann, Volker},
	month = jul,
	year = {2017},
	pages = {35--54},
	file = {josupeit2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\DBLQ5SMB\\josupeit2017.pdf:application/pdf},
}

@article{reinhart_effects_2017,
	title = {Effects of reverberation, background talker number, and compression release time on signal-to-noise ratio},
	volume = {142},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4994683},
	doi = {10.1121/1.4994683},
	language = {en},
	number = {1},
	urldate = {2017-10-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Reinhart, Paul and Zahorik, Pavel and Souza, Pamela E.},
	month = jul,
	year = {2017},
	pages = {EL130--EL135},
	file = {reinhart2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\A3K33CPN\\reinhart2017.pdf:application/pdf},
}

@article{yost_sound_2017,
	title = {Sound source localization identification accuracy: {Envelope} dependencies},
	volume = {142},
	issn = {0001-4966},
	shorttitle = {Sound source localization identification accuracy},
	url = {http://asa.scitation.org/doi/10.1121/1.4990656},
	doi = {10.1121/1.4990656},
	language = {en},
	number = {1},
	urldate = {2017-10-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Yost, William A.},
	month = jul,
	year = {2017},
	pages = {173--185},
	file = {yost2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\SDNIKBAZ\\yost2017.pdf:application/pdf},
}

@article{westermann_binaural_2013,
	title = {Binaural dereverberation based on interaural coherence histograms},
	volume = {133},
	number = {5},
	journal = {The Journal of the Acoustical Society of America},
	author = {Westermann, Adam and Buchholz, Jörg M. and Dau, Torsten},
	year = {2013},
	pages = {2767--2777},
	file = {westermann2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3IEBJTQF\\westermann2013.pdf:application/pdf},
}

@inproceedings{hamacher_comparison_2002,
	title = {Comparison of advanced monaural and binaural noise reduction algorithms for hearing aids},
	volume = {4},
	booktitle = {Acoustics, {Speech}, and {Signal} {Processing} ({ICASSP}), 2002 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Hamacher, Volkmar},
	year = {2002},
	pages = {IV--4008},
	file = {hamacher2002.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JLJJ4MZ6\\hamacher2002.pdf:application/pdf},
}

@article{guerin_two-sensor_2003,
	title = {A two-sensor noise reduction system: applications for hands-free car kit},
	volume = {2003},
	shorttitle = {A two-sensor noise reduction system},
	number = {11},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Guérin, Alexandre and Le Bouquin-Jeannés, Régine and Faucon, Gérard},
	year = {2003},
	pages = {720925},
	file = {guerin2003.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Y5DNLFRB\\guerin2003.pdf:application/pdf},
}

@article{white_cross_1990,
	title = {Cross spectral analysis of nonstationary processes},
	volume = {36},
	number = {4},
	journal = {IEEE Transactions on Information Theory},
	author = {White, Langford B. and Boashash, Boualem},
	year = {1990},
	pages = {830--835},
	file = {white1990.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\8ES9B3V3\\white1990.pdf:application/pdf},
}

@article{yousefian_dual-microphone_2013,
	title = {A {Dual}-{Microphone} {Algorithm} {That} {Can} {Cope} {With} {Competing}-{Talker} {Scenarios}},
	volume = {21},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/6307829/},
	doi = {10.1109/TASL.2012.2215594},
	number = {1},
	urldate = {2017-10-30},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Yousefian, N. and Loizou, P. C.},
	month = jan,
	year = {2013},
	pages = {145--155},
	file = {yousefian2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9SPAMZFI\\yousefian2013.pdf:application/pdf},
}

@article{tan_perception_2008,
	title = {Perception of nonlinear distortion by hearing-impaired people},
	volume = {47},
	issn = {1499-2027, 1708-8186},
	url = {http://www.tandfonline.com/doi/full/10.1080/14992020801945493},
	doi = {10.1080/14992020801945493},
	language = {en},
	number = {5},
	urldate = {2017-10-30},
	journal = {International Journal of Audiology},
	author = {Tan, Chin-Tuan and Moore, Brian C.J.},
	month = jan,
	year = {2008},
	pages = {246--256},
	file = {tan2008.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2NVN2T88\\tan2008.pdf:application/pdf},
}

@article{gomez_improving_2012,
	title = {Improving objective intelligibility prediction by combining correlation and coherence based methods with a measure based on the negative distortion ratio},
	volume = {54},
	issn = {01676393},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167639311001592},
	doi = {10.1016/j.specom.2011.11.001},
	language = {en},
	number = {3},
	urldate = {2017-10-30},
	journal = {Speech Communication},
	author = {Gómez, Angel M. and Schwerin, Belinda and Paliwal, Kuldip},
	month = mar,
	year = {2012},
	pages = {503--515},
	file = {spcom12_gomez.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5V2KMTDW\\spcom12_gomez.pdf:application/pdf},
}

@article{olofsson_objectively_2006,
	title = {Objectively measured and subjectively perceived distortion in nonlinear systems},
	volume = {120},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2372591},
	doi = {10.1121/1.2372591},
	language = {en},
	number = {6},
	urldate = {2017-10-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Olofsson, Åke and Hansen, Martin},
	month = dec,
	year = {2006},
	pages = {3759--3769},
	file = {olofsson2006.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XVDUMRFY\\olofsson2006.pdf:application/pdf},
}

@article{mare_detection_2002,
	title = {Detection of nonlinear distortion in audio signals},
	volume = {48},
	number = {2},
	journal = {IEEE transactions on broadcasting},
	author = {Maré, Stefan},
	year = {2002},
	pages = {76--80},
	file = {mare2002.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\FY5MZPFV\\mare2002.pdf:application/pdf},
}

@article{kates_understanding_2010,
	title = {Understanding compression: {Modeling} the effects of dynamic-range compression in hearing aids},
	volume = {49},
	issn = {1499-2027, 1708-8186},
	shorttitle = {Understanding compression},
	url = {http://www.tandfonline.com/doi/full/10.3109/14992020903426256},
	doi = {10.3109/14992020903426256},
	language = {en},
	number = {6},
	urldate = {2017-10-30},
	journal = {International Journal of Audiology},
	author = {Kates, James M.},
	month = jan,
	year = {2010},
	pages = {395--409},
	file = {Kates - 2010 - Understanding compression Modeling the effects of.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\SVJ8PTIQ\\Kates - 2010 - Understanding compression Modeling the effects of.pdf:application/pdf;Kates - 2010 - Understanding compression Modeling the effects of.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6853Y3P5\\Kates - 2010 - Understanding compression Modeling the effects of.pdf:application/pdf;kates2010.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VSIWXZIS\\kates2010.pdf:application/pdf},
}

@article{kates_coherence_2005,
	title = {Coherence and the speech intelligibility index},
	volume = {117},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1862575},
	doi = {10.1121/1.1862575},
	language = {en},
	number = {4},
	urldate = {2017-10-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kates, James M. and Arehart, Kathryn H.},
	month = apr,
	year = {2005},
	keywords = {distortion},
	pages = {2224--2237},
	file = {Kates et Arehart - 2005 - Coherence and the speech intelligibility index.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PNAJWFQB\\Kates et Arehart - 2005 - Coherence and the speech intelligibility index.pdf:application/pdf;kates2005.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XRGHV76P\\kates2005.pdf:application/pdf},
}

@inproceedings{adams_distortion_2010,
	address = {Cork},
	title = {On the distortion of binaural localization cues using headphones},
	booktitle = {Irish {Signals} and {Systems} {Conference}},
	publisher = {IET},
	author = {Adams, Stephen and Boland, Frank},
	month = jun,
	year = {2010},
	file = {adams2010.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GQH2TM6X\\adams2010.pdf:application/pdf},
}

@article{schnupp_hearing_2009,
	title = {On hearing with more than one ear: lessons from evolution},
	volume = {12},
	issn = {1097-6256, 1546-1726},
	shorttitle = {On hearing with more than one ear},
	url = {http://www.nature.com/doifinder/10.1038/nn.2325},
	doi = {10.1038/nn.2325},
	number = {6},
	urldate = {2017-10-30},
	journal = {Nature Neuroscience},
	author = {Schnupp, Jan W H and Carr, Catherine E},
	month = jun,
	year = {2009},
	pages = {692--697},
	file = {Schnupp2009.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5KHZ7Y45\\Schnupp2009.pdf:application/pdf},
}

@article{langendijk_sound_2001,
	title = {Sound localization in the presence of one or two distracters},
	volume = {109},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1356025},
	doi = {10.1121/1.1356025},
	language = {en},
	number = {5},
	urldate = {2017-10-31},
	journal = {The Journal of the Acoustical Society of America},
	author = {Langendijk, Erno H. A. and Kistler, Doris J. and Wightman, Frederic L.},
	month = may,
	year = {2001},
	pages = {2123--2134},
	file = {2001.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VBCVBMIU\\2001.pdf:application/pdf},
}

@article{langendijk_contribution_2002,
	title = {Contribution of spectral cues to human sound localization},
	volume = {112},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1501901},
	doi = {10.1121/1.1501901},
	language = {en},
	number = {4},
	urldate = {2017-10-31},
	journal = {The Journal of the Acoustical Society of America},
	author = {Langendijk, Erno H. A. and Bronkhorst, Adelbert W.},
	month = oct,
	year = {2002},
	pages = {1583--1596},
	file = {2002.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IQQMM2RW\\2002.pdf:application/pdf},
}

@article{dyrlund_characterization_1989,
	title = {Characterization of {Non}-{Linear} {Distortion} in {Hearing} {Aids} {Using} {Coherence} {Analysis} a {Pilot} {Study}},
	volume = {18},
	issn = {0105-0397},
	url = {http://www.tandfonline.com/doi/full/10.3109/01050398909070737},
	doi = {10.3109/01050398909070737},
	language = {en},
	number = {3},
	urldate = {2017-10-31},
	journal = {Scandinavian Audiology},
	author = {Dyrlund, Ole},
	month = jan,
	year = {1989},
	pages = {143--148},
	file = {dyrlund1989.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\W2HND84K\\dyrlund1989.pdf:application/pdf},
}

@article{kates_cross-correlation_2000,
	title = {Cross-correlation procedures for measuring noise and distortion in {AGC} hearing aids},
	volume = {107},
	number = {6},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kates, James M.},
	year = {2000},
	pages = {3407--3414},
	file = {kates2000.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PVFABU39\\kates2000.pdf:application/pdf},
}

@article{langendijk_fidelity_2000,
	title = {Fidelity of three-dimensional sound reproduction using a virtual auditory display},
	volume = {107},
	abstract = {The fidelity of reproducing free-field sounds using a virtual auditory display was investigated in two experiments. In the first experiment, listeners directly compared stimuli from an actual loudspeaker in the free field with those from small headphones placed in front of the ears. Headphone stimuli were filtered using head-related transfer functions  HRTFs , recorded while listeners were wearing the headphones, in order to reproduce the pressure signatures of the free-field sounds at the eardrum. Discriminability was investigated for six sound-source positions using broadband noise as a stimulus. The results show that the acoustic percepts of real and virtual sounds were identical. In the second experiment, discrimination between virtual sounds generated with measured and interpolated HRTFs was investigated. Interpolation was performed using HRTFs measured for loudspeaker positions with different spatial resolutions. Broadband noise bursts with flat and scrambled spectra were used as stimuli. The results indicate that, for a spatial resolution of about 6°, the interpolation does not introduce audible cues. For resolutions of 20° or more, the interpolation introduces audible cues related to timbre and position. For intermediate resolutions  10°–15°  the data suggest that only timbre cues were used.},
	number = {1},
	journal = {The Journal of the Acoustical Society of America},
	author = {Langendijk, Erno H. A. and Bronkhorst, Adelbert W.},
	month = jan,
	year = {2000},
	pages = {528--537},
	file = {2000.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JX7ZWRBB\\2000.pdf:application/pdf},
}

@article{brown_time-varying_2016,
	title = {Time-{Varying} {Distortions} of {Binaural} {Information} by {Bilateral} {Hearing} {Aids}: {Effects} of {Nonlinear} {Frequency} {Compression}},
	volume = {20},
	issn = {2331-2165, 2331-2165},
	shorttitle = {Time-{Varying} {Distortions} of {Binaural} {Information} by {Bilateral} {Hearing} {Aids}},
	url = {http://journals.sagepub.com/doi/10.1177/2331216516668303},
	doi = {10.1177/2331216516668303},
	language = {en},
	urldate = {2017-10-31},
	journal = {Trends in Hearing},
	author = {Brown, Andrew D. and Rodriguez, Francisco A. and Portnuff, Cory D. F. and Goupell, Matthew J. and Tollin, Daniel J.},
	month = sep,
	year = {2016},
	pages = {233121651666830},
	file = {Brown2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\SEF7UZBI\\Brown2016.pdf:application/pdf},
}

@article{veugen_effect_2017,
	title = {Effect of extreme adaptive frequency compression in bimodal listeners on sound localization and speech perception},
	volume = {18},
	issn = {1467-0100, 1754-7628},
	url = {https://www.tandfonline.com/doi/full/10.1080/14670100.2017.1353762},
	doi = {10.1080/14670100.2017.1353762},
	language = {en},
	number = {5},
	urldate = {2017-10-31},
	journal = {Cochlear Implants International},
	author = {Veugen, Lidwien C. E. and Chalupper, Josef and Mens, Lucas H. M. and Snik, Ad F. M. and van Opstal, A. John},
	month = sep,
	year = {2017},
	pages = {266--277},
	file = {veugen2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MKBRM5TV\\veugen2017.pdf:application/pdf},
}

@inproceedings{courtois_development_2015,
	title = {Development and assessment of a localization algorithm implemented in binaural hearing aids},
	booktitle = {23rd {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {Courtois, Gilles André and Marmaroli, Patrick and Lissek, Hervé and Oesch, Yves and Balande, William},
	year = {2015},
	file = {Courtois2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\P2ZBCQIR\\Courtois2015.pdf:application/pdf},
}

@article{dieudonne_head_2017,
	title = {Head shadow enhancement with fixed beamformers improves sound localization based on interaural level differences},
	journal = {Computing Resarch Repository - arXiv},
	author = {Dieudonné, Benjamin and Francart, Tom},
	year = {2017},
	file = {dieudonne2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\M52EM5GM\\dieudonne2017.pdf:application/pdf},
}

@inproceedings{ricketts_adaptive_nodate,
	title = {Adaptive directional benefit in the near field: competing sound angle and level effects},
	volume = {26},
	abstract = {Two experiments were performed that examined adaptive direction benefit and directional benefit as a function of competing noise level. Fourteen bilaterally fitted adult listeners with sloping, sensorineural hearing loss participated in both experiments. The results of the first experiment provide additional support for an adaptive advantage in environments with a discrete competing noise source placed near the listener. This advantage occurs even if the noise source is moving and only is present when the angle of the noise source relative to the listener does not correspond to an angle for which the fixed directional mode is optimized. Speed transmission index (STI) calculations do not generally support adaptive directional benefit in the presence of multiple competing noise sources. Specifically, an adaptive advantage was measured using the STI only when the intensity level of one of the competing noise sources in a group was at least 12 to 15dB greater than all other sources combined. The results of the second experment revealed more direction benefit for poorer signal-to-noise ratios (SNRs). However, if the SNR was held constant, the absolute noise level did not affect the magnitude of direction benefit.},
	author = {Ricketts, Todd A. and Hornsby, Benjamin W.Y. and Johnson, Earl E.},
	pages = {59--69},
	file = {Ricketts2005.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\M5LJSIMU\\Ricketts2005.pdf:application/pdf},
}

@article{hofman_relearning_1998,
	title = {Relearning sound localization with new ears},
	volume = {1},
	number = {5},
	journal = {Nature neuroscience},
	author = {Hofman, Paul M. and Van Riswick, Jos GA and Van Opstal, A. John},
	year = {1998},
	keywords = {HRTF, relearning, plasticity},
	pages = {417--421},
	file = {Hofman et al. - 1998 - Relearning sound localization with new ears.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NL442256\\Hofman et al. - 1998 - Relearning sound localization with new ears.pdf:application/pdf},
}

@article{glista_evaluation_2009,
	title = {Evaluation of nonlinear frequency compression: {Clinical} outcomes},
	volume = {48},
	issn = {1499-2027, 1708-8186},
	shorttitle = {Evaluation of nonlinear frequency compression},
	url = {http://www.tandfonline.com/doi/full/10.1080/14992020902971349},
	doi = {10.1080/14992020902971349},
	language = {en},
	number = {9},
	urldate = {2017-10-31},
	journal = {International Journal of Audiology},
	author = {Glista, Danielle and Scollie, Susan and Bagatto, Marlene and Seewald, Richard and Parsa, Vijay and Johnson, Andrew},
	month = jan,
	year = {2009},
	pages = {632--644},
	file = {glista2009.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6RRHBGHV\\glista2009.pdf:application/pdf},
}

@article{shaw_transformation_1974,
	title = {Transformation of sound pressure level from the free field to the eardrum in the horizontal plane},
	volume = {56},
	number = {6},
	journal = {The Journal of the Acoustical Society of America},
	author = {Shaw, E. A. G.},
	year = {1974},
	pages = {1848--1861},
	file = {shaw1974.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XIFRXB8H\\shaw1974.pdf:application/pdf},
}

@book{van_opstal_auditory_2016,
	title = {The auditory system and human sound-localization behavior},
	publisher = {Academic Press},
	author = {Van Opstal, John},
	year = {2016},
	file = {Van Opstal, John-The Auditory System and Human Sound-Localization Behavior-Academic Press (2016).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\T3SGZCEA\\Van Opstal, John-The Auditory System and Human Sound-Localization Behavior-Academic Press (2016).pdf:application/pdf},
}

@article{benesty_adaptive_2000,
	title = {Adaptive eigenvalue decomposition algorithm for passive acoustic source localization},
	volume = {107},
	number = {1},
	journal = {The Journal of the Acoustical Society of America},
	author = {Benesty, Jacob},
	year = {2000},
	pages = {384--391},
	file = {benesty2000.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Y3YHPPRU\\benesty2000.pdf:application/pdf},
}

@inproceedings{rohdenburg_robustness_2007,
	address = {New Paltz NY},
	title = {Robustness analysis of binaural hearing aid beamformer algorithms by means of objective perceptual quality measures},
	abstract = {In this contribution different microphone array-based noise reduc- tion schemes for hearing aids are suggested and compared in terms of their performance, signal quality and robustness against model errors. The algorithms all have binaural output and are evalu- ated using objective perceptual quality measures [1, 2, 3]. It has been shown earlier that these measures are able to predict sub- jective data that is relevant for the assessment of noise reduction algorithms. The quality measures showed clearly that fixed beam- formers designed with head models were relatively robust against steering errors whereas for the adaptive beamformers tested in this study the robustness was limited and the benefit due to higher noise reduction depended on the noise scenario and the reliability of a direction of arrival estimation. Furthermore, binaural cue distor- tions introduced by the different binaural output strategies could be identified by the binaural speech intelligibility measure [3] even in case monaural quality values were similar. Thus, this perceptual quality measure seems to be suitable to discover the benefit that the listener might have from the effect of spatial unmasking.},
	booktitle = {Workshop on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics}},
	publisher = {IEEE},
	author = {Rohdenburg, Thomas and Hohmann, Volker and Kollmeier, Birger},
	month = oct,
	year = {2007},
	file = {rohdenburg2007.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\R4ALM8ZD\\rohdenburg2007.pdf:application/pdf},
}

@article{yee_noise_2017,
	title = {A {Noise} {Reduction} {Post}-{Filter} for {Binaurally}-linked {Single}-{Microphone} {Hearing} {Aids} {Utilizing} a {Nearby} {External} {Microphone}},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Yee, Dianna and Kamkar-Parsi, Homayoun and Martin, Rainer and Henning, Puder},
	month = jun,
	year = {2017},
	keywords = {hearing aids, front-back ambiguity, external microphone, binaural beamforming, body-related transfer functions},
	file = {yee2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\FLTTN356\\yee2017.pdf:application/pdf},
}

@article{brungart_localization_2017,
	title = {The localization of non-individualized virtual sounds by hearing impaired listeners},
	volume = {141},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4979462},
	doi = {10.1121/1.4979462},
	language = {en},
	number = {4},
	urldate = {2017-11-07},
	journal = {The Journal of the Acoustical Society of America},
	author = {Brungart, Douglas S. and Cohen, Julie I. and Zion, Danielle and Romigh, Griffin},
	month = apr,
	year = {2017},
	pages = {2870--2881},
	file = {brungart2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\H3EEVL9N\\brungart2017.pdf:application/pdf},
}

@article{osses_vecchi_predicting_2017,
	title = {Predicting the perceived reverberation in different room acoustic environments using a binaural auditory model},
	volume = {141},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4979853},
	doi = {10.1121/1.4979853},
	language = {en},
	number = {4},
	urldate = {2017-11-07},
	journal = {The Journal of the Acoustical Society of America},
	author = {Osses Vecchi, Alejandro and Kohlrausch, Armin and Lachenmayr, Winfried and Mommertz, Eckard},
	month = apr,
	year = {2017},
	pages = {EL381--EL387},
}

@article{akeroyd_overview_2014,
	title = {An {Overview} of the {Major} {Phenomena} of the {Localization} of {Sound} {Sources} by {Normal}-{Hearing}, {Hearing}-{Impaired}, and {Aided} {Listeners}},
	volume = {18},
	issn = {2331-2165, 2331-2165},
	url = {http://journals.sagepub.com/doi/10.1177/2331216514560442},
	doi = {10.1177/2331216514560442},
	language = {en},
	urldate = {2017-11-07},
	journal = {Trends in Hearing},
	author = {Akeroyd, Michael A.},
	month = oct,
	year = {2014},
	pages = {1--7},
	file = {akeroyd2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KCKXNG5U\\akeroyd2014.pdf:application/pdf},
}

@article{boyd_auditory_2012,
	title = {Auditory externalization in hearing-impaired listeners: {The} effect of pinna cues and number of talkers},
	volume = {131},
	shorttitle = {Auditory externalization in hearing-impaired listeners},
	number = {3},
	journal = {The Journal of the Acoustical Society of America},
	author = {Boyd, Alan W. and Whitmer, William M. and Soraghan, John J. and Akeroyd, Michael A.},
	year = {2012},
	pages = {EL268--EL274},
	file = {boyd2012.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\K2GMLJDE\\boyd2012.pdf:application/pdf},
}

@article{byrne_hearing_1990,
	title = {Hearing aid gain and frequency response requirements for the severely/profoundly hearing impaired.},
	volume = {11},
	number = {1},
	journal = {Ear and Hearing},
	author = {Byrne, Denis and Parkinson, Aaron and Newall, Philip},
	year = {1990},
	pages = {40--49},
	file = {byrne1990.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BNUT45BQ\\byrne1990.pdf:application/pdf},
}

@article{musa-shufani_influence_2006,
	title = {Influence of dynamic compression on directional hearing in the horizontal plane},
	volume = {27},
	number = {3},
	journal = {Ear and hearing},
	author = {Musa-Shufani, Sharbal and Walger, Martin and von Wedel, Hasso and Meister, Hartmut},
	year = {2006},
	pages = {279--285},
	file = {musashufani2006.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\S5CN8FRZ\\musashufani2006.pdf:application/pdf},
}

@article{wiggins_dynamic-range_2011,
	title = {Dynamic-range compression affects the lateral position of sounds},
	volume = {130},
	doi = {10.1121/1.3652887},
	number = {6},
	journal = {The Journal of the Acoustical Society of America},
	author = {Wiggins, Ian M. and Seeber, Bernhard U.},
	year = {2011},
	pages = {3939--3953},
	file = {wiggins2011.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\B3AZHPTN\\wiggins2011.pdf:application/pdf},
}

@article{avni_spatial_2013,
	title = {Spatial perception of sound fields recorded by spherical microphone arrays with varying spatial resolution},
	volume = {133},
	number = {5},
	journal = {The Journal of the Acoustical Society of America},
	author = {Avni, Amir and Ahrens, Jens and Geier, Matthias and Spors, Sascha and Wierstorf, Hagen and Rafaely, Boaz},
	year = {2013},
	pages = {2711--2721},
	file = {avni2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\FCGMA3SY\\avni2013.pdf:application/pdf},
}

@article{abel_sound_1996,
	title = {Sound {Localization} the {Interaction} of {Aging}, {Hearing} {Loss} and {Hearing} {Protection}},
	volume = {25},
	issn = {0105-0397},
	url = {http://www.tandfonline.com/doi/full/10.3109/01050399609047549},
	doi = {10.3109/01050399609047549},
	language = {en},
	number = {1},
	urldate = {2017-11-08},
	journal = {Scandinavian Audiology},
	author = {Abel, Sharon M. and Hay, Valerie H.},
	month = jan,
	year = {1996},
	pages = {3--12},
	file = {abel1996.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KYJMYMD9\\abel1996.pdf:application/pdf},
}

@inproceedings{courtois_implementation_2014,
	title = {Implementation of a binaural localization algorithm in hearing aids: specifications and achievable solutions},
	shorttitle = {Implementation of a binaural localization algorithm in hearing aids},
	booktitle = {Audio {Engineering} {Society} {Convention} 136},
	publisher = {Audio Engineering Society},
	author = {Courtois, Gilles and Marmaroli, Patrick and Lindberg, Morten and Oesch, Yves and Balande, William},
	year = {2014},
	file = {Courtois2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AJXHBY69\\Courtois2014.pdf:application/pdf},
}

@article{durlach_binaural_1981,
	title = {Binaural {Interaction} in {Impaired} {Listeners}: a review of past research},
	volume = {20},
	journal = {Journal of Audiology},
	author = {Durlach, N. I. and Thompson, C. I. and Colburn, H. S.},
	year = {1981},
	file = {durlach1981.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XC9EDFKU\\durlach1981.pdf:application/pdf},
}

@article{lorenzi_sound_1999,
	title = {Sound localization in noise in hearing-impaired listeners},
	volume = {105},
	number = {6},
	journal = {The Journal of the Acoustical Society of America},
	author = {Lorenzi, Christian},
	month = jun,
	year = {1999},
	file = {lorenzi1999.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5AMH5WHV\\lorenzi1999.pdf:application/pdf},
}

@article{ibrahim_evaluation_2012,
	title = {Evaluation of speech intelligibility and sound localization abilities with hearing aids using binaural wireless technology},
	volume = {3},
	issn = {2039-4349, 2039-4330},
	url = {http://www.audiologyresearch.org/index.php/audio/article/view/audiores.2013.e1},
	doi = {10.4081/audiores.2013.e1},
	number = {1},
	urldate = {2017-11-08},
	journal = {Audiology Research},
	author = {Ibrahim, Iman and Parsa, Vijay and Macpherson, Ewan and Cheesman, Margaret},
	month = dec,
	year = {2012},
	pages = {10},
	file = {ibrahim2012.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VBZKZHLG\\ibrahim2012.pdf:application/pdf},
}

@article{schwartz_effects_2013,
	title = {Effects of dynamic range compression on spatial selective auditory attention in normal-hearing listeners},
	volume = {133},
	doi = {10.1121/1.4794386},
	number = {4},
	journal = {The Journal of the Acoustical Society of America},
	author = {Schwartz, Andrew H. and Shinn-Cunningham, Barbara G.},
	year = {2013},
	pages = {2329--2339},
	file = {schwartz2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\N8QIU52T\\schwartz2013.pdf:application/pdf},
}

@article{wiggins_effects_2012,
	title = {Effects of dynamic-range compression on the spatial attributes of sounds in normal-hearing listeners},
	volume = {33},
	doi = {10.1121/1.3652887},
	number = {3},
	journal = {Ear and hearing},
	author = {Wiggins, Ian M. and Seeber, Bernhard U.},
	year = {2012},
	pages = {399--410},
	file = {wiggins2012.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ICT2F6CH\\wiggins2012.pdf:application/pdf},
}

@article{wiggins_linking_2013,
	title = {Linking dynamic-range compression across the ears can improve speech intelligibility in spatially separated noise},
	volume = {133},
	number = {2},
	journal = {The Journal of the Acoustical Society of America},
	author = {Wiggins, Ian M. and Seeber, Bernhard U.},
	year = {2013},
	pages = {1004--1016},
	file = {wiggins2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4VM5KIEA\\wiggins2013.pdf:application/pdf},
}

@article{korhonen_effects_2015,
	title = {Effects of {Coordinated} {Compression} and {Pinna} {Compensation} {Features} on {Horizontal} {Localization} {Performance} in {Hearing} {Aid} {Users}},
	volume = {26},
	issn = {10500545, 21573107},
	url = {http://openurl.ingenta.com/content/xref?genre=article&issn=1050-0545&volume=26&issue=1&spage=80},
	doi = {10.3766/jaaa.26.1.9},
	language = {en},
	number = {1},
	urldate = {2017-11-08},
	journal = {Journal of the American Academy of Audiology},
	author = {Korhonen, Petri and Lau, Chi and Kuk, Francis and Keenan, Denise and Schumacher, Jennifer},
	year = {2015},
	keywords = {hearing aids, spatial hearing, localization},
	pages = {80--92},
	file = {korhonen2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WEQDWWFW\\korhonen2015.pdf:application/pdf},
}

@article{best_comparison_2010,
	title = {A comparison of {CIC} and {BTE} hearing aids for three-dimensional localization of speech},
	volume = {49},
	issn = {1499-2027, 1708-8186},
	url = {http://www.tandfonline.com/doi/full/10.3109/14992027.2010.484827},
	doi = {10.3109/14992027.2010.484827},
	language = {en},
	number = {10},
	urldate = {2017-11-08},
	journal = {International Journal of Audiology},
	author = {Best, Virginia and Kalluri, Sridhar and McLachlan, Sara and Valentine, Susie and Edwards, Brent and Carlile, Simon},
	month = oct,
	year = {2010},
	pages = {723--732},
	file = {best2010.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3PD9URS4\\best2010.pdf:application/pdf},
}

@article{byrne_effects_1992,
	title = {Effects of long-term bilateral and unilateral fitting of different hearing aid types on the ability to locate sounds},
	volume = {3},
	number = {6},
	journal = {Journal of the American Academy of Audiology},
	author = {Byrne, Denis and Noble, W. and LePage, B.},
	year = {1992},
	pages = {369--382},
	file = {bryne1992.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7NPHDART\\bryne1992.pdf:application/pdf},
}

@article{noble_auditory_1997,
	title = {Auditory localization, detection of spatial separateness, and speech hearing in noise by hearing impaired listeners},
	volume = {102},
	number = {4},
	journal = {The Journal of the Acoustical Society of America},
	author = {Noble, William and Byrne, Denis and Ter-Horst, Kim},
	year = {1997},
	pages = {2343--2352},
	file = {noble1997.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\97I526GR\\noble1997.pdf:application/pdf},
}

@article{bai_microphone_2005,
	title = {Microphone array signal processing with application in three-dimensional spatial hearing},
	volume = {117},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1853242},
	doi = {10.1121/1.1853242},
	language = {en},
	number = {4},
	urldate = {2017-11-09},
	journal = {The Journal of the Acoustical Society of America},
	author = {Bai, Mingsian R. and Lin, Chenpang},
	month = apr,
	year = {2005},
	pages = {2112--2121},
	file = {bai2005.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PRPDE9ZY\\bai2005.pdf:application/pdf},
}

@article{courtois_dynamic_2016,
	title = {Dynamic {Range} {Limiting} of {HRTFs}: {Principle} and {Objective} evaluation},
	volume = {64},
	issn = {15494950},
	shorttitle = {Dynamic {Range} {Limiting} of {HRTFs}},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=18515},
	doi = {10.17743/jaes.2016.0023},
	number = {10},
	urldate = {2017-11-09},
	journal = {Journal of the Audio Engineering Society},
	author = {Courtois, Gilles and Marmaroli, Patrick and Rohr, Lukas and al, et},
	month = oct,
	year = {2016},
	pages = {731--739},
	file = {Courtois et al. - 2016 - Dynamic Range Limiting of HRTFs Principle and Obj.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4SGYE4NA\\Courtois et al. - 2016 - Dynamic Range Limiting of HRTFs Principle and Obj.pdf:application/pdf},
}

@article{farahikia_optimized_2017,
	title = {Optimized {Finite} {Element} {Method} for {Acoustic} {Scattering} {Analysis} {With} {Application} to {Head}-{Related} {Transfer} {Function} {Estimation}},
	volume = {139},
	issn = {1048-9002},
	url = {http://vibrationacoustics.asmedigitalcollection.asme.org/article.aspx?doi=10.1115/1.4035813},
	doi = {10.1115/1.4035813},
	language = {en},
	number = {3},
	urldate = {2017-11-09},
	journal = {Journal of Vibration and Acoustics},
	author = {Farahikia, Mahdi and Su, Quang T.},
	month = apr,
	year = {2017},
	pages = {034501},
	file = {farahikia2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9WTIEP7J\\farahikia2017.pdf:application/pdf},
}

@article{harder_reliability_2015,
	title = {Reliability in {Measuring} {Head} {Related} {Transfer} {Functions} of {Hearing} {Aids}},
	volume = {101},
	issn = {16101928},
	url = {http://openurl.ingenta.com/content/xref?genre=article&issn=1610-1928&volume=101&issue=5&spage=1064},
	doi = {10.3813/AAA.918900},
	language = {en},
	number = {5},
	urldate = {2017-11-09},
	journal = {Acta Acustica united with Acustica},
	author = {Harder, S. and Paulsen, R. R. and Larsen, M. and Laugesen, S. and Mihocic, M. and Majdak, P.},
	month = sep,
	year = {2015},
	keywords = {hearing aids, HRTF measurements},
	pages = {1064--1066},
	file = {harder2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GLD4X2ML\\harder2015.pdf:application/pdf},
}

@inproceedings{nakashima_binaural_2015,
	title = {Binaural wind noise detection, cancellation and its evaluation for hearing aids based on {HRTF} cues},
	booktitle = {Industrial {Electronics} {Society}, {IECON} 2015-41st {Annual} {Conference} of the {IEEE}},
	publisher = {IEEE},
	author = {Nakashima, Hidetoshi and Kouyama, Ryousuke and Hiruma, Nobuhiko and Fujisaka, Yoh-ichi},
	year = {2015},
	pages = {004896--004899},
	file = {nakashima2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XQF2KKCP\\nakashima2015.pdf:application/pdf},
}

@article{majdak_effect_2013,
	title = {Effect of long-term training on sound localization performance with spectrally warped and band-limited head-related transfer functions},
	volume = {134},
	number = {3},
	journal = {The Journal of the Acoustical Society of America},
	author = {Majdak, Piotr and Walder, Thomas and Laback, Bernhard},
	year = {2013},
	pages = {2148--2159},
	file = {majdak2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\82UTJZ2F\\majdak2013.pdf:application/pdf},
}

@article{baumgartner_asymmetries_2017,
	title = {Asymmetries in behavioral and neural responses to spectral cues demonstrate the generality of auditory looming bias},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1703247114},
	doi = {10.1073/pnas.1703247114},
	language = {en},
	number = {36},
	urldate = {2017-11-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Baumgartner, Robert and Reed, Darrin K. and Tóth, Brigitta and Best, Virginia and Majdak, Piotr and Colburn, H. Steven and Shinn-Cunningham, Barbara},
	month = sep,
	year = {2017},
	pages = {9743--9748},
	file = {Baumgartner2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PC6KBPZP\\Baumgartner2017.pdf:application/pdf},
}

@inproceedings{epain_objective_2010,
	title = {Objective evaluation of a three-dimensional sound field reproduction system},
	booktitle = {Proceedings of the 20th {International} {Congress} on {Acoustics}, {Sydney}, {Australia}},
	author = {Epain, Nicolas and Guillon, Pierre and Kan, Alan and Kosobrodov, Roman and Sun, David and Jin, Craig and van Schaik, André},
	year = {2010},
	file = {epain2010.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6VBJ6HNR\\epain2010.pdf:application/pdf},
}

@article{jin_creating_2014,
	title = {Creating the {Sydney} {York} {Morphological} and {Acoustic} {Recordings} of {Ears} {Database}},
	volume = {16},
	issn = {1520-9210, 1941-0077},
	url = {http://ieeexplore.ieee.org/document/6600896/},
	doi = {10.1109/TMM.2013.2282134},
	number = {1},
	urldate = {2017-11-13},
	journal = {IEEE Transactions on Multimedia},
	author = {Jin, Craig T. and Guillon, Pierre and Epain, Nicolas and Zolfaghari, Reza and van Schaik, Andre and Tew, Anthony I. and Hetherington, Carl and Thorpe, Jonathan},
	month = jan,
	year = {2014},
	pages = {37--46},
	file = {jin2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\D7ZTDSL4\\jin2014.pdf:application/pdf},
}

@article{kacelnik_training-induced_2006,
	title = {Training-{Induced} {Plasticity} of {Auditory} {Localization} in {Adult} {Mammals}},
	volume = {4},
	issn = {1545-7885},
	url = {http://dx.plos.org/10.1371/journal.pbio.0040071},
	doi = {10.1371/journal.pbio.0040071},
	language = {en},
	number = {4},
	urldate = {2017-11-13},
	journal = {PLoS Biology},
	author = {Kacelnik, Oliver and Nodal, Fernando R and Parsons, Carl H and King, Andrew J},
	editor = {Shamma, Shihib},
	month = mar,
	year = {2006},
	pages = {e71},
	file = {kacelnik2006.PDF:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\DNSYEA56\\kacelnik2006.PDF:application/pdf},
}

@article{khaykin_acoustic_2012,
	title = {Acoustic analysis by spherical microphone array processing of room impulse responses},
	volume = {132},
	number = {1},
	journal = {The Journal of the Acoustical Society of America},
	author = {Khaykin, Dima and Rafaely, Boaz},
	year = {2012},
	pages = {261--270},
	file = {khaykin2012.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CRQTA2V4\\khaykin2012.pdf:application/pdf},
}

@article{kumpik_adaptive_2010,
	title = {Adaptive {Reweighting} of {Auditory} {Localization} {Cues} in {Response} to {Chronic} {Unilateral} {Earplugging} in {Humans}},
	volume = {30},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.5488-09.2010},
	doi = {10.1523/JNEUROSCI.5488-09.2010},
	language = {en},
	number = {14},
	urldate = {2017-11-13},
	journal = {Journal of Neuroscience},
	author = {Kumpik, D. P. and Kacelnik, O. and King, A. J.},
	month = apr,
	year = {2010},
	pages = {4883--4894},
	file = {kumpik2010.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\G8H7DLU4\\kumpik2010.pdf:application/pdf},
}

@article{van_wanrooij_relearning_2005,
	title = {Relearning {Sound} {Localization} with a {New} {Ear}},
	volume = {25},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.0850-05.2005},
	doi = {10.1523/JNEUROSCI.0850-05.2005},
	language = {en},
	number = {22},
	urldate = {2017-11-13},
	journal = {Journal of Neuroscience},
	author = {Van Wanrooij, M. M.},
	month = jun,
	year = {2005},
	pages = {5413--5424},
	file = {van-wanrooij2005.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\J7IN3DLE\\van-wanrooij2005.pdf:application/pdf},
}

@inproceedings{wu_dereverberation_2012,
	title = {A dereverberation algorithm for spherical microphone arrays using compressed sensing techniques},
	booktitle = {Acoustics, {Speech} and {Signal} {Processing} ({ICASSP}), 2012 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Wu, Ping Kun Tony and Epain, Nicolas and Jin, Craig},
	year = {2012},
	pages = {4053--4056},
	file = {wu2012.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\P5TSK2QG\\wu2012.pdf:application/pdf},
}

@misc{hawkey_hawkey2004.pdf_nodate,
	title = {hawkey2004.pdf},
	author = {Hawkey, David J. C. and Amitay, Sygal and Moore, David R.},
	file = {hawkey2004.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9E9FMI4H\\hawkey2004.pdf:application/pdf},
}

@inproceedings{duraiswaini_interpolation_2004,
	title = {Interpolation and range extrapolation of {HRTFs} [head related transfer functions]},
	volume = {4},
	booktitle = {Acoustics, {Speech}, and {Signal} {Processing}, 2004. {Proceedings}.({ICASSP}'04). {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Duraiswaini, R. and Zotkin, Dmitry N. and Gumerov, Nail A.},
	year = {2004},
	pages = {iv--iv},
	file = {duraiswami2004.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\G49PYGJJ\\duraiswami2004.pdf:application/pdf},
}

@mastersthesis{duval_etudes_2006,
	title = {Études de techniques d’extraction de l’information spatiale dans une scène sonore multicanal},
	language = {fr},
	school = {Universite Pierre et Marie Curie / IRCAM},
	author = {Duval, Benjamin},
	year = {2006},
	file = {duval2006.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TRP9J2RZ\\duval2006.pdf:application/pdf},
}

@phdthesis{moreau_etude_2006,
	address = {Le Mans, France},
	title = {Étude et réalisation d'outils avancés d'encodage spatial pour la technique de spatialisation sonore {Higher} {Order} {Ambisonics} : microphone {3D} et contrôle de distance},
	school = {Université du Maine},
	author = {Moreau, Sébastien},
	month = jul,
	year = {2006},
	file = {Moreau2006phd.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9ZX49H8B\\Moreau2006phd.pdf:application/pdf},
}

@article{gumerov_computation_2002,
	title = {Computation of scattering from {N} spheres using multipole reexpansion},
	volume = {112},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1517253},
	doi = {10.1121/1.1517253},
	language = {en},
	number = {6},
	urldate = {2017-11-15},
	journal = {The Journal of the Acoustical Society of America},
	author = {Gumerov, Nail A. and Duraiswami, Ramani},
	month = dec,
	year = {2002},
	pages = {2688--2701},
	file = {gumerov2002.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NJYBM6J8\\gumerov2002.pdf:application/pdf},
}

@article{wolfe_preliminary_2017,
	title = {Preliminary evaluation of a novel non-linear frequency compression scheme for use in children},
	volume = {56},
	issn = {1499-2027, 1708-8186},
	url = {https://www.tandfonline.com/doi/full/10.1080/14992027.2017.1358467},
	doi = {10.1080/14992027.2017.1358467},
	language = {en},
	number = {12},
	urldate = {2017-11-17},
	journal = {International Journal of Audiology},
	author = {Wolfe, Jace and Duke, Mila and Schafer, Erin C. and Rehmann, Julia and Jha, Siddhartha and Allegro Baumann, Silvia and John, Andrew and Jones, Christine},
	month = dec,
	year = {2017},
	pages = {976--988},
	file = {wolfe2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LUU9E2QM\\wolfe2017.pdf:application/pdf},
}

@article{rohr_evaluation_2015,
	title = {Evaluation of audio source separation in the context of {3D} audio},
	author = {Rohr, Lukas},
	year = {2015},
	file = {EPFL_TH6723.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\R8MR53ZV\\EPFL_TH6723.pdf:application/pdf},
}

@inproceedings{kearney_depth_2010,
	title = {Depth perception in interactive virtual acoustic environments using higher order ambisonic soundfields},
	booktitle = {Proccedings of the 2nd {International} {Symposium} on {Ambisonics} and {Spherical} {Acoustics}},
	author = {Kearney, Gavin and Gorzel, Marcin and Boland, Frank and Rice, Henry},
	year = {2010},
	file = {Kearney2010.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2IQF43UD\\Kearney2010.pdf:application/pdf},
}

@article{kearney_distance_2012,
	title = {Distance {Perception} in {Interactive} {Virtual} {Acoustic} {Environments} using {First} and {Higher} {Order} {Ambisonic} {Sound} {Fields}},
	volume = {98},
	journal = {Acta Acustica united with Acustica},
	author = {Kearney, Gavin and Gorzel, Marcin and Rice, Henry and Boland, Frank},
	year = {2012},
	pages = {61--71},
	file = {Kearney2012.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\X9R3FEXB\\Kearney2012.pdf:application/pdf},
}

@phdthesis{pinto_signal_2010,
	title = {Signal {Processing} in {Space} and {Time} - {A} {Multidimensional} {Fourier} {Approach}},
	school = {École Polytechnique Fédérale de Lausanne},
	author = {Pinto, Francisco Pereira Correia},
	year = {2010},
	file = {2010.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\36K8Z74B\\2010.pdf:application/pdf},
}

@phdthesis{lepauloux_prise_2010,
	title = {Prise de son distante par système multimicrophone. {Application} à la communication parlée en environnement bruyant},
	school = {Université Rennes 1},
	author = {Lepauloux, Ludovick},
	year = {2010},
	file = {Lepauloux2010phd.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\42NWDIRQ\\Lepauloux2010phd.pdf:application/pdf},
}

@article{lee_computation_2016,
	title = {Computation of scattering of a plane wave from multiple prolate spheroids using the collocation multipole method},
	volume = {140},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4963089},
	doi = {10.1121/1.4963089},
	language = {en},
	number = {4},
	urldate = {2017-11-17},
	journal = {The Journal of the Acoustical Society of America},
	author = {Lee, W. M. and Chen, J. T.},
	month = oct,
	year = {2016},
	pages = {2235--2246},
	file = {lee2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MKAH7ZVE\\lee2016.pdf:application/pdf},
}

@book{bruneau_fundamentals_2006,
	address = {London},
	title = {Fundamentals of acoustics},
	isbn = {978-1-905209-25-5},
	language = {eng},
	publisher = {ISTE},
	author = {Bruneau, Michel and Scelo, Thomas},
	year = {2006},
	note = {OCLC: 255489082},
	file = {Michel Bruneau, Socit Franaise d'Acoustique, Thomas Scelo-Fundamentals of Acoustics-ISTE Ltd (2006).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TPIW68DG\\Michel Bruneau, Socit Franaise d'Acoustique, Thomas Scelo-Fundamentals of Acoustics-ISTE Ltd (2006).pdf:application/pdf},
}

@article{mueller_localization_2012,
	title = {Localization of virtual sound sources with bilateral hearing aids in realistic acoustical scenes},
	volume = {131},
	number = {6},
	journal = {The Journal of the Acoustical Society of America},
	author = {Mueller, Martin F. and Kegel, Andrea and Schimmel, Steven M. and Dillier, Norbert and Hofbauer, Markus},
	year = {2012},
	pages = {4732--4742},
	file = {mueller2012.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VQNMKGSC\\mueller2012.pdf:application/pdf},
}

@article{keidser_effect_2006,
	title = {The effect of multi-channel wide dynamic range compression, noise reduction, and the directional microphone on horizontal localization performance in hearing aid wearers},
	volume = {45},
	issn = {1499-2027, 1708-8186},
	shorttitle = {The effect of multi-channel wide dynamic range compression, noise reduction, and the directional microphone on horizontal localization performance in hearing aid wearers},
	url = {http://www.tandfonline.com/doi/full/10.1080/14992020600920804},
	doi = {10.1080/14992020600920804},
	language = {en},
	number = {10},
	urldate = {2017-11-21},
	journal = {International Journal of Audiology},
	author = {Keidser, Gitte and Rohrseitz, Kristin and Dillon, Harvey and Hamacher, Volkmar and Carter, Lyndal and Rass, Uwe and Convery, Elizabeth},
	month = jan,
	year = {2006},
	pages = {563--579},
	file = {keidser2006.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZQWIKVE3\\keidser2006.pdf:application/pdf},
}

@article{masterton_evolution_1969,
	title = {The {Evolution} of {Human} {Hearing}},
	volume = {45},
	number = {4},
	journal = {The Journal of Acoustical Society of America},
	author = {Masterton, Bruce and Heffner, Henry and Ravizza, Richard},
	year = {1969},
	pages = {966--985},
	file = {Evolution_of_Human_Hearing_1969.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6TF7UUB2\\Evolution_of_Human_Hearing_1969.pdf:application/pdf},
}

@article{byrne_optimizing_1998,
	title = {Optimizing {Sound} {Localization} with {Hearing} {Aids}},
	volume = {3},
	number = {2},
	journal = {Trends in Amplification},
	author = {Byrne, Denis and Noble, William},
	year = {1998},
	pages = {51--73},
	file = {byrne1998.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KJ4PV33M\\byrne1998.pdf:application/pdf},
}

@article{thiemann_speech_2016,
	title = {Speech enhancement for multimicrophone binaural hearing aids aiming to preserve the spatial auditory scene},
	volume = {2016},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-016-0314-6},
	doi = {10.1186/s13634-016-0314-6},
	language = {en},
	number = {1},
	urldate = {2017-11-21},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Thiemann, Joachim and Müller, Menno and Marquardt, Daniel and Doclo, Simon and van de Par, Steven},
	month = dec,
	year = {2016},
	pages = {11},
	file = {thiemann2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZC57AWCW\\thiemann2016.pdf:application/pdf},
}

@article{shinn-cunningham_tori_2000,
	title = {Tori of confusion: {Binaural} localization cues for sources within reach of a listener},
	volume = {107},
	shorttitle = {Tori of confusion},
	number = {3},
	journal = {The Journal of the Acoustical Society of America},
	author = {Shinn-Cunningham, Barbara G. and Santarelli, Scott and Kopco, Norbert},
	year = {2000},
	pages = {1627--1636},
	file = {JASA_Tori.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IXDZSYV5\\JASA_Tori.pdf:application/pdf},
}

@book{suzuki_principles_2009,
	address = {Zao, Japan},
	series = {World {Scientific}},
	title = {Principles and {Applications} of {Spatial} {Hearing}},
	author = {Suzuki, Yôiti and Brungart, Douglas S. and Iwaya, Yukio and Iida, Kazuhiro and Cabrera, Densil and Kato, Hiroaki},
	month = nov,
	year = {2009},
	file = {Yôiti Suzuki, Douglas Brungart, Yukio Iwaya, Kazuhiro Iida, Densil Cabrera, Hiroaki Kato, Editors-Principles and Applications of Spatial Hearing-World Scientific Publishing Company (2011).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4U9TVES8\\Yôiti Suzuki, Douglas Brungart, Yukio Iwaya, Kazuhiro Iida, Densil Cabrera, Hiroaki Kato, Editors-Principles and Applications of Spatial Hearing-World Scientific Publishing Company (2011).pdf:application/pdf},
}

@article{doclo_multichannel_2015,
	title = {Multichannel {Signal} {Enhancement} {Algorithms} for {Assisted} {Listening} {Devices}: {Exploiting} spatial diversity using multiple microphones},
	volume = {32},
	issn = {1053-5888},
	shorttitle = {Multichannel {Signal} {Enhancement} {Algorithms} for {Assisted} {Listening} {Devices}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7038719},
	doi = {10.1109/MSP.2014.2366780},
	number = {2},
	urldate = {2017-11-23},
	journal = {IEEE Signal Processing Magazine},
	author = {Doclo, Simon and Kellermann, Walter and Makino, Shoji and Nordholm, Sven Erik},
	month = mar,
	year = {2015},
	pages = {18--30},
	file = {doclo2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\I5R9KFNX\\doclo2015.pdf:application/pdf;SPM-Feb-2014-022-final.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7JIN366G\\SPM-Feb-2014-022-final.pdf:application/pdf},
}

@book{tichavsky_latent_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Latent {Variable} {Analysis} and {Signal} {Separation}},
	volume = {10169},
	isbn = {978-3-319-53546-3 978-3-319-53547-0},
	url = {http://link.springer.com/10.1007/978-3-319-53547-0},
	urldate = {2017-11-24},
	publisher = {Springer International Publishing},
	editor = {Tichavský, Petr and Babaie-Zadeh, Massoud and Michel, Olivier J.J. and Thirion-Moreau, Nadège},
	year = {2017},
	doi = {10.1007/978-3-319-53547-0},
	file = {Latent Variable Analysis and Signal Separation - LVA_ICA_2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PW6B9ESK\\Latent Variable Analysis and Signal Separation - LVA_ICA_2017.pdf:application/pdf},
}

@inproceedings{duong_audio_2017,
	address = {Grenoble, France},
	title = {Audio {Zoom} for {Smartphones} {Based} on {Multiple} {Adaptive} {Beamformers}},
	volume = {10169},
	shorttitle = {{LVA}/{ICA} 2017},
	booktitle = {Latent {Variable} {Analysis} and {Signal} {Separation}: 13th {International} {Conference}, {Proceedings}},
	publisher = {Springer},
	author = {Duong, Ngoc Q. K. and Berthet, Pierre and Ozerov, Alexey and Chevallier, Louis and Zabre, Sidkièta},
	month = feb,
	year = {2017},
	pages = {121},
	file = {duong2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QCBU2G54\\duong2016.pdf:application/pdf},
}

@inproceedings{thiemann_experimental_2013,
	title = {An experimental comparison of source separation and beamforming techniques for microphone array signal enhancement},
	booktitle = {Machine {Learning} for {Signal} {Processing} ({MLSP}), 2013 {IEEE} {International} {Workshop} on},
	publisher = {IEEE},
	author = {Thiemann, Joachim and Vincent, Emmanuel},
	year = {2013},
	pages = {1--5},
	file = {thiemann2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CCT5SAJ4\\thiemann2013.pdf:application/pdf},
}

@inproceedings{ba_enhanced_2007,
	title = {Enhanced {MVDR} beamforming for arrays of directional microphones},
	booktitle = {Multimedia and {Expo}, 2007 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Ba, Demba E. and Florencio, Dinei and Zhang, Cha},
	year = {2007},
	pages = {1307--1310},
	file = {ba2007.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KY6U3MIX\\ba2007.pdf:application/pdf},
}

@article{best_examination_2017,
	title = {Examination of a hybrid beamformer that preserves auditory spatial cues},
	volume = {142},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.5007279},
	doi = {10.1121/1.5007279},
	language = {en},
	number = {4},
	urldate = {2017-11-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Best, Virginia and Roverud, Elin and Mason, Christine R. and Kidd, Gerald},
	month = oct,
	year = {2017},
	pages = {EL369--EL374},
	file = {best2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\36CLLNLG\\best2017.pdf:application/pdf},
}

@book{moreau_tools_2011,
	address = {London},
	title = {Tools for signal compression},
	isbn = {978-1-84821-255-8},
	language = {eng},
	publisher = {ISTE},
	author = {Moreau, Nicolas},
	year = {2011},
	note = {OCLC: 723524173},
	file = {Nicolas Moreau-Tools for Signal Compression_ Applications to Speech and Audio Coding-Wiley (2011).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5LAL6SG9\\Nicolas Moreau-Tools for Signal Compression_ Applications to Speech and Audio Coding-Wiley (2011).pdf:application/pdf},
}

@article{wouters_sound_2013,
	title = {Sound {Processing} for {Better} {Coding} of {Monaural} and {Binaural} {Cues} in {Auditory} {Prostheses}},
	volume = {101},
	issn = {0018-9219, 1558-2256},
	url = {http://ieeexplore.ieee.org/document/6570749/},
	doi = {10.1109/JPROC.2013.2257635},
	number = {9},
	urldate = {2017-11-27},
	journal = {Proceedings of the IEEE},
	author = {Wouters, Jan and Doclo, Simon and Koning, Raphael and Francart, Tom},
	month = sep,
	year = {2013},
	pages = {1986--1997},
	file = {wouters2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Y6C4UC3K\\wouters2013.pdf:application/pdf},
}

@book{haykin_handbook_2009,
	address = {Hoboken, NJ},
	title = {Handbook on array processing and sensor networks},
	isbn = {978-0-470-37176-3},
	language = {eng},
	publisher = {Wiley},
	author = {Haykin, Simon S. and Liu, K. J. Ray},
	year = {2009},
	note = {OCLC: 845468569},
	file = {(Adaptive and Learning Systems for Signal Processing, Communications and Control) Simon Haykin, K. J. Ray Liu-Handbook on Array Processing and Sensor Networks -Wiley-IEEE Press (2010).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3GQ5SWJB\\(Adaptive and Learning Systems for Signal Processing, Communications and Control) Simon Haykin, K. J. Ray Liu-Handbook on Array Processing and Sensor Networks -Wiley-IEEE Press (2010).pdf:application/pdf},
}

@article{frost_algorithm_1972,
	title = {An {Algorithm} for {Linearly} {Constrained} {Adaptive} {Array} {Processing}},
	volume = {60},
	number = {8},
	journal = {Proceedings of the IEEE},
	author = {Frost, Otis L.},
	month = aug,
	year = {1972},
	pages = {926--935},
	file = {frost1972.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3PUNDUGZ\\frost1972.pdf:application/pdf},
}

@book{brandstein_microphone_2001,
	address = {Berlin, Heidelberg},
	series = {Digital {Signal} {Processing}},
	title = {Microphone {Arrays}},
	isbn = {978-3-642-07547-6 978-3-662-04619-7},
	url = {http://link.springer.com/10.1007/978-3-662-04619-7},
	urldate = {2017-11-27},
	publisher = {Springer Berlin Heidelberg},
	editor = {Brandstein, Michael and Ward, Darren and Lacroix, Arild and Venetsanopoulos, Anastasios},
	year = {2001},
	doi = {10.1007/978-3-662-04619-7},
	file = {(Digital Signal Processing) Darren B. Ward, Rodney A. Kennedy, Robert C. Williamson (auth.), Prof. Michael Brandstein, Dr. Darren Ward (eds.)-Microphone Arrays_ Signal Processing Techniques and Applic.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\J76TYDF9\\(Digital Signal Processing) Darren B. Ward, Rodney A. Kennedy, Robert C. Williamson (auth.), Prof. Michael Brandstein, Dr. Darren Ward (eds.)-Microphone Arrays_ Signal Processing Techniques and Applic.pdf:application/pdf},
}

@article{cox_robust_1987,
	title = {Robust {Adaptive} {Beamforming}},
	volume = {ASSP-35},
	number = {10},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Cox, Henry and Zeskind, Robert M. and Owen, Mark},
	month = oct,
	year = {1987},
	pages = {1365--1376},
	file = {cox1987.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\F2YG6N88\\cox1987.pdf:application/pdf},
}

@article{oxenham_speech_2017,
	title = {Speech intelligibility is best predicted by intensity, not cochlea-scaled entropy},
	volume = {142},
	number = {3},
	journal = {The Journal of the Acoustical Society of America},
	author = {Oxenham, Andrew J. and Boucher, Jeffrey E. and Kreft, Heather A.},
	year = {2017},
	pages = {EL264--EL269},
	file = {oxenham2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\R9HJFYIU\\oxenham2017.pdf:application/pdf},
}

@article{swanson_small-aperture_2017,
	title = {Small-aperture array processing for passive multi-target angle of arrival estimation},
	volume = {142},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.5006910},
	doi = {10.1121/1.5006910},
	language = {en},
	number = {4},
	urldate = {2017-11-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Swanson, D. C. and Culver, R. L.},
	month = oct,
	year = {2017},
	pages = {2030--2042},
	file = {swanson2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YVQ5FD9V\\swanson2017.pdf:application/pdf},
}

@article{bai_localization_2017,
	title = {Localization and separation of acoustic sources by using a 2.5-dimensional circular microphone array},
	volume = {142},
	number = {1},
	journal = {The Journal of the Acoustical Society of America},
	author = {Bai, Mingsian R. and Lai, Chang-Sheng and Wu, Po-Chen},
	year = {2017},
	pages = {286--297},
	file = {bai2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\DB7ZWULU\\bai2017.pdf:application/pdf},
}

@article{bissmeyer_adaptive_2017,
	title = {Adaptive spatial filtering improves speech reception in noise while preserving binaural cues},
	volume = {142},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.5002691},
	doi = {10.1121/1.5002691},
	language = {en},
	number = {3},
	urldate = {2017-11-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Bissmeyer, Susan R. S. and Goldsworthy, Raymond L.},
	month = sep,
	year = {2017},
	pages = {1441--1453},
	file = {bissmeyer2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4ZANXKLS\\bissmeyer2017.pdf:application/pdf},
}

@article{samarasinghe_acoustic_2017,
	title = {Acoustic reciprocity: {An} extension to spherical harmonics domain},
	volume = {142},
	issn = {0001-4966},
	shorttitle = {Acoustic reciprocity},
	url = {http://asa.scitation.org/doi/10.1121/1.5002078},
	doi = {10.1121/1.5002078},
	language = {en},
	number = {4},
	urldate = {2017-11-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Samarasinghe, Prasanga and Abhayapala, Thushara D. and Kellermann, Walter},
	month = oct,
	year = {2017},
	pages = {EL337--EL343},
	file = {samarasinghe2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YVWMZB6H\\samarasinghe2017.pdf:application/pdf},
}

@article{vorobyov_principles_2013,
	title = {Principles of minimum variance robust adaptive beamforming design},
	volume = {93},
	issn = {01651684},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0165168412003830},
	doi = {10.1016/j.sigpro.2012.10.021},
	language = {en},
	number = {12},
	urldate = {2017-11-27},
	journal = {Signal Processing},
	author = {Vorobyov, Sergiy A.},
	month = dec,
	year = {2013},
	pages = {3264--3277},
	file = {vorobyov2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YYZY8SZH\\vorobyov2013.pdf:application/pdf},
}

@article{yujie_gu_robust_2012,
	title = {Robust {Adaptive} {Beamforming} {Based} on {Interference} {Covariance} {Matrix} {Reconstruction} and {Steering} {Vector} {Estimation}},
	volume = {60},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/6180020/},
	doi = {10.1109/TSP.2012.2194289},
	number = {7},
	urldate = {2017-11-27},
	journal = {IEEE Transactions on Signal Processing},
	author = {{Yujie Gu} and Leshem, A.},
	month = jul,
	year = {2012},
	pages = {3881--3885},
	file = {Gu2012.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CQDCSE7K\\Gu2012.pdf:application/pdf},
}

@article{griffiths_alternative_1982,
	title = {An alternative approach to linearly constrained adaptive beamforming},
	volume = {30},
	number = {1},
	journal = {IEEE Transactions on antennas and propagation},
	author = {Griffiths, Lloyd and Jim, C. W.},
	year = {1982},
	pages = {27--34},
	file = {griffiths1982.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\FHKC4PCT\\griffiths1982.pdf:application/pdf},
}

@article{andreopoulou_identification_2017,
	title = {Identification of perceptually relevant methods of inter-aural time difference estimation},
	volume = {142},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4996457},
	doi = {10.1121/1.4996457},
	language = {en},
	number = {2},
	urldate = {2017-11-28},
	journal = {The Journal of the Acoustical Society of America},
	author = {Andreopoulou, Areti and Katz, Brian F. G.},
	month = aug,
	year = {2017},
	pages = {588--598},
	file = {andreopoulou2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6W35DSW9\\andreopoulou2017.pdf:application/pdf},
}

@article{yan_optimal_2006,
	title = {Optimal design of {FIR} beamformer with frequency invariant patterns},
	volume = {67},
	issn = {0003682X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0003682X05001519},
	doi = {10.1016/j.apacoust.2005.09.008},
	language = {en},
	number = {6},
	urldate = {2017-11-28},
	journal = {Applied Acoustics},
	author = {Yan, Shefeng},
	month = jun,
	year = {2006},
	pages = {511--528},
	file = {yan2006.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TPYWGSV3\\yan2006.pdf:application/pdf},
}

@article{ward_theory_1995,
	title = {Theory and design of broadband sensor arrays with frequency invariant far-field beam patterns},
	volume = {97},
	number = {2},
	journal = {The Journal of Acoustical Society of America},
	author = {Ward, Darren and Kennedy, Rodney A. and Williamson, Robert C.},
	month = feb,
	year = {1995},
	pages = {1023--1034},
	file = {ward1995.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\X3KE9BAK\\ward1995.pdf:application/pdf},
}

@article{ward_fir_1996,
	title = {{FIR} filter design for frequency invariant beamformers},
	volume = {3},
	number = {3},
	journal = {IEEE Signal Processing Letters},
	author = {Ward, Darren B. and Kennedy, Rodney A. and Williamson, Robert C.},
	year = {1996},
	pages = {69--71},
	file = {ward1996.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YREN5AEZ\\ward1996.pdf:application/pdf},
}

@book{bai_acoustic_2013,
	address = {Singapore},
	title = {Acoustic array systems: theory, implementation, and application},
	isbn = {978-0-470-82723-9 978-0-470-82724-6},
	shorttitle = {Acoustic array systems},
	language = {eng},
	publisher = {Wiley},
	author = {Bai, Mingsian R. and Ih, Jeong-Guon and Benesty, Jacob},
	year = {2013},
	note = {OCLC: 934907243},
	file = {Mingsian R. Bai, Jeong_Guon Ih, Jacob Benesty(auth.)-Acoustic Array Systems_ Theory, Implementation, and Application.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IRDA6LDT\\Mingsian R. Bai, Jeong_Guon Ih, Jacob Benesty(auth.)-Acoustic Array Systems_ Theory, Implementation, and Application.pdf:application/pdf},
}

@article{poletti_beamforming_2008,
	title = {Beamforming synthesis of binaural responses from computer simulations of acoustic spaces},
	volume = {124},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2924206},
	doi = {10.1121/1.2924206},
	language = {en},
	number = {1},
	urldate = {2017-11-28},
	journal = {The Journal of the Acoustical Society of America},
	author = {Poletti, Mark A. and Svensson, U. Peter},
	month = jul,
	year = {2008},
	pages = {301--315},
	file = {poletti2008.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IB9Z7VV7\\poletti2008.pdf:application/pdf},
}

@article{bai_integrated_2015,
	title = {An integrated analysis-synthesis array system for spatial sound fields},
	volume = {137},
	number = {3},
	journal = {The Journal of the Acoustical Society of America},
	author = {Bai, Mingsian R. and Hua, Yi-Hsin and Kuo, Chia-Hao and Hsieh, Yu-Hao},
	year = {2015},
	pages = {1366--1376},
	file = {bai2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ECJBJW88\\bai2015.pdf:application/pdf},
}

@article{oreinos_objective_2015,
	title = {Objective analysis of ambisonics for hearing aid applications: {Effect} of listener's head, room reverberation, and directional microphones},
	volume = {137},
	shorttitle = {Objective analysis of ambisonics for hearing aid applications},
	number = {6},
	journal = {The Journal of the Acoustical Society of America},
	author = {Oreinos, Chris and Buchholz, Jörg M.},
	year = {2015},
	pages = {3447--3465},
	file = {oreinos2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QX7TF7G6\\oreinos2015.pdf:application/pdf},
}

@article{harder_framework_2016,
	title = {A framework for geometry acquisition, 3-{D} printing, simulation, and measurement of head-related transfer functions with a focus on hearing-assistive devices},
	volume = {75-76},
	issn = {00104485},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010448516000270},
	doi = {10.1016/j.cad.2016.02.006},
	language = {en},
	urldate = {2017-11-29},
	journal = {Computer-Aided Design},
	author = {Harder, Stine and Paulsen, Rasmus R. and Larsen, Martin and Laugesen, Søren and Mihocic, Michael and Majdak, Piotr},
	month = jun,
	year = {2016},
	keywords = {hearing aids, HRTF measurements, FEM, Scan},
	pages = {39--46},
	file = {harder2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Y8EA6X8M\\harder2016.pdf:application/pdf},
}

@phdthesis{harder_individualized_2015,
	address = {Kongens Lyngby},
	title = {Individualized directional microphone optimization in hearing aids based on reconstructing the {3D} geometry of the head and ear from {2D} images},
	school = {Technical University of Denmark (DTU)},
	author = {Harder, Stine},
	year = {2015},
	note = {OCLC: 931880689},
	file = {phd365_Harder_S_small.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6SSAQEBM\\phd365_Harder_S_small.pdf:application/pdf},
}

@article{oreinos_measurement_2013,
	title = {Measurement of {Full} {3D} {Set} of {HRTFs} for {In}-{Ear} and {Hearing} {Aid} {Microphones} on a {Head} and {Torso} {Simulator}},
	volume = {99},
	journal = {Acta Acustica united with Acustica},
	author = {Oreinos, Chris and Buchholz, Jörg M.},
	year = {2013},
	pages = {836--844},
	file = {oreinos2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\C9U7LIJP\\oreinos2013.pdf:application/pdf},
}

@phdthesis{guillon_individualisation_2009,
	address = {Le Mans, France},
	title = {Individualisation des indices spectraux pour la synthèse binaurale : recherche et exploitation des similarités inter-individuelles pour l’adaptation ou la reconstruction de {HRTF}},
	school = {Université du Maine},
	author = {Guillon, Pierre},
	year = {2009},
	file = {guillon2009phd.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VKEP9M46\\guillon2009phd.pdf:application/pdf},
}

@phdthesis{busson_individualisation_2006,
	address = {Marseille},
	title = {Individualisation d'indices acoustiques pour la synthèse binaurale},
	school = {Université de la méditerranée Aix-Marseille II},
	author = {Busson, Sylvain},
	year = {2006},
	file = {busson2006phd.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9V5DXU53\\busson2006phd.pdf:application/pdf},
}

@inproceedings{merks_design_2014,
	title = {Design of a high order binaural microphone array for hearing aids using a rigid spherical model},
	booktitle = {Acoustics, {Speech} and {Signal} {Processing} ({ICASSP}), 2014 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Merks, Ivo and Xu, Buye and Zhang, Tao},
	year = {2014},
	pages = {3650--3654},
	file = {merks2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\FB2JCL53\\merks2014.pdf:application/pdf},
}

@article{song_using_2008,
	title = {Using beamforming and binaural synthesis for the psychoacoustical evaluation of target sources in noise},
	volume = {123},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2822669},
	doi = {10.1121/1.2822669},
	language = {en},
	number = {2},
	urldate = {2017-11-29},
	journal = {The Journal of the Acoustical Society of America},
	author = {Song, Wookeun and Ellermeier, Wolfgang and Hald, Jørgen},
	month = feb,
	year = {2008},
	pages = {910--924},
	file = {song2008.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UY3SXFWA\\song2008.pdf:application/pdf},
}

@book{xie_head-related_2013,
	address = {Plantation, Fla},
	edition = {2. ed},
	title = {Head-related transfer function and virtual auditory display},
	isbn = {978-1-60427-070-9},
	language = {eng},
	publisher = {J. Ross Publ},
	author = {Xie, Bosun},
	year = {2013},
	note = {OCLC: 864582915},
	file = {Xie, Bosun-Head-related transfer function and virtual auditory display-J. Ross Publishing (2013).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VARTPLUM\\Xie, Bosun-Head-related transfer function and virtual auditory display-J. Ross Publishing (2013).pdf:application/pdf},
}

@book{huang_acoustic_2006,
	address = {Berlin},
	series = {Springer series on signals and communication technology},
	title = {Acoustic {MIMO} signal processing},
	isbn = {978-3-540-37630-9 978-0-387-27674-8},
	language = {eng},
	publisher = {Springer},
	author = {Huang, Yiteng and Benesty, Jacob and Chen, Jingdong},
	year = {2006},
	note = {OCLC: 180939042},
	file = {Jens Blauert, Ning Xiang-Acoustic MIMO Signal Processing-Springer (2006).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EMQASGW5\\Jens Blauert, Ning Xiang-Acoustic MIMO Signal Processing-Springer (2006).pdf:application/pdf},
}

@book{blauert_technology_2013,
	address = {Berlin, Heidelberg},
	title = {The {Technology} of {Binaural} {Listening}},
	isbn = {978-3-642-37761-7 978-3-642-37762-4},
	url = {http://link.springer.com/10.1007/978-3-642-37762-4},
	language = {en},
	urldate = {2017-12-06},
	publisher = {Springer Berlin Heidelberg},
	editor = {Blauert, Jens},
	year = {2013},
	doi = {10.1007/978-3-642-37762-4},
	file = {(Modern Acoustics and Signal Processing) A. Kohlrausch, J. Braasch, D. Kolossa, J. Blauert (auth.), Jens Blauert (eds.)-The Technology of Binaural Listening-Springer-Verlag Berlin Heidelberg (2013).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VWDVPEP6\\(Modern Acoustics and Signal Processing) A. Kohlrausch, J. Braasch, D. Kolossa, J. Blauert (auth.), Jens Blauert (eds.)-The Technology of Binaural Listening-Springer-Verlag Berlin Heidelberg (2013).pdf:application/pdf},
}

@book{benesty_study_2013,
	address = {Berlin, Heidelberg},
	series = {Springer {Topics} in {Signal} {Processing}},
	title = {Study and {Design} of {Differential} {Microphone} {Arrays}},
	volume = {6},
	isbn = {978-3-642-33752-9 978-3-642-33753-6},
	url = {http://link.springer.com/10.1007/978-3-642-33753-6},
	urldate = {2017-12-11},
	publisher = {Springer Berlin Heidelberg},
	author = {Benesty, Jacob and Chen, Jingdong},
	year = {2013},
	doi = {10.1007/978-3-642-33753-6},
	file = {(Springer Topics in Signal Processing 6) Jacob Benesty, Jingdong Chen (auth.)-Study and Design of Differential Microphone Arrays-Springer-Verlag Berlin Heidelberg (2013).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GDNW5KD3\\(Springer Topics in Signal Processing 6) Jacob Benesty, Jingdong Chen (auth.)-Study and Design of Differential Microphone Arrays-Springer-Verlag Berlin Heidelberg (2013).pdf:application/pdf},
}

@phdthesis{aussal_methodes_2014,
	type = {phdthesis},
	title = {Méthodes numériques pour la spatialisation sonore, de la simulation à la synthèse binaurale},
	url = {https://pastel.archives-ouvertes.fr/tel-01095801/document},
	abstract = {Les recherches effectuées lors de ce doctorat furent initiées par un besoin industriel identifié par la société Digital Media Solutions, à savoir la réalisation d'un outil de spatialisation sonore au casque destiné aux non-voyants. Malgré les nombreux dispositifs déjà existants, les techniques de son 3D sur casque butent, depuis leur apparition, sur un obstacle majeur, qui empêche la réalisation de produits de haute qualité. En effet, pour rendre l'illusion d'un espace sonore perceptible, il est nécessaire d'individualiser l'écoute pour chaque auditeur. Des mesures en laboratoire offrent des résultats convaincants, mais le processus n'est pas encore applicable au grand public. La modélisation mathématique de cette expérience permet d'envisager de remplacer la mesure par une simulation numérique. Cette thèse propose à la fois un moteur de calcul numérique efficace et adapté au problème posé, ainsi qu'un moteur de rendu audio 3D pour valider cette approche},
	language = {fr},
	urldate = {2017-09-29},
	school = {Ecole Polytechnique X},
	author = {Aussal, Matthieu},
	month = oct,
	year = {2014},
	file = {Aussal - 2014 - Méthodes numériques pour la spatialisation sonore,.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XPXCJHZB\\Aussal - 2014 - Méthodes numériques pour la spatialisation sonore,.pdf:application/pdf;Snapshot:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GI665F8M\\tel-01095801.html:text/html},
}

@article{zhang_study_2015,
	title = {Study of nonuniform linear differential microphone arrays with the minimum-norm filter},
	volume = {98},
	issn = {0003682X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0003682X15001346},
	doi = {10.1016/j.apacoust.2015.04.018},
	language = {en},
	urldate = {2017-12-12},
	journal = {Applied Acoustics},
	author = {Zhang, Hao and Chen, Jingdong and Benesty, Jacob},
	month = nov,
	year = {2015},
	pages = {62--69},
	file = {zhang2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CHKTW38U\\zhang2015.pdf:application/pdf},
}

@article{widrow_microphone_2003,
	title = {Microphone arrays for hearing aids: {An} overview},
	volume = {39},
	shorttitle = {Microphone arrays for hearing aids},
	number = {1},
	journal = {Speech Communication},
	author = {Widrow, Bernard and Luo, Fa-Long},
	year = {2003},
	pages = {139--146},
	file = {widrow2003.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QHU2SSMQ\\widrow2003.pdf:application/pdf},
}

@article{meyer_beamforming_2001,
	title = {Beamforming for a circular microphone array mounted on spherically shaped objects},
	volume = {109},
	issn = {0001-4966},
	url = {http://scitation.aip.org/content/asa/journal/jasa/109/1/10.1121/1.1329616},
	doi = {10.1121/1.1329616},
	language = {en},
	number = {1},
	urldate = {2017-12-12},
	journal = {The Journal of the Acoustical Society of America},
	author = {Meyer, Jens},
	month = jan,
	year = {2001},
	pages = {185--193},
	file = {Meyer2001.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YRC98396\\Meyer2001.pdf:application/pdf},
}

@book{benesty_fundamentals_2016,
	address = {Singapore},
	series = {{SpringerBriefs} in {Electrical} and {Computer} {Engineering}},
	title = {Fundamentals of {Differential} {Beamforming}},
	isbn = {978-981-10-1045-3 978-981-10-1046-0},
	url = {http://link.springer.com/10.1007/978-981-10-1046-0},
	urldate = {2017-12-12},
	publisher = {Springer Singapore},
	author = {Benesty, Jacob and Chen, Jingdong and Pan, Chao},
	year = {2016},
	doi = {10.1007/978-981-10-1046-0},
	file = {Benesty, Jacob\; Chen, Jingdong\; Pan, Chao -- [SpringerBriefs in Electrical and Computer Engineering] Fundamentals of Differential Beamforming __.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\8K5KPHHH\\Benesty, Jacob\; Chen, Jingdong\; Pan, Chao -- [SpringerBriefs in Electrical and Computer Engineering] Fundamentals of Differential Beamforming __.pdf:application/pdf},
}

@article{huang_design_2017,
	title = {Design of robust concentric circular differential microphone arrays},
	volume = {141},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4983122},
	doi = {10.1121/1.4983122},
	language = {en},
	number = {5},
	urldate = {2017-12-12},
	journal = {The Journal of the Acoustical Society of America},
	author = {Huang, Gongping and Benesty, Jacob and Chen, Jingdong},
	month = may,
	year = {2017},
	pages = {3236--3249},
	file = {huang2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XUR7ZANB\\huang2017.pdf:application/pdf},
}

@article{tiana-roig_beamforming_2011,
	title = {Beamforming with a circular array of microphones mounted on a rigid sphere ({L})},
	volume = {130},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3621294},
	doi = {10.1121/1.3621294},
	language = {en},
	number = {3},
	urldate = {2017-12-12},
	journal = {The Journal of the Acoustical Society of America},
	author = {Tiana-Roig, Elisabet and Jacobsen, Finn and Fernandez-Grande, Efren},
	month = sep,
	year = {2011},
	pages = {1095--1098},
	file = {tiana_roig2011.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\P3TTILT7\\tiana_roig2011.pdf:application/pdf},
}

@patent{dedieu_method_2007,
	title = {Method of broadband constant directivity beamforming for non linear and non axi-symmetric sensor arrays embedded in an obstacle},
	author = {Dedieu, Stéphane and Moquin, Philippe},
	month = sep,
	year = {2007},
	file = {dedieu2007brevet.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WCVEFR8F\\dedieu2007brevet.pdf:application/pdf},
}

@patent{reiter_hybrid_1997,
	title = {Hybrid behind-the-ear and completely-in-canal hearing aid},
	nationality = {US},
	assignee = {Siemens Hearing Instruments, Inc.},
	number = {5606621},
	author = {Reiter, James J. and Berkholcs, Gordon},
	month = feb,
	year = {1997},
	file = {US5606621.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YY43Q9AX\\US5606621.pdf:application/pdf},
}

@article{swaminathan_role_2016,
	title = {Role of {Binaural} {Temporal} {Fine} {Structure} and {Envelope} {Cues} in {Cocktail}-{Party} {Listening}},
	volume = {36},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.4421-15.2016},
	doi = {10.1523/JNEUROSCI.4421-15.2016},
	language = {en},
	number = {31},
	urldate = {2017-12-15},
	journal = {Journal of Neuroscience},
	author = {Swaminathan, J. and Mason, C. R. and Streeter, T. M. and Best, V. and Roverud, E. and Kidd, G.},
	month = aug,
	year = {2016},
	pages = {8250--8257},
	file = {swaminathan2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EK2B5H52\\swaminathan2016.pdf:application/pdf},
}

@article{naylor_long-term_2009,
	title = {Long-{Term} {Signal}-to-{Noise} {Ratio} at the {Input} and {Output} of {Amplitude}-{Compression} {Systems}},
	volume = {20},
	issn = {10500545},
	url = {http://openurl.ingenta.com/content/xref?genre=article&issn=1050-0545&volume=20&issue=3&spage=161},
	doi = {10.3766/jaaa.20.3.2},
	language = {en},
	number = {3},
	urldate = {2017-12-15},
	journal = {Journal of the American Academy of Audiology},
	author = {Naylor, Graham and Johannesson, René Burmand},
	month = mar,
	year = {2009},
	pages = {161--171},
	file = {naylor2009.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\S94JJEIW\\naylor2009.pdf:application/pdf},
}

@article{neher_speech_2017,
	title = {Speech reception with different bilateral directional processing schemes: {Influence} of binaural hearing, audiometric asymmetry, and acoustic scenario},
	volume = {353},
	issn = {03785955},
	shorttitle = {Speech reception with different bilateral directional processing schemes},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0378595517301193},
	doi = {10.1016/j.heares.2017.07.014},
	language = {en},
	urldate = {2017-12-15},
	journal = {Hearing Research},
	author = {Neher, Tobias and Wagener, Kirsten C. and Latzel, Matthias},
	month = sep,
	year = {2017},
	pages = {36--48},
	file = {neher2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\J2RVJ56M\\neher2017.pdf:application/pdf},
}

@article{neher_can_2011,
	title = {Can basic auditory and cognitive measures predict hearing-impaired listeners’ localization and spatial speech recognition abilities? a},
	volume = {130},
	shorttitle = {Can basic auditory and cognitive measures predict hearing-impaired listeners’ localization and spatial speech recognition abilities?},
	number = {3},
	journal = {The Journal of the Acoustical Society of America},
	author = {Neher, Tobias and Laugesen, Søren and Søgaard Jensen, Niels and Kragelund, Louise},
	year = {2011},
	pages = {1542--1558},
	file = {neher2011.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\SD3G2MSS\\neher2011.pdf:application/pdf},
}

@article{neher_benefit_2009,
	title = {Benefit from spatial separation of multiple talkers in bilateral hearing-aid users: {Effects} of hearing loss, age, and cognition},
	volume = {48},
	issn = {1499-2027, 1708-8186},
	shorttitle = {Benefit from spatial separation of multiple talkers in bilateral hearing-aid users},
	url = {http://www.tandfonline.com/doi/full/10.3109/14992020903079332},
	doi = {10.3109/14992020903079332},
	language = {en},
	number = {11},
	urldate = {2017-12-15},
	journal = {International Journal of Audiology},
	author = {Neher, Tobias and Behrens, Thomas and Carlile, Simon and Jin, Craig and Kragelund, Louise and Petersen, Anne Specht and Schaik, André van},
	month = jan,
	year = {2009},
	pages = {758--774},
	file = {neher2009.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4H5VK2X2\\neher2009.pdf:application/pdf},
}

@article{best_use_2017,
	title = {Use of a glimpsing model to understand the performance of listeners with and without hearing loss in spatialized speech mixtures},
	volume = {141},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4973620},
	doi = {10.1121/1.4973620},
	language = {en},
	number = {1},
	urldate = {2017-12-15},
	journal = {The Journal of the Acoustical Society of America},
	author = {Best, Virginia and Mason, Christine R. and Swaminathan, Jayaganesh and Roverud, Elin and Kidd, Gerald},
	month = jan,
	year = {2017},
	pages = {81--91},
	file = {Best et al. - 2017 - Use of a glimpsing model to understand the perform.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NICK643E\\Best et al. - 2017 - Use of a glimpsing model to understand the perform.pdf:application/pdf},
}

@phdthesis{desloge_location-estimating_1998,
	title = {The {Location}-{Estimating}, {Null}-{Steering} ({LENS}) {Algorithm} for {Adaptive} {Microphone}-{Array} {Processing}},
	school = {Massachusetts Institute of Technology},
	author = {Desloge, Joseph G.},
	month = sep,
	year = {1998},
	file = {desloge1998phd.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LM3746G2\\desloge1998phd.pdf:application/pdf},
}

@patent{desloge_location-estimating_1999,
	title = {The {Location}-{Estimating}, {Null}-{Steering} ({LENS}) {Algorithm} for {Adaptive} {Array} {Processing}},
	assignee = {Massachusetts Institute of Technology},
	number = {7254199},
	author = {Desloge, Joseph G.},
	month = sep,
	year = {1999},
	file = {US7254199.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5NF6E8E6\\US7254199.pdf:application/pdf},
}

@article{glyde_effect_2015,
	title = {Effect of audibility on spatial release from speech-on-speech masking},
	volume = {138},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4934732},
	doi = {10.1121/1.4934732},
	language = {en},
	number = {5},
	urldate = {2017-12-19},
	journal = {The Journal of the Acoustical Society of America},
	author = {Glyde, Helen and Buchholz, Jörg M. and Nielsen, Lillian and Best, Virginia and Dillon, Harvey and Cameron, Sharon and Hickson, Louise},
	month = nov,
	year = {2015},
	pages = {3311--3319},
	file = {Glyde et al. - 2015 - Effect of audibility on spatial release from speec.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\THBXQDQP\\Glyde et al. - 2015 - Effect of audibility on spatial release from speec.pdf:application/pdf;glyde2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AB9HM2HZ\\glyde2015.pdf:application/pdf},
}

@article{glyde_effects_2013,
	title = {The effects of hearing impairment and aging on spatial processing},
	volume = {34},
	number = {1},
	journal = {Ear and hearing},
	author = {Glyde, Helen and Cameron, Sharon and Dillon, Harvey and Hickson, Louise and Seeto, Mark},
	year = {2013},
	pages = {15--28},
	file = {glyde2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HFDSEARV\\glyde2013.pdf:application/pdf},
}

@article{doclo_superdirective_2007,
	title = {Superdirective {Beamforming} {Robust} {Against} {Microphone} {Mismatch}},
	volume = {15},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/4067029/},
	doi = {10.1109/TASL.2006.881676},
	number = {2},
	urldate = {2017-12-19},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	author = {Doclo, Simon and Moonen, Marc},
	month = feb,
	year = {2007},
	pages = {617--631},
	file = {doclo2007.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3Z4RD76G\\doclo2007.pdf:application/pdf},
}

@article{doclo_design_2003,
	title = {Design of broadband beamformers robust against gain and phase errors in the microphone array characteristics},
	volume = {51},
	issn = {1053-587X},
	url = {http://ieeexplore.ieee.org/document/1232319/},
	doi = {10.1109/TSP.2003.816885},
	language = {en},
	number = {10},
	urldate = {2017-12-19},
	journal = {IEEE Transactions on Signal Processing},
	author = {Doclo, S. and Moonen, M.},
	month = oct,
	year = {2003},
	pages = {2511--2526},
	file = {doclo2003.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JF7ZJIHQ\\doclo2003.pdf:application/pdf},
}

@article{derkx_theoretical_2009,
	title = {Theoretical {Analysis} of a {First}-{Order} {Azimuth}-{Steerable} {Superdirective} {Microphone} {Array}},
	volume = {17},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/4740155/},
	doi = {10.1109/TASL.2008.2006583},
	number = {1},
	urldate = {2017-12-19},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Derkx, RenÉ M. M. and Janse, Kees},
	month = jan,
	year = {2009},
	pages = {150--162},
	file = {derkx2009.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\P9AL2VPD\\derkx2009.pdf:application/pdf},
}

@article{azarpour_binaural_2017,
	title = {Binaural noise reduction via cue-preserving {MMSE} filter and adaptive-blocking-based noise {PSD} estimation},
	volume = {2017},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-017-0485-9},
	doi = {10.1186/s13634-017-0485-9},
	language = {en},
	number = {1},
	urldate = {2017-12-19},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Azarpour, Masoumeh and Enzner, Gerald},
	month = dec,
	year = {2017},
	file = {azarpour2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9FLRXNT2\\azarpour2017.pdf:application/pdf},
}

@inproceedings{han_development_2013,
	title = {Development of a speech-distortionless beamformer for two-microphone digital hearing aids},
	booktitle = {Consumer {Electronics} ({ICCE}), 2013 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Han, Jonghee and Cho, Kyeongwon and Kim, In Young and Hong, Sung Hwa and Kim, Dong Wook},
	year = {2013},
	pages = {358--359},
	file = {han2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PF4ZWT58\\han2013.pdf:application/pdf},
}

@article{okuno_listening_1999,
	title = {Listening to two simultaneous speeches},
	volume = {27},
	number = {3},
	journal = {Speech communication},
	author = {Okuno, Hiroshi G. and Nakatani, Tomohiro and Kawabata, Takeshi},
	year = {1999},
	pages = {299--310},
	file = {okuno1999.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IHNQRS72\\okuno1999.pdf:application/pdf},
}

@article{stadler_potential_1993,
	title = {On the potential of fixed arrays for hearing aids},
	volume = {94},
	number = {3},
	journal = {The Journal of Acoustical Society of America},
	author = {Stadler, Robert W. and Rabinowitz, William M.},
	month = sep,
	year = {1993},
	pages = {1332--1342},
	file = {stadler1993.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\87VH68CI\\stadler1993.pdf:application/pdf},
}

@article{saunders_speech_1997,
	title = {Speech intelligibility enhancement using hearing-aid array processing},
	volume = {102},
	number = {3},
	journal = {The Journal of the Acoustical Society of America},
	author = {Saunders, Gabrielle H. and Kates, James M.},
	year = {1997},
	pages = {1827--1837},
	file = {saunders1997.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HU6V6LCP\\saunders1997.pdf:application/pdf},
}

@article{soede_development_1993,
	title = {Development of a directional hearing instrument based on array technology},
	volume = {94},
	number = {2},
	journal = {The Journal of Acoustical Society of America},
	author = {Soede, Wim and Berkhout, Augustinus J. and Bilsen, Frans A.},
	month = aug,
	year = {1993},
	file = {soede1993.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\FWIN4TB3\\soede1993.pdf:application/pdf},
}

@article{debrunner_directional_1995,
	title = {A directional adaptive least-mean-square acoustic array for hearing aid enhancement},
	volume = {98},
	number = {1},
	journal = {The Journal of Acoustical Society of America},
	author = {DeBrunner, Victor E. and McKinney, Elaine D.},
	month = jul,
	year = {1995},
	pages = {437--444},
	file = {debrunner1995.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WMCFFF34\\debrunner1995.pdf:application/pdf},
}

@article{nishimura_array_2004,
	title = {Array signal processing with two outputs preserving binaural information},
	volume = {65},
	issn = {0003682X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0003682X04000131},
	doi = {10.1016/j.apacoust.2004.01.002},
	language = {en},
	number = {7},
	urldate = {2017-12-19},
	journal = {Applied Acoustics},
	author = {Nishimura, Ryouichi and Suzuki, Yôiti and Tsukui, Shinji and Asano, Futoshi},
	month = jul,
	year = {2004},
	pages = {657--672},
	file = {nishimura2004.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IGLW4SDH\\nishimura2004.pdf:application/pdf},
}

@article{hafizovic_design_2012,
	title = {Design and implementation of a {MEMS} microphone array system for real-time speech acquisition},
	volume = {73},
	issn = {0003682X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0003682X11002192},
	doi = {10.1016/j.apacoust.2011.07.009},
	language = {en},
	number = {2},
	urldate = {2017-12-19},
	journal = {Applied Acoustics},
	author = {Hafizovic, Ines and Nilsen, Carl-Inge Colombo and Kjølerbakken, Morgan and Jahr, Vibeke},
	month = feb,
	year = {2012},
	pages = {132--143},
	file = {hafizovic2012.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QZT37E78\\hafizovic2012.pdf:application/pdf},
}

@article{katz_comparative_2014,
	title = {A comparative study of interaural time delay estimation methods},
	volume = {135},
	number = {6},
	journal = {The Journal of the Acoustical Society of America},
	author = {Katz, Brian FG and Noisternig, Markus},
	year = {2014},
	pages = {3530--3540},
	file = {katz2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GJYI5IX2\\katz2014.pdf:application/pdf},
}

@article{aaronson_testing_2014,
	title = {Testing, correcting, and extending the {Woodworth} model for interaural time difference},
	volume = {135},
	number = {2},
	journal = {The Journal of the Acoustical Society of America},
	author = {Aaronson, Neil L. and Hartmann, William M.},
	year = {2014},
	pages = {817--823},
	file = {aaronson2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ET2675Y8\\aaronson2014.pdf:application/pdf},
}

@article{matsumura_orthogonal_2011,
	title = {Orthogonal beamforming using {Gram}-{Schmidt} orthogonalization for multi-user {MIMO} downlink system},
	volume = {2011},
	number = {1},
	journal = {EURASIP Journal on Wireless Communications and Networking},
	author = {Matsumura, Kunitaka and Ohtsuki, Tomoaki},
	year = {2011},
	pages = {41},
	file = {matsumura2011.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AQJ46AQI\\matsumura2011.pdf:application/pdf},
}

@inproceedings{laitinen_binaural_2009,
	title = {Binaural reproduction for directional audio coding},
	booktitle = {Applications of {Signal} {Processing} to {Audio} and {Acoustics}, 2009. {WASPAA}'09. {IEEE} {Workshop} on},
	publisher = {IEEE},
	author = {Laitinen, Mikko-Ville and Pulkki, Ville},
	year = {2009},
	pages = {337--340},
	file = {laitinen2009.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\68KHV84Q\\laitinen2009.pdf:application/pdf},
}

@inproceedings{woods_real-world_2015,
	title = {A real-world recording database for ad hoc microphone arrays},
	booktitle = {Applications of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA}), 2015 {IEEE} {Workshop} on},
	publisher = {IEEE},
	author = {Woods, William S. and Hadad, Elior and Merks, Ivo and Xu, Buye and Gannot, Sharon and Zhang, Tao},
	year = {2015},
	pages = {1--5},
	file = {woods2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GTZ8UQXQ\\woods2015.pdf:application/pdf},
}

@article{durin_acoustic_2014,
	title = {Acoustic analysis of the directional information captured by five different hearing aid styles},
	volume = {136},
	number = {2},
	journal = {The Journal of the Acoustical Society of America},
	author = {Durin, Virginie and Carlile, Simon and Guillon, Pierre and Best, Virginia and Kalluri, Sridhar},
	year = {2014},
	pages = {818--828},
	file = {durin2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2LT7HGHR\\durin2014.pdf:application/pdf},
}

@inproceedings{schimmel_fast_2009,
	title = {A fast and accurate “shoebox” room acoustics simulator},
	booktitle = {Acoustics, {Speech} and {Signal} {Processing}, 2009. {ICASSP} 2009. {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Schimmel, Steven M. and Muller, Martin F. and Dillier, Norbert},
	year = {2009},
	pages = {241--244},
	file = {schimmel2009.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NFXVGDEQ\\schimmel2009.pdf:application/pdf},
}

@phdthesis{rohdenburg_development_2008,
	title = {Development and objective perceptual quality assessment of monaural and binaural noise reduction schemes for hearing aids},
	school = {Universität Oldenburg},
	author = {Rohdenburg, Thomas},
	year = {2008},
	file = {rohdev08.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RVDL9Z9L\\rohdev08.pdf:application/pdf},
}

@inproceedings{marschall_robustness_2012,
	title = {Robustness of a mixed-order {Ambisonics} microphone array for sound field reproduction},
	booktitle = {Audio {Engineering} {Society} {Convention} 132},
	publisher = {Audio Engineering Society},
	author = {Marschall, Márton and Favrot, Sylvain and Buchholz, Jörg},
	year = {2012},
	file = {marschall2012.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HYJLAXGE\\marschall2012.pdf:application/pdf},
}

@inproceedings{favrot_mixed-order_2011,
	title = {Mixed-order {Ambisonics} recording and playback for improving horizontal directionality},
	booktitle = {Audio {Engineering} {Society} {Convention} 131},
	publisher = {Audio Engineering Society},
	author = {Favrot, Sylvain and Marschall, Marton and Käsbach, Johannes and Buchholz, Jörg and Weller, Tobias},
	year = {2011},
	file = {favrot2011.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BKXJEKS8\\favrot2011.pdf:application/pdf},
}

@inproceedings{weller_frequency_2014,
	title = {Frequency dependent regularization of a mixed-order {Ambisonics} encoding system using psychoacoustically motivated metrics},
	booktitle = {Audio {Engineering} {Society} {Conference}: 55th {International} {Conference}: {Spatial} {Audio}},
	publisher = {Audio Engineering Society},
	author = {Weller, Tobias and Buchholz, Jörg M. and Oreinos, Chris},
	year = {2014},
	file = {weller2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3V2ST4VK\\weller2014.pdf:application/pdf},
}

@article{pausch_real-time_nodate,
	title = {Real-time acoustic scene rendering to investigate auditory and cognitive aspects of hearing impaired subjects},
	author = {Pausch, Florian and Aspöck, Lukas},
	file = {Pausch2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UUBQ6LJN\\Pausch2014.pdf:application/pdf;Pausch2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9MNICSR8\\Pausch2014.pdf:application/pdf},
}

@article{minnaar_improving_2010,
	title = {Improving hearing aids through listening tests in a virtual sound environment},
	volume = {63},
	number = {10},
	journal = {The Hearing Journal},
	author = {Minnaar, Pauli and Favrot, Sylvain and Buchholz, Jörg M.},
	year = {2010},
	pages = {40--42},
	file = {minnaar2010.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WYQG9MW3\\minnaar2010.pdf:application/pdf},
}

@inproceedings{minnaar_simulating_2011,
	title = {Simulating complex listening environments in the laboratory for testing hearing aids},
	booktitle = {Proceedings of {Forum} {Acusticum}},
	author = {Minnaar, Pauli and Breitsprecher, Claudia and Holmberg, Marcus},
	year = {2011},
	file = {Minaar2011.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Y68BTWK2\\Minaar2011.pdf:application/pdf},
}

@article{cubick_validation_2016,
	title = {Validation of a {Virtual} {Sound} {Environment} {System} for {Testing} {Hearing} {Aids}},
	volume = {102},
	issn = {16101928},
	url = {http://openurl.ingenta.com/content/xref?genre=article&issn=1610-1928&volume=102&issue=3&spage=547},
	doi = {10.3813/AAA.918972},
	language = {en},
	number = {3},
	urldate = {2017-12-21},
	journal = {Acta Acustica united with Acustica},
	author = {Cubick, J. and Dau, T.},
	month = may,
	year = {2016},
	pages = {547--557},
	file = {Cubick2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\S2TA2DYU\\Cubick2016.pdf:application/pdf},
}

@article{vorlander_sound_2011,
	title = {Sound fields in complex listening environments},
	volume = {15},
	number = {3},
	journal = {Trends in amplification},
	author = {Vorländer, Michael},
	year = {2011},
	pages = {106--115},
	file = {vorlander2011.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PICUAXDV\\vorlander2011.pdf:application/pdf},
}

@inproceedings{fels_robustness_2004,
	address = {Strasbourg, France},
	title = {Robustness of a mixed-order {Ambisonics} microphone array for sound field reproduction},
	booktitle = {{CFA}/{DAGA}},
	author = {Fels, Janina},
	month = mar,
	year = {2004},
	pages = {937--938},
	file = {fels2004.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9LQ388H4\\fels2004.pdf:application/pdf},
}

@article{kayser_database_2009,
	title = {Database of {Multichannel} {In}-{Ear} and {Behind}-the-{Ear} {Head}-{Related} and {Binaural} {Room} {Impulse} {Responses}},
	volume = {2009},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/2009/298605},
	doi = {10.1155/2009/298605},
	language = {en},
	number = {1},
	urldate = {2017-12-21},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Kayser, H. and Ewert, S. D. and Anemüller, J. and Rohdenburg, T. and Hohmann, V. and Kollmeier, B.},
	month = dec,
	year = {2009},
	pages = {10},
	file = {kayser2009.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\J6FH752Q\\kayser2009.pdf:application/pdf},
}

@inproceedings{rohdenburg_objective_2008,
	title = {Objective perceptual quality assessment for self-steering binaural hearing aid microphone arrays},
	booktitle = {Acoustics, {Speech} and {Signal} {Processing}, 2008. {ICASSP} 2008. {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Rohdenburg, Thomas and Goetze, Stefan and Hohmann, Volker and Kammeyer, Karl-Dirk and Kollmeier, Birger},
	year = {2008},
	pages = {2449--2452},
	file = {Rohdenburg2008.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YR4QZBXJ\\Rohdenburg2008.pdf:application/pdf},
}

@article{baskent_cognitive_2016,
	title = {Cognitive {Compensation} of {Speech} {Perception} {With} {Hearing} {Impairment}, {Cochlear} {Implants}, and {Aging}: {How} and to {What} {Degree} {Can} {It} {Be} {Achieved}?},
	volume = {20},
	issn = {2331-2165, 2331-2165},
	shorttitle = {Cognitive {Compensation} of {Speech} {Perception} {With} {Hearing} {Impairment}, {Cochlear} {Implants}, and {Aging}},
	url = {http://journals.sagepub.com/doi/10.1177/2331216516670279},
	doi = {10.1177/2331216516670279},
	language = {en},
	urldate = {2017-12-21},
	journal = {Trends in Hearing},
	author = {Başkent, Deniz and Clarke, Jeanne and Pals, Carina and Benard, Michel R. and Bhargava, Pranesh and Saija, Jefta and Sarampalis, Anastasios and Wagner, Anita and Gaudrain, Etienne},
	month = sep,
	year = {2016},
	pages = {233121651667027},
	file = {baskent2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NE2YR42L\\baskent2016.pdf:application/pdf},
}

@article{shinn-cunningham_bottom-up_2005,
	title = {Bottom-up and {Top}-down {Influences} on {Spatial} {Unmasking}},
	volume = {91},
	journal = {Acta Acustica united with Acustica},
	author = {Shinn-Cunningham, Barbara and Ihlefeld, Antje and Satyavarta, Satyavarta and Larson, Eric},
	year = {2005},
	pages = {967--979},
	file = {Acustica05_Shinn.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\FWEBL9X8\\Acustica05_Shinn.pdf:application/pdf},
}

@article{akeroyd_across_2004,
	title = {The across frequency independence of equalization of interaural time delay in the equalization-cancellation model of binaural unmasking},
	volume = {116},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1768959},
	doi = {10.1121/1.1768959},
	language = {en},
	number = {2},
	urldate = {2017-12-22},
	journal = {The Journal of the Acoustical Society of America},
	author = {Akeroyd, Michael A.},
	month = aug,
	year = {2004},
	pages = {1135--1148},
}

@article{gygi_spatial_2014,
	title = {Spatial and temporal modifications of multitalker speech can improve speech perception in older adults},
	volume = {310},
	issn = {03785955},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0378595514000173},
	doi = {10.1016/j.heares.2014.01.009},
	language = {en},
	urldate = {2017-12-22},
	journal = {Hearing Research},
	author = {Gygi, Brian and Shafiro, Valeriy},
	month = apr,
	year = {2014},
	pages = {76--86},
	file = {gygi2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UBS4WUZ9\\gygi2014.pdf:application/pdf},
}

@article{kong_auditory_2014,
	title = {Auditory {Spatial} {Attention} {Representations} in the {Human} {Cerebral} {Cortex}},
	volume = {24},
	issn = {1047-3211, 1460-2199},
	url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/bhs359},
	doi = {10.1093/cercor/bhs359},
	language = {en},
	number = {3},
	urldate = {2017-12-22},
	journal = {Cerebral Cortex},
	author = {Kong, L. and Michalka, S. W. and Rosen, M. L. and Sheremata, S. L. and Swisher, J. D. and Shinn-Cunningham, B. G. and Somers, D. C.},
	month = mar,
	year = {2014},
	pages = {773--784},
	file = {kong2012.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3MD3M7TR\\kong2012.pdf:application/pdf},
}

@article{lavandier_binaural_2012,
	title = {Binaural prediction of speech intelligibility in reverberant rooms with multiple noise sources},
	volume = {131},
	number = {1},
	journal = {The Journal of the Acoustical Society of America},
	author = {Lavandier, Mathieu and Jelfs, Sam and Culling, John F. and Watkins, Anthony J. and Raimond, Andrew P. and Makin, Simon J.},
	year = {2012},
	pages = {218--231},
	file = {lavandier2012.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2ZMDSSMK\\lavandier2012.pdf:application/pdf},
}

@article{marrone_evaluating_2008,
	title = {Evaluating the {Benefit} of {Hearing} {Aids} in {Solving} the {Cocktail} {Party} {Problem}},
	volume = {12},
	issn = {1084-7138},
	url = {http://journals.sagepub.com/doi/10.1177/1084713808325880},
	doi = {10.1177/1084713808325880},
	language = {en},
	number = {4},
	urldate = {2017-12-22},
	journal = {Trends in Amplification},
	author = {Marrone, Nicole and Mason, Christine R. and Kidd, Gerald},
	month = dec,
	year = {2008},
	pages = {300--315},
	file = {marrone2008.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IK2P9J9Q\\marrone2008.pdf:application/pdf},
}

@article{ihlefeld_disentangling_2008,
	title = {Disentangling the effects of spatial cues on selection and formation of auditory objects},
	volume = {124},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2973185},
	doi = {10.1121/1.2973185},
	language = {en},
	number = {4},
	urldate = {2017-12-22},
	journal = {The Journal of the Acoustical Society of America},
	author = {Ihlefeld, Antje and Shinn-Cunningham, Barbara},
	month = oct,
	year = {2008},
	pages = {2224--2235},
	file = {JASA08_Ihlefeld_Disentanleg.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\F5Z6UAQW\\JASA08_Ihlefeld_Disentanleg.pdf:application/pdf},
}

@article{majdak_3-d_2010,
	title = {3-{D} localization of virtual sound sources: {Effects} of visual environment, pointing method, and training},
	volume = {72},
	issn = {1943-3921, 1943-393X},
	shorttitle = {3-{D} localization of virtual sound sources},
	url = {http://www.springerlink.com/index/10.3758/APP.72.2.454},
	doi = {10.3758/APP.72.2.454},
	language = {en},
	number = {2},
	urldate = {2017-12-22},
	journal = {Attention, Perception, \& Psychophysics},
	author = {Majdak, Piotr and Goupell, Matthew J. and Laback, Bernhard},
	month = feb,
	year = {2010},
	pages = {454--469},
	file = {majdak2010.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YJV7FU47\\majdak2010.pdf:application/pdf},
}

@inproceedings{lehmann_reverberation-time_2007,
	title = {Reverberation-time prediction method for room impulse responses simulated with the image-source model},
	booktitle = {Applications of {Signal} {Processing} to {Audio} and {Acoustics}, 2007 {IEEE} {Workshop} on},
	publisher = {IEEE},
	author = {Lehmann, Eric A. and Johansson, Anders M. and Nordholm, Sven},
	year = {2007},
	pages = {159--162},
	file = {lehmann2007.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5GAL9JP8\\lehmann2007.pdf:application/pdf},
}

@article{lehmann_prediction_2008,
	title = {Prediction of energy decay in room impulse responses simulated with an image-source model},
	volume = {124},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2936367},
	doi = {10.1121/1.2936367},
	language = {en},
	number = {1},
	urldate = {2017-12-22},
	journal = {The Journal of the Acoustical Society of America},
	author = {Lehmann, Eric A. and Johansson, Anders M.},
	month = jul,
	year = {2008},
	pages = {269--277},
	file = {lehmann2008.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Q7KEDSA7\\lehmann2008.pdf:application/pdf},
}

@article{allen_image_1979,
	title = {Image method for efficiently simulating small-room acoustics},
	volume = {65},
	number = {4},
	journal = {The Journal of Acoustical Society of America},
	author = {Allen, Jont B. and Berkley, David A.},
	month = apr,
	year = {1979},
	pages = {943--950},
	file = {allen1979.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\75L8R839\\allen1979.pdf:application/pdf},
}

@inproceedings{carpentier_measurement_2014,
	title = {Measurement of a head-related transfer function database with high spatial resolution},
	booktitle = {7th {Forum} {Acusticum} ({EAA})},
	author = {Carpentier, Thibaut and Bahu, Hélène and Noisternig, Markus and Warusfel, Olivier},
	year = {2014},
	file = {carpentier2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZW4ZC653\\carpentier2014.pdf:application/pdf},
}

@article{duda_range_1998,
	title = {Range dependence of the response of a spherical head model},
	volume = {104},
	number = {5},
	journal = {The Journal of the Acoustical Society of America},
	author = {Duda, Richard O. and Martens, William L.},
	year = {1998},
	pages = {3048--3058},
	file = {duda1998.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AB42NTHQ\\duda1998.pdf:application/pdf},
}

@article{brown_structural_1998,
	title = {A structural model for binaural sound synthesis},
	volume = {6},
	number = {5},
	journal = {IEEE transactions on speech and audio processing},
	author = {Brown, C. Phillip and Duda, Richard O.},
	year = {1998},
	pages = {476--488},
	file = {Brown1998.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WGK9NJEZ\\Brown1998.pdf:application/pdf},
}

@inproceedings{hoene_mysofadesign_2017,
	title = {{MySofa}—{Design} {Your} {Personal} {HRTF}},
	booktitle = {Audio {Engineering} {Society} {Convention} 142},
	publisher = {Audio Engineering Society},
	author = {Hoene, Christian and Patino Mejia, Isabel C. and Cacerovschi, Alexandru},
	year = {2017},
	file = {hoene2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IBPA8XBD\\hoene2017.pdf:application/pdf},
}

@article{peterson_simulating_1986,
	title = {Simulating the response of multiple microphones to a single acoustic source in a reverberant room},
	volume = {80},
	number = {5},
	journal = {The Journal of Acoustical Society of America},
	author = {Peterson, Patrick M.},
	month = nov,
	year = {1986},
	pages = {1527--1529},
	file = {peterson1986.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\522TD5AE\\peterson1986.pdf:application/pdf},
}

@inproceedings{kyriakakis_virtual_2000,
	title = {Virtual microphones and virtual loudspeakers for multichannel audio},
	booktitle = {Consumer {Electronics}, 2000. {ICCE}. 2000 {Digest} of {Technical} {Papers}. {International} {Conference} on},
	publisher = {IEEE},
	author = {Kyriakakis, C.},
	year = {2000},
	pages = {404--405},
	file = {Kyriakakis - 2000 - Virtual microphones and virtual loudspeakers for m.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5HMYFEXM\\Kyriakakis - 2000 - Virtual microphones and virtual loudspeakers for m.pdf:application/pdf},
}

@inproceedings{harder_three_2013,
	title = {A three dimensional children head database for acoustical research and development},
	url = {http://asa.scitation.org/doi/abs/10.1121/1.4800159},
	doi = {10.1121/1.4800159},
	urldate = {2018-01-10},
	booktitle = {Acoustical {Society} of {America}, 050013},
	author = {Harder, Stine and Paulsen, Rasmus R. and Larsen, Martin and Laugesen, So̸ren},
	year = {2013},
	pages = {050013--050013},
	file = {harder2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TRAVQRY6\\harder2013.pdf:application/pdf},
}

@article{kuhn_model_1977,
	title = {Model for the interaural time differences in the azimuthal plane},
	volume = {62},
	number = {1},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kuhn, George F.},
	year = {1977},
	pages = {157--167},
	file = {kuhn1977.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2QQLZLBD\\kuhn1977.pdf:application/pdf},
}

@article{hausler_sound_1983,
	title = {Sound {Localization} in {Subjects} with {Impaired} {Hearing}},
	volume = {400},
	journal = {Acta Oto-laryngologica},
	author = {Häusler, R. and Colburn, S. and Marr, E.},
	year = {1983},
	file = {husler1983.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\DJ5XV78M\\husler1983.pdf:application/pdf},
}

@inproceedings{gosling_performance_2017,
	title = {Performance analysis of the extended binaural {MVDR} beamformer with partial noise estimation in a homogeneous noise field},
	booktitle = {Hands-free {Speech} {Communications} and {Microphone} {Arrays} ({HSCMA}), 2017},
	publisher = {IEEE},
	author = {Gößling, Nico and Marquardt, Daniel and Doclo, Simon},
	year = {2017},
	pages = {1--5},
	file = {gobling2017.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7CUKDI4U\\gobling2017.pdf:application/pdf},
}

@inproceedings{hadad_generalized_2016,
	title = {A generalized binaural {MVDR} beamformer with interferer relative transfer function preservation},
	booktitle = {Signal {Processing} {Conference} ({EUSIPCO}), 2016 24th {European}},
	publisher = {IEEE},
	author = {Hadad, Elior and Doclo, Simon and Gannot, Sharon},
	year = {2016},
	pages = {1643--1647},
	file = {hadad2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9S535TFT\\hadad2016.pdf:application/pdf},
}

@inproceedings{markovich-golan_reduced_2010,
	title = {A reduced bandwidth binaural {MVDR} beamformer},
	booktitle = {Proc. of the {International} {Workshop} on {Acoustic} {Echo} and {Noise} {Control} ({IWAENC}), {Tel}-{Aviv}, {Israel}},
	author = {Markovich-Golan, Shmulik and Gannot, Sharon and Cohen, Israel},
	year = {2010},
	file = {Markovich-Golan2010.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\T5ZPTQP6\\Markovich-Golan2010.pdf:application/pdf},
}

@inproceedings{marquardt_performance_2016,
	title = {Performance {Comparison} of {Bilateral} and {Binaural} {MVDR}-based {Noise} {Reduction} {Algorithms} in the {Presence} of {DOA} {Estimation} {Errors}},
	booktitle = {Speech {Communication}},
	publisher = {VDE},
	author = {Marquardt, Daniel and Doclo, Simon},
	year = {2016},
	pages = {1--5},
	file = {Marquardt,2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AAQ4DHLA\\Marquardt,2016.pdf:application/pdf},
}

@article{cornelis_theoretical_2010,
	title = {Theoretical {Analysis} of {Binaural} {Multimicrophone} {Noise} {Reduction} {Techniques}},
	volume = {18},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/5173569/},
	doi = {10.1109/TASL.2009.2028374},
	number = {2},
	urldate = {2018-01-12},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Cornelis, B. and Doclo, S. and Van dan Bogaert, T. and Moonen, M. and Wouters, J.},
	month = feb,
	year = {2010},
	pages = {342--355},
	file = {cornelis2010.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4M2CYISW\\cornelis2010.pdf:application/pdf},
}

@inproceedings{markovich-golan_distributed_2012,
	title = {Distributed {GSC} beamforming using the relative transfer function},
	booktitle = {Signal {Processing} {Conference} ({EUSIPCO}), 2012 {Proceedings} of the 20th {European}},
	publisher = {IEEE},
	author = {Markovich-Golan, Shmulik and Gannot, Sharon and Cohen, Israel},
	year = {2012},
	pages = {1274--1278},
	file = {EUSIPCO2012_Markovich.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\T369EMD5\\EUSIPCO2012_Markovich.pdf:application/pdf},
}

@article{francart_amplification_2009,
	title = {Amplification of interaural level differences improves sound localization in acoustic simulations of bimodal hearing},
	volume = {126},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3243304},
	doi = {10.1121/1.3243304},
	language = {en},
	number = {6},
	urldate = {2018-01-15},
	journal = {The Journal of the Acoustical Society of America},
	author = {Francart, Tom and Van den Bogaert, Tim and Moonen, Marc and Wouters, Jan},
	month = dec,
	year = {2009},
	pages = {3209--3213},
	file = {francart2009.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\A7P7UEMM\\francart2009.pdf:application/pdf},
}

@article{bronkhorst_effect_1988,
	title = {The effect of head-induced interaural time and level differences on speech intelligibility in noise},
	volume = {83},
	number = {4},
	journal = {The Journal of the Acoustical Society of America},
	author = {Bronkhorst, Adelbert W. and Plomp, Reinier},
	month = apr,
	year = {1988},
	pages = {1508--1516},
	file = {bronkhorst1988.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MWQQQZAL\\bronkhorst1988.pdf:application/pdf},
}

@article{hamacher_signal_2005,
	title = {Signal processing in high-end hearing aids: state of the art, challenges, and future trends},
	volume = {2005},
	shorttitle = {Signal processing in high-end hearing aids},
	journal = {EURASIP Journal on Applied Signal Processing},
	author = {Hamacher, V. and Chalupper, J. and Eggers, Joachim and Fischer, Eghart and Kornagel, Ulrich and Puder, Henning and Rass, U.},
	year = {2005},
	pages = {2915--2929},
	file = {hamacher2005.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AB727UWV\\hamacher2005.pdf:application/pdf},
}

@article{dmochowski_linearly_2008,
	title = {Linearly {Constrained} {Minimum} {Variance} {Source} {Localization} and {Spectral} {Estimation}},
	volume = {16},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/4648926/},
	doi = {10.1109/TASL.2008.2005029},
	number = {8},
	urldate = {2018-01-18},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Dmochowski, Jacek and Benesty, Jacob and Affes, SofiÈne},
	month = nov,
	year = {2008},
	pages = {1490--1502},
	file = {dmochowski2008.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7J9DAEMF\\dmochowski2008.pdf:application/pdf},
}

@article{hadad_binaural_2016,
	title = {The {Binaural} {LCMV} {Beamformer} and its {Performance} {Analysis}},
	volume = {24},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/7372431/},
	doi = {10.1109/TASLP.2016.2514496},
	number = {3},
	urldate = {2018-01-18},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Hadad, Elior and Doclo, Simon and Gannot, Sharon},
	month = mar,
	year = {2016},
	pages = {543--558},
	file = {hadad2016a.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\SP4L2G74\\hadad2016a.pdf:application/pdf},
}

@book{martin_advances_2008,
	address = {Chichester},
	title = {Advances in digital speech transmission},
	isbn = {978-0-470-51739-0},
	language = {eng},
	publisher = {Wiley},
	editor = {Martin, Rainer and Heute, Ulrich and Antweiler, Christiane},
	year = {2008},
	note = {OCLC: 636885899},
	file = {-Advances in Digital Speech Transmission (2008).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LK6NHCL8\\-Advances in Digital Speech Transmission (2008).pdf:application/pdf},
}

@phdthesis{roy_distributed_2008,
	address = {Lausanne, Suisse},
	title = {Distributed {Signal} {Processing} for {Binaural} {Hearing} {Aids}},
	school = {Ecole Polytechnique Fédérale de Lausanne},
	author = {Roy, Olivier},
	year = {2008},
	file = {EPFL_TH4220.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AJSKJIG4\\EPFL_TH4220.pdf:application/pdf},
}

@article{suzuki_new_1999,
	title = {New {Design} {Method} of a {Binaural} {Microphone} {Array} {Using} {Multiple} {Constraints}},
	volume = {82},
	number = {4},
	journal = {IEICE Trans. Fundamentals},
	author = {Suzuki, Yôiti and Tsukui, Shinji and Asano, Futoshi and Nishimura, Ryouichi and Sone, Toshio},
	month = apr,
	year = {1999},
	pages = {588--596},
	file = {suzuki1999.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MTPRFDUA\\suzuki1999.pdf:application/pdf},
}

@book{bourgeois_time-domain_2009,
	address = {Boston, MA},
	series = {Lecture {Notes} in {Electrical} {Engineering}},
	title = {Time-{Domain} {Beamforming} and {Blind} {Source} {Separation}},
	volume = {3},
	isbn = {978-0-387-68835-0 978-0-387-68836-7},
	url = {http://link.springer.com/10.1007/978-0-387-68836-7},
	urldate = {2018-01-18},
	publisher = {Springer US},
	editor = {Bourgeois, Julien and Minker, Wolfgang},
	year = {2009},
	doi = {10.1007/978-0-387-68836-7},
	file = {(Lecture Notes in Electrical Engineering 3) Julien Bourgeois, Wolfgang Minker (eds.)-Time-domain beamforming and blind source separation_ speech input in the car environment-Springer US (2009).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WNV5BSMU\\(Lecture Notes in Electrical Engineering 3) Julien Bourgeois, Wolfgang Minker (eds.)-Time-domain beamforming and blind source separation_ speech input in the car environment-Springer US (2009).pdf:application/pdf},
}

@article{moller_fundamentals_1992,
	title = {Fundamentals of binaural technology},
	volume = {36},
	number = {3-4},
	journal = {Applied acoustics},
	author = {Møller, Henrik},
	year = {1992},
	pages = {171--218},
	file = {Moller_1992.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LA7PXLH7\\Moller_1992.pdf:application/pdf},
}

@article{pulkki_virtual_1997,
	title = {Virtual sound source positioning using vector base amplitude panning},
	volume = {45},
	number = {6},
	journal = {Journal of the audio engineering society},
	author = {Pulkki, Ville},
	year = {1997},
	pages = {456--466},
	file = {pulkki1997.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2G9JMPZ5\\pulkki1997.pdf:application/pdf},
}

@article{freyman_role_1999,
	title = {The role of perceived spatial separation in the unmasking of speech},
	volume = {106},
	number = {6},
	journal = {The Journal of the Acoustical Society of America},
	author = {Freyman, Richard L. and Helfer, Karen S. and McCall, Daniel D. and Clifton, Rachel K.},
	year = {1999},
	pages = {3578--3588},
	file = {freyman1999.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MBPXL6IP\\freyman1999.pdf:application/pdf},
}

@article{edmonds_spatial_2005,
	title = {The spatial unmasking of speech: evidence for within-channel processing of interaural time delay},
	volume = {117},
	issn = {0001-4966},
	shorttitle = {The spatial unmasking of speech},
	url = {http://asa.scitation.org/doi/10.1121/1.1880752},
	doi = {10.1121/1.1880752},
	language = {en},
	number = {5},
	urldate = {2018-01-19},
	journal = {The Journal of the Acoustical Society of America},
	author = {Edmonds, Barrie A. and Culling, John F.},
	month = may,
	year = {2005},
	pages = {3069--3078},
	file = {Edmonds et Culling - 2005 - The spatial unmasking of speech evidence for with.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JKJAZGEG\\Edmonds et Culling - 2005 - The spatial unmasking of speech evidence for with.pdf:application/pdf},
}

@article{brungart_informational_2001,
	title = {Informational and energetic masking effects in the perception of multiple simultaneous talkers},
	volume = {110},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1408946},
	doi = {10.1121/1.1408946},
	language = {en},
	number = {5},
	urldate = {2018-01-19},
	journal = {The Journal of the Acoustical Society of America},
	author = {Brungart, Douglas S. and Simpson, Brian D. and Ericson, Mark A. and Scott, Kimberly R.},
	month = nov,
	year = {2001},
	pages = {2527--2538},
	file = {brungart2001.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CF9SBFQY\\brungart2001.pdf:application/pdf},
}

@article{archer-boyd_biomimetic_2015,
	title = {Biomimetic direction of arrival estimation for resolving front-back confusions in hearing aids},
	volume = {137},
	number = {5},
	journal = {The Journal of the Acoustical Society of America},
	author = {Archer-Boyd, Alan W. and Whitmer, William M. and Brimijoin, W. Owen and Soraghan, John J.},
	year = {2015},
	pages = {EL360--EL366},
	file = {Archer-Boyd et al. - 2015 - Biomimetic direction of arrival estimation for res.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZPM8CQJQ\\Archer-Boyd et al. - 2015 - Biomimetic direction of arrival estimation for res.pdf:application/pdf;archerboyd2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TIZF3FYY\\archerboyd2015.pdf:application/pdf},
}

@book{toole_sound_2009,
	address = {Amsterdam},
	edition = {Nachdr.},
	title = {Sound reproduction: the acoustics and psychoacoustics of loudspeakers and rooms},
	isbn = {978-0-240-52009-4},
	shorttitle = {Sound reproduction},
	language = {eng},
	publisher = {Elsevier, Focal Press},
	author = {Toole, Floyd E.},
	year = {2009},
	note = {OCLC: 838818655},
	file = {Floyd Toole-Sound reproduction _ loudspeakers and rooms-Elsevier, Focal Press (2009).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\38FLL74K\\Floyd Toole-Sound reproduction _ loudspeakers and rooms-Elsevier, Focal Press (2009).pdf:application/pdf},
}

@inproceedings{jeffet_study_2014,
	title = {Study of a generalized spherical array beamformer with adjustable binaural reproduction},
	booktitle = {Hands-free {Speech} {Communication} and {Microphone} {Arrays} ({HSCMA}), 2014 4th {Joint} {Workshop} on},
	publisher = {IEEE},
	author = {Jeffet, Michael and Rafaely, Boaz},
	year = {2014},
	pages = {77--81},
	file = {jeffet2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\D2IN4E6G\\jeffet2014.pdf:application/pdf},
}

@article{jeffet_theory_2016,
	title = {Theory and {Perceptual} {Evaluation} of the {Binaural} {Reproduction} and {Beamforming} {Tradeoff} in the {Generalized} {Spherical} {Array} {Beamformer}},
	volume = {24},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/7393775/},
	doi = {10.1109/TASLP.2016.2522649},
	number = {4},
	urldate = {2018-01-23},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Jeffet, Michael and Shabtai, Noam R. and Rafaely, Boaz},
	month = apr,
	year = {2016},
	pages = {708--718},
	file = {jeffet2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EBZHD3YX\\jeffet2016.pdf:application/pdf},
}

@phdthesis{deleforge_acoustic_2013,
	type = {{PhD} {Thesis}},
	title = {Acoustic {Space} {Mapping}: {A} {Machine} {Learning} {Approach} to {Sound} {Source} {Separation} and {Localization}},
	shorttitle = {Acoustic {Space} {Mapping}},
	school = {Université de Grenoble},
	author = {Deleforge, Antoine},
	year = {2013},
	file = {deleforge2013phd.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\R6ADPVLF\\deleforge2013phd.pdf:application/pdf},
}

@article{majdak_multiple_2007,
	title = {Multiple exponential sweep method for fast measurement of head-related transfer functions},
	volume = {55},
	number = {7/8},
	journal = {Journal of the Audio Engineering Society},
	author = {Majdak, Piotr and Balazs, Peter and Laback, Bernhard},
	year = {2007},
	pages = {623--637},
	file = {Multiple_exponential_sweep_method_for_fast_measure.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZKBD6BR3\\Multiple_exponential_sweep_method_for_fast_measure.pdf:application/pdf},
}

@inproceedings{farina_simultaneous_2000,
	address = {Paris, France},
	title = {Simultaneous {Measurement} of {Impulse} {Response} and {Distortion} with a {Swept}-{Sine} {Technique}},
	booktitle = {Audio {Engineering} {Society} {Convention} 108},
	author = {Farina, Angelo},
	month = feb,
	year = {2000},
	file = {farina2000.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EE72MYAZ\\farina2000.pdf:application/pdf},
}

@inproceedings{farina_advancements_2007,
	title = {Advancements in impulse response measurements by sine sweeps},
	booktitle = {Audio {Engineering} {Society} {Convention} 122},
	publisher = {Audio Engineering Society},
	author = {Farina, Angelo},
	year = {2007},
	file = {farina2007.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\S95BC5DG\\farina2007.pdf:application/pdf},
}

@article{lotter_dual-channel_2006,
	title = {Dual-{Channel} {Speech} {Enhancement} by {Superdirective} {Beamforming}},
	volume = {2006},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/63297},
	doi = {10.1155/ASP/2006/63297},
	language = {en},
	number = {1},
	urldate = {2018-01-31},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Lotter, Thomas and Vary, Peter},
	month = dec,
	year = {2006},
	pages = {14},
	file = {lotter2006.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9MMHP749\\lotter2006.pdf:application/pdf},
}

@inproceedings{klasen_preservation_2005,
	title = {Preservation of interaural time delay for binaural hearing aids through multi-channel {Wiener} filtering based noise reduction},
	volume = {3},
	booktitle = {Acoustics, {Speech}, and {Signal} {Processing}, 2005. {Proceedings}.({ICASSP}'05). {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Klasen, Thomas J. and Moonen, Marc and Van den Bogaert, Tim and Wouters, Jan},
	year = {2005},
	pages = {iii--29},
	file = {klasen2005.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WTURBEJS\\klasen2005.pdf:application/pdf},
}

@article{luo_adaptive_2002,
	title = {Adaptive null-forming scheme in digital hearing aids},
	volume = {50},
	number = {7},
	journal = {IEEE Transactions on signal processing},
	author = {Luo, Fa-Long and Yang, Jun and Pavlovic, Chaslav and Nehorai, Arye},
	year = {2002},
	pages = {1583--1590},
	file = {luo2002.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\SHXLYCT3\\luo2002.pdf:application/pdf},
}

@article{maj_comparison_2006,
	title = {Comparison of adaptive noise reduction algorithms in dual microphone hearing aids},
	volume = {48},
	issn = {01676393},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167639305002906},
	doi = {10.1016/j.specom.2005.12.005},
	language = {en},
	number = {8},
	urldate = {2018-01-31},
	journal = {Speech Communication},
	author = {Maj, Jean-Baptiste and Royackers, Liesbeth and Wouters, Jan and Moonen, Marc},
	month = aug,
	year = {2006},
	pages = {957--970},
	file = {maj2006.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MF2HGH9R\\maj2006.pdf:application/pdf},
}

@inproceedings{spriet_spatially_2003,
	address = {Kyoto, Japan},
	title = {Spatially {Pre}-{Processed} {Speech} {Distortion} {Weighted} {Multi}-{Channel} {Wiener} {Filtering} {For} {Noise} {Reduction} {In} {Hearing} {Aids}},
	volume = {84},
	booktitle = {International {Workshop} on {Acoustic} {Echo} and {Noise} {Control}},
	publisher = {Signal Processing},
	author = {Spriet, Ann and Moonen, Marc and Wouters, Jan},
	month = sep,
	year = {2003},
	pages = {2367--2387},
	file = {spriet2004.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JYP2SRCI\\spriet2004.pdf:application/pdf},
}

@article{gutierrez-parera_influence_2016,
	title = {Influence of the {Quality} of {Consumer} {Headphones} in the {Perception} of {Spatial} {Audio}},
	volume = {6},
	issn = {2076-3417},
	url = {http://www.mdpi.com/2076-3417/6/4/117},
	doi = {10.3390/app6040117},
	language = {en},
	number = {4},
	urldate = {2018-02-02},
	journal = {Applied Sciences},
	author = {Gutierrez-Parera, Pablo and Lopez, Jose},
	month = apr,
	year = {2016},
	pages = {117},
	file = {gutierrez-parera2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MAJM76EM\\gutierrez-parera2016.pdf:application/pdf},
}

@inproceedings{farina_emulation_2005,
	title = {Emulation of not-linear, time-variant devices by the convolution technique},
	booktitle = {Congresso {AES} {Italia}, {Como}},
	author = {Farina, Angelo and Armelloni, Enrico},
	year = {2005},
	file = {farina2005.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GZT78LLX\\farina2005.pdf:application/pdf},
}

@article{macpherson_listener_2002,
	title = {Listener weighting of cues for lateral angle: {The} duplex theory of sound localization revisited},
	volume = {111},
	issn = {00014966},
	shorttitle = {Listener weighting of cues for lateral angle},
	url = {http://scitation.aip.org/content/asa/journal/jasa/111/5/10.1121/1.1471898},
	doi = {10.1121/1.1471898},
	language = {en},
	number = {5},
	urldate = {2018-02-05},
	journal = {The Journal of the Acoustical Society of America},
	author = {Macpherson, Ewan A. and Middlebrooks, John C.},
	year = {2002},
	pages = {2219},
	file = {MacphersonMiddlebrooks2002.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BLFJ52J6\\MacphersonMiddlebrooks2002.pdf:application/pdf},
}

@article{brughera_human_2013,
	title = {Human interaural time difference thresholds for sine tones: the high-frequency limit},
	volume = {133},
	shorttitle = {Human interaural time difference thresholds for sine tones},
	number = {5},
	journal = {The Journal of the Acoustical Society of America},
	author = {Brughera, Andrew and Dunai, Larisa and Hartmann, William M.},
	year = {2013},
	pages = {2839--2855},
	file = {brughera2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YQFXL8FH\\brughera2013.pdf:application/pdf},
}

@article{odwyer_virtual_2016,
	title = {Virtual {Headphone} testing for {Spatial} {Audio}},
	author = {O’Dwyer, Hugh and Gorzel, Marcin and Ferguson, Luke and Bates, Enda and Boland, Francis M.},
	year = {2016},
	keywords = {headphone},
	file = {dwyer2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Y33CTNJQ\\dwyer2016.pdf:application/pdf},
}

@inproceedings{kearney_hrtf_2015,
	title = {An {HRTF} {Database} for {Virtual} {Loudspeaker} {Rendering}},
	booktitle = {Audio {Engineering} {Society} {Convention} 139},
	publisher = {Audio Engineering Society},
	author = {Kearney, Gavin and Doyle, Tony},
	year = {2015},
	file = {kearney2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\M5PM37B8\\kearney2015.pdf:application/pdf},
}

@article{brungart_cocktail_2007,
	title = {Cocktail party listening in a dynamic multitalker environment},
	volume = {69},
	number = {1},
	journal = {Perception \& Psychophysics},
	author = {Brungart, Douglas S. and Simpson, Brian D.},
	year = {2007},
	pages = {79--91},
	file = {brungart2007.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\SEHMBMX3\\brungart2007.pdf:application/pdf},
}

@book{popelka_hearing_2016,
	address = {Cham},
	series = {Springer {Handbook} of {Auditory} {Research}},
	title = {Hearing {Aids}},
	volume = {56},
	isbn = {978-3-319-33034-1 978-3-319-33036-5},
	url = {http://link.springer.com/10.1007/978-3-319-33036-5},
	urldate = {2018-02-20},
	publisher = {Springer International Publishing},
	editor = {Popelka, Gerald R. and Moore, Brian C. J. and Fay, Richard R. and Popper, Arthur N.},
	year = {2016},
	doi = {10.1007/978-3-319-33036-5},
	file = {(Springer Handbook of Auditory Research 56) Gerald R. Popelka, Brian C. J. Moore, Richard R. Fay, Arthur N. Popper (eds.)-Hearing Aids-Springer International Publishing (2016)(1).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WX4BI89V\\(Springer Handbook of Auditory Research 56) Gerald R. Popelka, Brian C. J. Moore, Richard R. Fay, Arthur N. Popper (eds.)-Hearing Aids-Springer International Publishing (2016)(1).pdf:application/pdf},
}

@article{carette_left-right_2014,
	title = {Left-{Right} and {Front}-{Back} {Spatial} {Hearing} with {Multiple} {Directional} {Microphone} {Configurations} in {Modern} {Hearing} {Aids}},
	volume = {25},
	issn = {10500545, 21573107},
	url = {http://openurl.ingenta.com/content/xref?genre=article&issn=1050-0545&volume=25&issue=9&spage=791},
	doi = {10.3766/jaaa.25.9.2},
	language = {en},
	number = {9},
	urldate = {2018-02-20},
	journal = {Journal of the American Academy of Audiology},
	author = {Carette, Evelyne and Van den Bogaert, Tim and Laureyns, Mark and Wouters, Jan},
	month = oct,
	year = {2014},
	pages = {791--803},
	file = {carette2014.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BT8J55TJ\\carette2014.pdf:application/pdf},
}

@article{morgenstern_modal_2018,
	title = {Modal smoothing for analysis of room reflections measured with spherical microphone and loudspeaker arrays},
	volume = {143},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.5024234},
	doi = {10.1121/1.5024234},
	language = {en},
	number = {2},
	urldate = {2018-02-21},
	journal = {The Journal of the Acoustical Society of America},
	author = {Morgenstern, Hai and Rafaely, Boaz},
	month = feb,
	year = {2018},
	pages = {1008--1018},
	file = {morgenstern2018.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\26PVXM2H\\morgenstern2018.pdf:application/pdf},
}

@article{hadad_theoretical_2015,
	title = {Theoretical {Analysis} of {Binaural} {Transfer} {Function} {MVDR} {Beamformers} with {Interference} {Cue} {Preservation} {Constraints}},
	volume = {23},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/7287749/},
	doi = {10.1109/TASLP.2015.2486381},
	number = {12},
	urldate = {2018-02-21},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Hadad, Elior and Marquardt, Daniel and Doclo, Simon and Gannot, Sharon},
	month = dec,
	year = {2015},
	pages = {2449--2464},
	file = {hadad2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\K2PILDKW\\hadad2015.pdf:application/pdf},
}

@article{faller_source_2004,
	title = {Source localization in complex listening situations: {Selection} of binaural cues based on interaural coherence},
	volume = {116},
	issn = {0001-4966},
	shorttitle = {Source localization in complex listening situations},
	url = {http://asa.scitation.org/doi/10.1121/1.1791872},
	doi = {10.1121/1.1791872},
	language = {en},
	number = {5},
	urldate = {2018-02-21},
	journal = {The Journal of the Acoustical Society of America},
	author = {Faller, Christof and Merimaa, Juha},
	month = nov,
	year = {2004},
	pages = {3075--3089},
	file = {faller2004.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\X7HIDAM5\\faller2004.pdf:application/pdf},
}

@article{koutrouvelis_relaxed_2017,
	title = {Relaxed {Binaural} {LCMV} {Beamforming}},
	volume = {25},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/7742910/},
	doi = {10.1109/TASLP.2016.2628642},
	number = {1},
	urldate = {2018-02-21},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Koutrouvelis, Andreas I. and Hendriks, Richard Christian and Heusdens, Richard and Jensen, Jesper},
	month = jan,
	year = {2017},
	pages = {137--152},
	file = {Koutrouvelis et al. - Relaxed Binaural LCMV Beamforming.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3XAFQTF6\\Koutrouvelis et al. - Relaxed Binaural LCMV Beamforming.pdf:application/pdf;koutrouvelis2016.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WRGF2DCJ\\koutrouvelis2016.pdf:application/pdf},
}

@article{algazi_elevation_2001,
	title = {Elevation localization and head-related transfer function analysis at low frequencies},
	volume = {109},
	issn = {0001-4966},
	url = {http://scitation.aip.org/content/asa/journal/jasa/109/3/10.1121/1.1349185},
	doi = {10.1121/1.1349185},
	language = {en},
	number = {3},
	urldate = {2018-02-21},
	journal = {The Journal of the Acoustical Society of America},
	author = {Algazi, V. Ralph and Avendano, Carlos and Duda, Richard O.},
	month = mar,
	year = {2001},
	pages = {1110--1122},
	file = {algazi2001.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UWFHVR9N\\algazi2001.pdf:application/pdf},
}

@article{giannoulis_digital_2012,
	title = {Digital dynamic range compressor design—{A} tutorial and analysis},
	volume = {60},
	number = {6},
	journal = {Journal of the Audio Engineering Society},
	author = {Giannoulis, Dimitrios and Massberg, Michael and Reiss, Joshua D.},
	year = {2012},
	pages = {399--408},
	file = {giannoulis2012.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5QGZPE3B\\giannoulis2012.pdf:application/pdf},
}

@article{dokmanic_raking_2015,
	title = {Raking the {Cocktail} {Party}},
	volume = {9},
	issn = {1932-4553, 1941-0484},
	url = {http://ieeexplore.ieee.org/document/7065205/},
	doi = {10.1109/JSTSP.2015.2415761},
	number = {5},
	urldate = {2018-03-05},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Dokmanic, Ivan and Scheibler, Robin and Vetterli, Martin},
	month = aug,
	year = {2015},
	pages = {825--836},
	file = {dokmanic2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4G9UBV34\\dokmanic2015.pdf:application/pdf},
}

@article{pinto_space-time-frequency_2010,
	title = {Space-{Time}-{Frequency} {Processing} of {Acoustic} {Wave} {Fields}: {Theory}, {Algorithms}, and {Applications}},
	volume = {58},
	issn = {1053-587X, 1941-0476},
	shorttitle = {Space-{Time}-{Frequency} {Processing} of {Acoustic} {Wave} {Fields}},
	url = {http://ieeexplore.ieee.org/document/5482103/},
	doi = {10.1109/TSP.2010.2052045},
	number = {9},
	urldate = {2018-03-05},
	journal = {IEEE Transactions on Signal Processing},
	author = {Pinto, F and Vetterli, M},
	month = sep,
	year = {2010},
	pages = {4608--4620},
	file = {pinto2010.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\S5YT4EYI\\pinto2010.pdf:application/pdf},
}

@article{van_veen_beamforming:_1988,
	title = {Beamforming: a versatile approach to spatial filtering},
	volume = {5},
	issn = {0740-7467},
	shorttitle = {Beamforming},
	url = {http://ieeexplore.ieee.org/document/665/},
	doi = {10.1109/53.665},
	number = {2},
	urldate = {2018-03-13},
	journal = {IEEE ASSP Magazine},
	author = {Van Veen, B.D. and Buckley, K.M.},
	month = apr,
	year = {1988},
	pages = {4--24},
	file = {Van Veen et Buckley - 1988 - Beamforming a versatile approach to spatial filte.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\N9FM8YWG\\Van Veen et Buckley - 1988 - Beamforming a versatile approach to spatial filte.pdf:application/pdf},
}

@article{trapeau_encoding_2018,
	title = {The encoding of sound source elevation in the human auditory cortex},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.2530-17.2018},
	doi = {10.1523/JNEUROSCI.2530-17.2018},
	language = {en},
	urldate = {2018-03-14},
	journal = {The Journal of Neuroscience},
	author = {Trapeau, Régis and Schönwiesner, Marc},
	month = mar,
	year = {2018},
	keywords = {HRTF, relearning, plasticity},
	pages = {2530--17},
	file = {Trapeau et Schönwiesner - 2018 - The encoding of sound source elevation in the huma.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\C6D3IT6S\\Trapeau et Schönwiesner - 2018 - The encoding of sound source elevation in the huma.pdf:application/pdf},
}

@inproceedings{hurley_flexibeam:_2016,
	title = {Flexibeam: {Analytic} spatial filtering by beamforming},
	isbn = {978-1-4799-9988-0},
	shorttitle = {Flexibeam},
	url = {http://ieeexplore.ieee.org/document/7472203/},
	doi = {10.1109/ICASSP.2016.7472203},
	abstract = {We propose a new, general, method for spatial ﬁltering by beamforming. The desired ﬁlter, speciﬁed analytically on an n-dimensional sphere, is extended to n + 1-dimensional Euclidean space. A continuous beamforming function is then obtained by the n + 1dimensional Fourier transform of the extended ﬁlter. The beamforming weight at a given array element corresponds then to a sample of the function at the array element location.},
	urldate = {2018-03-14},
	publisher = {IEEE},
	author = {Hurley, Paul and Simeoni, Matthieu},
	month = mar,
	year = {2016},
	pages = {2877--2880},
	file = {Hurley et Simeoni - 2016 - Flexibeam Analytic spatial filtering by beamformi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VW47TNL4\\Hurley et Simeoni - 2016 - Flexibeam Analytic spatial filtering by beamformi.pdf:application/pdf},
}

@article{ajdler_plenacoustic_2006,
	title = {The {Plenacoustic} {Function} and {Its} {Sampling}},
	volume = {54},
	issn = {1053-587X},
	url = {http://ieeexplore.ieee.org/document/1703848/},
	doi = {10.1109/TSP.2006.879280},
	abstract = {The spatialization of the sound ﬁeld in a room is studied, in particular the evolution of room impulse responses as a function of their spatial positions. It was observed that the multidimensional spectrum of the solution of the wave equation has an almost bandlimited character. Therefore, sampling and interpolation can easily be applied using signals on an array. The decay of the spectrum is studied on both temporal and spatial frequency axes. The inﬂuence of the decay on the performance of the interpolation is analyzed. Based on the support of the spectrum, the number and the spacing between the microphones is determined for the reconstruction of the sound pressure ﬁeld up to a certain temporal frequency and with a certain reconstruction quality. The optimal sampling pattern for the microphone positions is given for the linear, planar and three-dimensional case. Existing techniques usually make use of room models to recreate the sound ﬁeld present at some point in the space. The presented technique simply starts from the measurements of the sound pressure ﬁeld in a ﬁnite number of positions and with this information the sound pressure ﬁeld can be recreated at any spatial position. Finally, simulations and experimental results are presented and compared with the theory.},
	number = {10},
	urldate = {2018-03-14},
	journal = {IEEE Transactions on Signal Processing},
	author = {Ajdler, T. and Sbaiz, L. and Vetterli, M.},
	month = oct,
	year = {2006},
	pages = {3790--3804},
	file = {Ajdler et al. - 2006 - The Plenacoustic Function and Its Sampling.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RH87D34R\\Ajdler et al. - 2006 - The Plenacoustic Function and Its Sampling.pdf:application/pdf},
}

@article{ricketts_sound_2005,
	title = {Sound {Quality} {Measures} for {Speech} in {Noise} through a {Commercial} {Hearing} {Aid} {Implementing} "{Digital} {Noise} {Reduction}"},
	volume = {16},
	issn = {10500545},
	url = {http://openurl.ingenta.com/content/xref?genre=article&issn=1050-0545&volume=16&issue=5&spage=270},
	doi = {10.3766/jaaa.16.5.2},
	abstract = {This brief report discusses the affect of digital noise reduction (DNR) processing on aided speech recognition and sound quality measures in 14 adults fitted with a commercial hearing aid. Measures of speech recognition and sound quality were obtained in two different speech-in-noise conditions (71 dBA speech, +6 dB SNR and 75 dBA speech, +1 dB SNR). The results revealed that the presence or absence of DNR processing did not impact speech recognition in noise (either positively or negatively). Paired comparisons of sound quality for the same speech in noise signals, however, revealed a strong preference for DNR processing. These data suggest that at least one implementation of DNR processing is capable of providing improved sound quality, for speech in noise, in the absence of improved speech recognition.},
	language = {en},
	number = {5},
	urldate = {2018-03-21},
	journal = {Journal of the American Academy of Audiology},
	author = {Ricketts, Todd A. and Hornsby, Benjamin W.Y.},
	month = may,
	year = {2005},
	pages = {270--277},
	file = {Ricketts et Hornsby - 2005 - Sound Quality Measures for Speech in Noise through.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WQDA6YBR\\Ricketts et Hornsby - 2005 - Sound Quality Measures for Speech in Noise through.pdf:application/pdf},
}

@article{boymans_field_2000,
	title = {Field {Trials} {Using} a {Digital} {Hearing} {Aid} with {Active} {Noise} {Reduction} and {Dual}-{Microphone} {Directionality}: {Estudios} de campo utilizando un audifono digital con reduccion activa del ruido y micrófono de direccionalidad dual},
	volume = {39},
	issn = {1499-2027, 1708-8186},
	shorttitle = {Field {Trials} {Using} a {Digital} {Hearing} {Aid} with {Active} {Noise} {Reduction} and {Dual}-{Microphone} {Directionality}},
	url = {http://www.tandfonline.com/doi/full/10.3109/00206090009073090},
	doi = {10.3109/00206090009073090},
	language = {en},
	number = {5},
	urldate = {2018-03-26},
	journal = {International Journal of Audiology},
	author = {Boymans, Monique and Dreschler, Wouter A.},
	month = jan,
	year = {2000},
	pages = {260--268},
	file = {Boymans et Dreschler - 2000 - Field Trials Using a Digital Hearing Aid with Acti.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GQSGQ3TD\\Boymans et Dreschler - 2000 - Field Trials Using a Digital Hearing Aid with Acti.pdf:application/pdf},
}

@article{phatak_development_2018,
	title = {Development of a {Test} {Battery} for {Evaluating} {Speech} {Perception} in {Complex} {Listening} {Environments}: {Effects} of {Sensorineural} {Hearing} {Loss}},
	abstract = {Objective: To evaluate the speech-in-noise performance of listeners with different levels of hearing loss in a variety of complex listening environments. Design: The quick speech-in-noise (QuickSIN)-based test battery was used to measure the speech recognition performance of listeners with different levels of hearing loss. Subjective estimates of speech reception thresholds (SRTs) corresponding to 100\% and 0\% speech intelligibility, respectively, were obtained using a method of adjustment before objective measurement of the actual SRT corresponding to 50\% speech intelligibility in every listening condition.
Results: Of the seven alternative listening conditions, two conditions, one involving time-compressed, reverberant speech (TC+Rev), and the other (N0Sπ) having in-phase noise masker (N0) and out-of-phase target (Sπ), were found to be substantially more sensitive to the effect of hearing loss than the standard QuickSIN test. The performance in these two conditions also correlated with self-reported difficulties in attention/ concentration during speech communication and in localizing the sound source, respectively. Hearing thresholds could account for about 50\% or less variance in SRTs in any listening condition. Subjectively estimated SRTs (SRTs corresponding to 0\% and 100\% speech intelligibility) were highly correlated with the objective SRT measurements (SRT corresponding to 50\% speech intelligibility).
Conclusions: A test battery that includes the TC+Rev and the N0Sπ conditions would be useful in identifying individuals with hearing loss with speech-in-noise deficits in everyday communication.},
	language = {en},
	author = {Phatak, Sandeep A and Sheffield, Benjamin M and Brungart, Douglas S and Grant, Ken W},
	year = {2018},
	pages = {8},
	file = {Phatak et al. - 2018 - Development of a Test Battery for Evaluating Speec.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HATNWIDU\\Phatak et al. - 2018 - Development of a Test Battery for Evaluating Speec.pdf:application/pdf},
}

@article{dieudonne_head_2018,
	title = {Head shadow enhancement with low-frequency beamforming improves sound localization and speech perception for simulated bimodal listeners},
	issn = {03785955},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0378595518300169},
	doi = {10.1016/j.heares.2018.03.007},
	abstract = {Many hearing-impaired listeners struggle to localize sounds due to poor availability of binaural cues. Listeners with a cochlear implant and a contralateral hearing aid e so-called bimodal listeners e are amongst the worst performers, as both interaural time and level differences are poorly transmitted. We present a new method to enhance head shadow in the low frequencies. Head shadow enhancement is achieved with a ﬁxed beamformer with contralateral attenuation in each ear. The method results in interaural level differences which vary monotonically with angle. It also improves low-frequency signalto-noise ratios in conditions with spatially separated speech and noise. We validated the method in two experiments with acoustic simulations of bimodal listening. In the localization experiment, performance improved from 50:5+ to 26:8+ root-mean-square error compared with standard omni-directional microphones. In the speech-in-noise experiment, speech was presented from the frontal direction. Speech reception thresholds improved by 15.7 dB SNR when the noise was presented from the cochlear implant side, improved by 7.6 dB SNR when the noise was presented from the hearing aid side, and was not affected when noise was presented from all directions. Apart from bimodal listeners, the method might also be promising for bilateral cochlear implant or hearing aid users. Its low computational complexity makes the method suitable for application in current clinical devices.},
	language = {en},
	urldate = {2018-03-26},
	journal = {Hearing Research},
	author = {Dieudonné, Benjamin and Francart, Tom},
	month = mar,
	year = {2018},
	file = {Dieudonné et Francart - 2018 - Head shadow enhancement with low-frequency beamfor.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KDSXYV67\\Dieudonné et Francart - 2018 - Head shadow enhancement with low-frequency beamfor.pdf:application/pdf},
}

@article{buechner_advanced_2014,
	title = {Advanced {Beamformers} for {Cochlear} {Implant} {Users}: {Acute} {Measurement} of {Speech} {Perception} in {Challenging} {Listening} {Conditions}},
	volume = {9},
	issn = {1932-6203},
	shorttitle = {Advanced {Beamformers} for {Cochlear} {Implant} {Users}},
	url = {http://dx.plos.org/10.1371/journal.pone.0095542},
	doi = {10.1371/journal.pone.0095542},
	abstract = {Objective: To investigate the performance of monaural and binaural beamforming technology with an additional noise reduction algorithm, in cochlear implant recipients.
Method: This experimental study was conducted as a single subject repeated measures design within a large German cochlear implant centre. Twelve experienced users of an Advanced Bionics HiRes90K or CII implant with a Harmony speech processor were enrolled. The cochlear implant processor of each subject was connected to one of two bilaterally placed state-of-the-art hearing aids (Phonak Ambra) providing three alternative directional processing options: an omnidirectional setting, an adaptive monaural beamformer, and a binaural beamformer. A further noise reduction algorithm (ClearVoice) was applied to the signal on the cochlear implant processor itself. The speech signal was presented from 0u and speech shaped noise presented from loudspeakers placed at 670u, 6135u and 180u. The Oldenburg sentence test was used to determine the signal-to-noise ratio at which subjects scored 50\% correct.
Results: Both the adaptive and binaural beamformer were significantly better than the omnidirectional condition (5.3 dB61.2 dB and 7.1 dB61.6 dB (p,0.001) respectively). The best score was achieved with the binaural beamformer in combination with the ClearVoice noise reduction algorithm, with a significant improvement in SRT of 7.9 dB62.4 dB (p, 0.001) over the omnidirectional alone condition.
Conclusions: The study showed that the binaural beamformer implemented in the Phonak Ambra hearing aid could be used in conjunction with a Harmony speech processor to produce substantial average improvements in SRT of 7.1 dB. The monaural, adaptive beamformer provided an averaged SRT improvement of 5.3 dB.},
	language = {en},
	number = {4},
	urldate = {2018-04-03},
	journal = {PLoS ONE},
	author = {Buechner, Andreas and Dyballa, Karl-Heinz and Hehrmann, Phillipp and Fredelake, Stefan and Lenarz, Thomas},
	editor = {Wanunu, Meni},
	month = apr,
	year = {2014},
	pages = {e95542},
	file = {Buechner et al. - 2014 - Advanced Beamformers for Cochlear Implant Users A.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\83FCLHDT\\Buechner et al. - 2014 - Advanced Beamformers for Cochlear Implant Users A.pdf:application/pdf},
}

@article{amieva_self-reported_2015,
	title = {Self-{Reported} {Hearing} {Loss}, {Hearing} {Aids}, and {Cognitive} {Decline} in {Elderly} {Adults}: {A} 25-{Year} {Study}},
	volume = {63},
	issn = {00028614},
	shorttitle = {Self-{Reported} {Hearing} {Loss}, {Hearing} {Aids}, and {Cognitive} {Decline} in {Elderly} {Adults}},
	url = {http://doi.wiley.com/10.1111/jgs.13649},
	doi = {10.1111/jgs.13649},
	abstract = {OBJECTIVES: To investigate the association between hearing loss, hearing aid use, and cognitive decline. DESIGN: Prospective population-based study. SETTING: Data gathered from the Personnes Ag ees QUID study, a cohort study begun in 1989–90. PARTICIPANTS: Individuals aged 65 and older (N = 3,670).
MEASUREMENTS: At baseline, hearing loss was determined using a questionnaire assessing self-perceived hearing loss; 137 subjects reported major hearing loss, 1,139 reported moderate problems (difﬁculty following the conversation when several persons talk at the same time or in a noisy background), and 2,394 reported no hearing trouble. Cognitive decline was measured using the Mini-Mental State Examination (MMSE), administered at follow-up visits over 25 years.
RESULTS: Self-reported hearing loss was signiﬁcantly associated with lower baseline MMSE score (b = À0.69, P {\textless} .001) and greater decline during the 25-year follow-up period (b = À0.04, P = .01) independent of age, sex, and education. A difference in the rate of change in MMSE score over the 25-year follow-up was observed between participants with hearing loss not using hearing aids and controls (b = À0.06, P {\textless} .001). In contrast, subjects with hearing loss using a hearing aid had no difference in cognitive decline (b = 0.07, P = .08) from controls.
CONCLUSION: Self-reported hearing loss is associated with accelerated cognitive decline in older adults; hearing aid use attenuates such decline. J Am Geriatr Soc 63:2099–2104, 2015.},
	language = {en},
	number = {10},
	urldate = {2018-04-06},
	journal = {Journal of the American Geriatrics Society},
	author = {Amieva, Hélène and Ouvrard, Camille and Giulioli, Caroline and Meillon, Céline and Rullier, Laetitia and Dartigues, Jean-François},
	month = oct,
	year = {2015},
	pages = {2099--2104},
	file = {Amieva et al. - 2015 - Self-Reported Hearing Loss, Hearing Aids, and Cogn.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TJUNV4JP\\Amieva et al. - 2015 - Self-Reported Hearing Loss, Hearing Aids, and Cogn.pdf:application/pdf},
}

@inproceedings{ferguson_bottle_1998,
	title = {A bottle model for head-related transfer functions},
	volume = {6},
	isbn = {978-0-7803-4428-0},
	url = {http://ieeexplore.ieee.org/document/679632/},
	doi = {10.1109/ICASSP.1998.679632},
	abstract = {We describe a parsimonious model for the directiondependent transfer function of the pinna. The model describes the transfer function with reference to resonators located in particular physical positions relative to the ear cand. The purpose of the work is to provide a parametric model that permits identification with moderate data-gathering, and filter specification for any direction without the need for interpolation of responses.},
	language = {en},
	urldate = {2018-04-06},
	publisher = {IEEE},
	author = {Ferguson, B.S. and Bogner, R.E. and Wawryk, S.},
	year = {1998},
	pages = {3533--3536},
	file = {Ferguson et al. - 1998 - A bottle model for head-related transfer functions.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\C28G9WUD\\Ferguson et al. - 1998 - A bottle model for head-related transfer functions.pdf:application/pdf},
}

@inproceedings{martin_speech_2008,
	title = {Speech enhancement in hearing aids - from noise suppression to rendering of auditory scenes},
	isbn = {978-1-4244-2481-8},
	url = {http://ieeexplore.ieee.org/document/4736547/},
	doi = {10.1109/EEEI.2008.4736547},
	abstract = {In this paper we review state-of-the-art signal processing technologies for enhancing speech and audio signals in hearing aids and assess their beneﬁts and their potential for the purpose of acoustic scene analysis and synthesis. We discuss single and multi microphone techniques for source detection, localization, classiﬁcation, and enhancement and we emphasize the need for rendering enhanced output signals which provide a consistent and natural mapping of the acoustic environment. Despite considerable progress in many relevant areas we argue that this requires a processing framework which goes much beyond the current signal processing approaches and in fact requires the fusion of sound ﬁeld analysis, signal enhancement techniques and binaural rendering approaches.},
	language = {en},
	urldate = {2018-04-06},
	publisher = {IEEE},
	author = {Martin, Rainer and Enzner, Gerald},
	month = mar,
	year = {2008},
	pages = {363--367},
	file = {Martin et Enzner - 2008 - Speech enhancement in hearing aids - from noise su.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RSBIGVYU\\Martin et Enzner - 2008 - Speech enhancement in hearing aids - from noise su.pdf:application/pdf},
}

@article{kuk_theoretical_1996,
	title = {Theoretical and {Practical} {Considerations} in {Compression} {Hearing} {Aids}},
	volume = {1},
	issn = {1084-7138},
	url = {http://journals.sagepub.com/doi/10.1177/108471389600100102},
	doi = {10.1177/108471389600100102},
	language = {en},
	number = {1},
	urldate = {2018-04-06},
	journal = {Trends in Amplification},
	author = {Kuk, Francis K.},
	month = mar,
	year = {1996},
	pages = {5--39},
	file = {Kuk - 1996 - Theoretical and Practical Considerations in Compre.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VPDT33CZ\\Kuk - 1996 - Theoretical and Practical Considerations in Compre.pdf:application/pdf},
}

@article{souza_effects_2002,
	title = {Effects of {Compression} on {Speech} {Acoustics}, {Intelligibility}, and {Sound} {Quality}},
	volume = {6},
	issn = {1084-7138},
	url = {http://journals.sagepub.com/doi/10.1177/108471380200600402},
	doi = {10.1177/108471380200600402},
	language = {en},
	number = {4},
	urldate = {2018-04-06},
	journal = {Trends in Amplification},
	author = {Souza, Pamela E.},
	month = dec,
	year = {2002},
	pages = {131--165},
	file = {Souza - 2002 - Effects of Compression on Speech Acoustics, Intell.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QHXMU8K5\\Souza - 2002 - Effects of Compression on Speech Acoustics, Intell.pdf:application/pdf},
}

@article{martin_adaptive_2001,
	title = {Adaptive {Dynamic} {Range} {Optimisation} for {Hearing} {Aids}},
	abstract = {ADRO (Adaptive Dynamic Range Optimisation) is a slowly-adapting digital signal processorlhat controls the output 1{\textless}:{\textbackslash}'1'18 of a set of narrow frequency bands So that the levels fall within a spedficd dynamic range. ADRO is suitable for a variety of applications, including control of a hearing aill Tn the case of a hearing aid, the output dynamic range is dl:fined by the threshold of hearing (I) and a comfortable level(Cl at each frequency fortheindividual listener. A set ofrul". iS1l!ledm control the output levels, witll each rule directly addressingarequirementforafunctiooalhearingaid. For example, Ihe audibility rule specifies lhatthe output level shouldbegreaterthan a flMld level between T and C at least 70\% of the time. The discomfort rule specifies that the output level should be bel{\textless}IW C at leost 90\% of the time. Tn this study, open-set sentence perception scores foc 15 listeners were oomparcd for ADRO and a linear bearing aid fit. Speech WBIl preoonted at three levels\_ ADRO improved scores by 1.9\% at 15 dB SPL (NS), 15.9"A{\textgreater} at 65 dB SPL (p = 0-'114) and 36\% at 55 dB SPL (p{\textless}O.OOI).},
	language = {en},
	author = {Martin, Lois F. A. and Blamey, Peter J. and James, Christopher J. and Galvin, Karyn L. and Macfarlane, David},
	year = {2001},
	pages = {4},
	file = {Macfarlane'' - ADAPTIVE DYNAMIC RANGE OPTIMISATION FOR HEARING AI.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3RK8UW7G\\Macfarlane'' - ADAPTIVE DYNAMIC RANGE OPTIMISATION FOR HEARING AI.pdf:application/pdf},
}

@article{blamey_adaptive_2005,
	title = {Adaptive {Dynamic} {Range} {Optimization} ({ADRO}): {A} {Digital} {Amplification} {Strategy} for {Hearing} {Aids} and {Cochlear} {Implants}},
	volume = {9},
	issn = {1084-7138},
	shorttitle = {Adaptive {Dynamic} {Range} {Optimization} ({ADRO})},
	url = {http://journals.sagepub.com/doi/10.1177/108471380500900203},
	doi = {10.1177/108471380500900203},
	language = {en},
	number = {2},
	urldate = {2018-04-06},
	journal = {Trends in Amplification},
	author = {Blamey, Peter J.},
	month = mar,
	year = {2005},
	pages = {77--98},
	file = {Blamey - 2005 - Adaptive Dynamic Range Optimization (ADRO) A Digi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\D3VEGF6X\\Blamey - 2005 - Adaptive Dynamic Range Optimization (ADRO) A Digi.pdf:application/pdf},
}

@article{best_evaluation_2015,
	title = {An evaluation of the performance of two binaural beamformers in complex and dynamic multitalker environments},
	volume = {54},
	issn = {1499-2027, 1708-8186},
	url = {http://www.tandfonline.com/doi/full/10.3109/14992027.2015.1059502},
	doi = {10.3109/14992027.2015.1059502},
	abstract = {Objective: Binaural beamformers are super-directional hearing aids created by combining microphone outputs from each side of the head. While they offer substantial improvements in SNR over conventional directional hearing aids, the beneﬁts (and possible limitations) of these devices in realistic, complex listening situations have not yet been fully explored. In this study we evaluated the performance of two experimental binaural beamformers. Design: Testing was carried out using a horizontal loudspeaker array. Background noise was created using recorded conversations. Performance measures included speech intelligibility, localization in noise, acceptable noise level, subjective ratings, and a novel dynamic speech intelligibility measure. Study sample: Participants were 27 listeners with bilateral hearing loss, ﬁtted with BTE prototypes that could be switched between conventional directional or binaural beamformer microphone modes. Results: Relative to the conventional directional microphones, both binaural beamformer modes were generally superior for tasks involving ﬁxed frontal targets, but not always for situations involving dynamic target locations. Conclusions: Binaural beamformers show promise for enhancing listening in complex situations when the location of the source of interest is predictable.},
	language = {en},
	number = {10},
	urldate = {2018-04-06},
	journal = {International Journal of Audiology},
	author = {Best, Virginia and Mejia, Jorge and Freeston, Katrina and van Hoesel, Richard J. and Dillon, Harvey},
	month = oct,
	year = {2015},
	pages = {727--735},
	file = {Best et al. - 2015 - An evaluation of the performance of two binaural b.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LG7YTZMU\\Best et al. - 2015 - An evaluation of the performance of two binaural b.pdf:application/pdf},
}

@article{brimijoin_effect_2014,
	title = {The {Effect} of {Hearing} {Aid} {Microphone} {Mode} on {Performance} in an {Auditory} {Orienting} {Task}},
	volume = {35},
	issn = {0196-0202},
	shorttitle = {The {Effect} of {Hearing} {Aid} {Microphone} {Mode} on {Performance} in an {Auditory} {Orienting} {Task}},
	url = {https://insights.ovid.com/crossref?an=00003446-900000000-99381},
	doi = {10.1097/AUD.0000000000000053},
	abstract = {Objectives: Although directional microphones on a hearing aid provide a signal-to-noise ratio benefit in a noisy background, the amount of benefit is dependent on how close the signal of interest is to the front of the user. It is assumed that when the signal of interest is off-axis, users can reorient themselves to the signal to make use of the directional microphones to improve signal-to-noise ratio. The present study tested this assumption by measuring the head-orienting behavior of bilaterally fit hearing-impaired individuals with their microphones set to omnidirectional and directional modes. The authors hypothesized that listeners using directional microphones would have greater difficulty in rapidly and accurately orienting to off-axis signals than they would when using omnidirectional microphones. Design: The authors instructed hearing-impaired individuals to turn and face a female talker in simultaneous surrounding male-talker babble. Participants pressed a button when they felt they were accurately oriented in the direction of the female talker. Participants completed three blocks of trials with their hearing aids in omnidirectional mode and three blocks in directional mode, with mode order randomized. Using a Vicon motion tracking system, the authors measured head position and computed fixation error, fixation latency, trajectory complexity, and proportion of misorientations.
Results: Results showed that for larger off-axis target angles, listeners using directional microphones took longer to reach their targets than they did when using omnidirectional microphones, although they were just as accurate. They also used more complex movements and frequently made initial turns in the wrong direction. For smaller off-axis target angles, this pattern was reversed, and listeners using directional microphones oriented more quickly and smoothly to the targets than when using omnidirectional microphones.
Conclusions: The authors argue that an increase in movement complexity indicates a switch from a simple orienting movement to a search behavior. For the most off-axis target angles, listeners using directional microphones appear to not know which direction to turn, so they pick a direction at random and simply rotate their heads until the signal becomes more audible. The changes in fixation latency and head orientation trajectories suggest that the decrease in off-axis audibility is a primary concern in the use of directional microphones, and listeners could experience a loss of initial target speech while turning toward a new signal of interest. If hearing-aid users are to receive maximum directional benefit in noisy environments, both adaptive directionality in hearing aids and clinical advice on using directional microphones should take head movement and orientation behavior into account.},
	language = {en},
	urldate = {2018-04-06},
	journal = {Ear and Hearing},
	author = {Brimijoin, W. Owen and Whitmer, William M. and McShefferty, David and Akeroyd, Michael A.},
	month = nov,
	year = {2014},
	pages = {204--212},
	file = {Brimijoin et al. - 2014 - The Effect of Hearing Aid Microphone Mode on Perfo.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MGGWBFLQ\\Brimijoin et al. - 2014 - The Effect of Hearing Aid Microphone Mode on Perfo.pdf:application/pdf},
}

@inproceedings{kjems_maximum_2012,
	address = {Bucharest, Romania},
	title = {Maximum {Likelihood} {Based} {Noise} {Covariance} {Matrix} {Estimation} for {Multi}-{Microphone} {Speech} {Enhancement}},
	abstract = {Multi-microphone speech enhancement systems can often be decomposed into a concatenation of a beamformer, which provides spatial ﬁltering of the noisy signal, and a singlechannel (SC) noise reduction ﬁlter, which reduces the noise remaining in the beamformer output. Here, we propose a maximum likelihood based method for estimating the intermicrophone covariance matrix of the noise impinging on the microphone array. The method allows prediction of this covariance matrix for non-stationary noise sources even in signal regions where the target speech signal is present. Although the noise covariance matrix may have several purposes, we use it in this paper for estimating the power spectral density (psd) of the noise entering the SC ﬁlter, as this is important for optimal operation of the SC ﬁlter. In simulation experiments with a binaural hearing aid setup in a realistic acoustical scenario, the proposed method performs better than existing methods for estimating this noise psd.},
	language = {en},
	booktitle = {20th {European} {Signal} {Processing} {Conference}},
	author = {Kjems, Ulrik and Jensen, Jesper},
	month = aug,
	year = {2012},
	pages = {5},
	file = {Kjems et Jensen - Maximum Likelihood Based Noise Covariance Matrix E.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NQH7ZWFR\\Kjems et Jensen - Maximum Likelihood Based Noise Covariance Matrix E.pdf:application/pdf},
}

@article{kolb_multi-talker_nodate,
	title = {Multi-talker {Speech} {Separation} with {Utterance}-level {Permutation} {Invariant} {Training} of {Deep} {Recurrent} {Neural} {Networks}},
	language = {en},
	author = {Kolb, Morten and Tan, Zheng-Hua},
	pages = {12},
	file = {Kolb et Tan - Multi-talker Speech Separation with Utterance-leve.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5CQYDBGJ\\Kolb et Tan - Multi-talker Speech Separation with Utterance-leve.pdf:application/pdf},
}

@inproceedings{jensen_analysis_2015,
	title = {Analysis of beamformer directed single-channel noise reduction system for hearing aid applications},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7179069/},
	doi = {10.1109/ICASSP.2015.7179069},
	abstract = {We study multi-microphone noise reduction systems consisting of a beamformer and a single-channel (SC) noise reduction stage. In par­ ticular,we present and analyse a maximum likelihood (ML) method for jointly estimating the target and noise power spectral densities (psd's) entering the SC filter. We show that the estimators are min­ imum variance and unbiased, and provide closed-form expressions for their mean-square error (MSE). Furthermore, we show that the MSE of the noise psd estimator is particularly simple: it is inde­ pendent of target signal characteristics, frequency, and microphone locations. In a hearing aid context, we analyze the performance of the estimators as a function of target angle-of-arrival and frequency. Finally, we demonstrate the advantage of the proposed method in a hearing aid situation with a target speaker in large-crowd noise.},
	language = {en},
	urldate = {2018-04-06},
	publisher = {IEEE},
	author = {Jensen, Jesper and Pedersen, Michael Syskind},
	month = apr,
	year = {2015},
	pages = {5728--5732},
	file = {Jensen et Pedersen - 2015 - Analysis of beamformer directed single-channel noi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KSX3ZPF2\\Jensen et Pedersen - 2015 - Analysis of beamformer directed single-channel noi.pdf:application/pdf},
}

@inproceedings{elko_simple_1995,
	address = {New Paltz, NY, USA},
	title = {A simple adaptive first-order differential microphone},
	isbn = {978-0-7803-3064-1},
	url = {http://ieeexplore.ieee.org/document/482983/},
	doi = {10.1109/ASPAA.1995.482983},
	language = {en},
	urldate = {2018-04-06},
	booktitle = {Proceedings of 1995 {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Accoustics}},
	publisher = {IEEE},
	author = {Elko, G.W. and {Anh-Tho Nguyen Pong}},
	year = {1995},
	pages = {169--172},
	file = {Elko et Anh-Tho Nguyen Pong - 1995 - A simple adaptive first-order differential microph.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7HDDTSTS\\Elko et Anh-Tho Nguyen Pong - 1995 - A simple adaptive first-order differential microph.pdf:application/pdf},
}

@article{iida_personalization_2014,
	title = {Personalization of head-related transfer functions in the median plane based on the anthropometry of the listener's pinnae},
	volume = {136},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4880856},
	doi = {10.1121/1.4880856},
	language = {en},
	number = {1},
	urldate = {2018-04-06},
	journal = {The Journal of the Acoustical Society of America},
	author = {Iida, Kazuhiro and Ishii, Yohji and Nishioka, Shinsuke},
	month = jul,
	year = {2014},
	pages = {317--333},
	file = {Iida et al. - 2014 - Personalization of head-related transfer functions.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RJQAN8DQ\\Iida et al. - 2014 - Personalization of head-related transfer functions.pdf:application/pdf},
}

@article{baumgartner_modeling_2016,
	title = {Modeling the {Effects} of {Sensorineural} {Hearing} {Loss} on {Sound} {Localization} in the {Median} {Plane}},
	volume = {20},
	issn = {2331-2165, 2331-2165},
	url = {http://journals.sagepub.com/doi/10.1177/2331216516662003},
	doi = {10.1177/2331216516662003},
	abstract = {Listeners use monaural spectral cues to localize sound sources in sagittal planes (along the up-down and front-back directions). How sensorineural hearing loss affects the salience of monaural spectral cues is unclear. To simulate the effects of outer-hair-cell (OHC) dysfunction and the contribution of different auditory-nerve fiber types on localization performance, we incorporated a nonlinear model of the auditory periphery into a model of sagittal-plane sound localization for normal-hearing listeners. The localization model was first evaluated in its ability to predict the effects of spectral cue modifications for normal-hearing listeners. Then, we used it to simulate various degrees of OHC dysfunction applied to different types of auditory-nerve fibers. Predicted localization performance was hardly affected by mild OHC dysfunction but was strongly degraded in conditions involving severe and complete OHC dysfunction. These predictions resemble the usually observed degradation in localization performance induced by sensorineural hearing loss. Predicted localization performance was best when preserving fibers with medium spontaneous rates, which is particularly important in view of noise-induced hearing loss associated with degeneration of this fiber type. On average across listeners, predicted localization performance was strongly related to level discrimination sensitivity of auditory-nerve fibers, indicating an essential role of this coding property for localization accuracy in sagittal planes.},
	language = {en},
	urldate = {2018-04-06},
	journal = {Trends in Hearing},
	author = {Baumgartner, Robert and Majdak, Piotr and Laback, Bernhard},
	month = sep,
	year = {2016},
	pages = {233121651666200},
	file = {Baumgartner et al. - 2016 - Modeling the Effects of Sensorineural Hearing Loss.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UB8DTSQR\\Baumgartner et al. - 2016 - Modeling the Effects of Sensorineural Hearing Loss.pdf:application/pdf},
}

@inproceedings{chen_general_2013,
	title = {A general approach to the design and implementation of linear differential microphone arrays},
	isbn = {978-986-90006-0-4},
	url = {http://ieeexplore.ieee.org/document/6694113/},
	doi = {10.1109/APSIPA.2013.6694113},
	abstract = {The design of differential microphone arrays (DMAs) and the associated beamforming algorithms have become very important problems. Traditionally, an N th order DMA is formed by subtractively combining the outputs of two DMAs of order N − 1. This method, though simple and easy to implement, suffers from a number of limitations. For example, it is difﬁcult to design the equalization ﬁlter that is needed for compensating the array’s non-uniform frequency response, particularly for high-order DMAs. In this paper, we propose a new approach to the design and implementation of linear DMAs for speech enhancement. Unlike the traditional method that works in the time domain, this proposed approach works in the short-time Fourier transform (STFT) domain. The core issue with this framework is how to design the desired differential beamformer in each subband, which is accomplished by solving a linear system consisting of N + 1 fundamental constraints for an N th-order DMA.},
	language = {en},
	urldate = {2018-04-06},
	publisher = {IEEE},
	author = {Chen, Jingdong and Benesty, Jacob},
	month = oct,
	year = {2013},
	pages = {1--7},
	file = {Chen et Benesty - 2013 - A general approach to the design and implementatio.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7L2IXMY6\\Chen et Benesty - 2013 - A general approach to the design and implementatio.pdf:application/pdf},
}

@inproceedings{messner_adaptive_2015,
	title = {Adaptive differential microphone arrays used as a front-end for an automatic speech recognition system},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178459/},
	doi = {10.1109/ICASSP.2015.7178459},
	abstract = {For automatic speech recognition (ASR) systems it is important that the input signal mainly contains the desired speech signal. For a compact arrangement, differential microphone arrays (DMAs) are a suitable choice as front-end of ASR systems. The limiting factor of DMAs is the white noise gain, which can be treated by the minimum norm solution (MNS). In this paper, we introduce the ﬁrst time the MNS to adaptive differential microphone arrays. We compare its effect to the conventional implementation when used as front-end of an ASR system. In experiments we show that the proposed algorithms consistently increase the word accuracy up to 50 \% relative to their conventional implementations. For PESQ we achieve an improvement of up to 0.1 points.},
	language = {en},
	urldate = {2018-04-06},
	publisher = {IEEE},
	author = {Messner, Elmar and Pessentheiner, Hannes and Morales-Cordovilla, Juan A. and Hagmuller, Martin},
	month = apr,
	year = {2015},
	pages = {2689--2693},
	file = {Messner et al. - 2015 - Adaptive differential microphone arrays used as a .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\E7W3CZRZ\\Messner et al. - 2015 - Adaptive differential microphone arrays used as a .pdf:application/pdf},
}

@article{vanden_berghe_adaptive_1998,
	title = {An adaptive noise canceller for hearing aids using two nearby microphones},
	volume = {103},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.423066},
	doi = {10.1121/1.423066},
	language = {en},
	number = {6},
	urldate = {2018-04-06},
	journal = {The Journal of the Acoustical Society of America},
	author = {Vanden Berghe, Jeff and Wouters, Jan},
	month = jun,
	year = {1998},
	pages = {3621--3626},
	file = {Vanden Berghe et Wouters - 1998 - An adaptive noise canceller for hearing aids using.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EBQWQTDA\\Vanden Berghe et Wouters - 1998 - An adaptive noise canceller for hearing aids using.pdf:application/pdf},
}

@inproceedings{elko_second-order_2009,
	title = {Second-order differential adaptive microphone array},
	isbn = {978-1-4244-2353-8},
	url = {http://ieeexplore.ieee.org/document/4959523/},
	doi = {10.1109/ICASSP.2009.4959523},
	abstract = {An adaptive second-order differential microphone design is proposed here that is constructed from a weighted sum of omnidirectional microphones. Theoretically, only three microphones are required to form a second-order array. The three microphone signals are combined to form three unique ﬁxed second-order beams. Any second-order differential beampattern can be realized using a weighted sum of these three “building-block” beam outputs. If certain simple constraints are placed on the weighting of the three ﬁxed beams, the two null locations that deﬁne the ﬁnal second-order beampattern can be constrained to deﬁned angular regions.},
	language = {en},
	urldate = {2018-04-06},
	publisher = {IEEE},
	author = {Elko, Gary W. and Meyer, Jens},
	month = apr,
	year = {2009},
	pages = {73--76},
	file = {Elko et Meyer - 2009 - Second-order differential adaptive microphone arra.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Z7J7AFFU\\Elko et Meyer - 2009 - Second-order differential adaptive microphone arra.pdf:application/pdf},
}

@inproceedings{doblinger_adaptive_2006,
	address = {Florence, Italy},
	title = {An {Adaptive} {Microphone} {Array} for {Optimum} {Beamforming} and {Noise} {Reduction}},
	language = {en},
	booktitle = {14th {European} {Signal} {Processing} {Conference} ({EUSIPCO} 2006)},
	author = {Doblinger, Gerhard},
	month = sep,
	year = {2006},
	pages = {5},
	file = {Doblinger - 2006 - AN ADAPTIVE MICROPHONE ARRAY FOR OPTIMUM BEAMFORMI.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VEMVBT5X\\Doblinger - 2006 - AN ADAPTIVE MICROPHONE ARRAY FOR OPTIMUM BEAMFORMI.pdf:application/pdf},
}

@article{hassager_preserving_2017,
	title = {Preserving spatial perception in rooms using direct-sound driven dynamic range compression},
	volume = {141},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4984040},
	doi = {10.1121/1.4984040},
	language = {en},
	number = {6},
	urldate = {2018-04-10},
	journal = {The Journal of the Acoustical Society of America},
	author = {Hassager, Henrik Gert and May, Tobias and Wiinberg, Alan and Dau, Torsten},
	month = jun,
	year = {2017},
	keywords = {NH, HI},
	pages = {4556--4566},
	file = {Hassager et al. - 2017 - Preserving spatial perception in rooms using direc.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BXN6375G\\Hassager et al. - 2017 - Preserving spatial perception in rooms using direc.pdf:application/pdf},
}

@article{hassager_effects_2017,
	title = {Effects of hearing-aid dynamic range compression on spatial perception in a reverberant environment},
	volume = {141},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4979783},
	doi = {10.1121/1.4979783},
	language = {en},
	number = {4},
	urldate = {2018-04-10},
	journal = {The Journal of the Acoustical Society of America},
	author = {Hassager, Henrik Gert and Wiinberg, Alan and Dau, Torsten},
	month = apr,
	year = {2017},
	keywords = {NH, HI},
	pages = {2556--2568},
	file = {Hassager et al. - 2017 - Effects of hearing-aid dynamic range compression o.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RSVTDETR\\Hassager et al. - 2017 - Effects of hearing-aid dynamic range compression o.pdf:application/pdf},
}

@article{buchris_analysis_nodate,
	title = {Analysis and design of time-domain ﬁrst-order circular differential microphone arrays},
	abstract = {Circular differential microphone arrays (CDMAs) are characterized as compact superdirective beamformers whose beampatterns are almost frequency invariant. In contrast to linear differential microphone arrays (LDMAs) where the optimal steering direction is at the endﬁre, CDMAs provide almost perfect steering for all azimuthal directions. Herein, we present the design of a ﬁrst-order CDMA in the time domain which is motivated by several aspects. First, time-domain implementation is important in some applications where minimal delay is required, such as realtime communications. Moreover, direct design in the time domain can reduce the computational efforts compared to the frequency-domain design, especially when short ﬁlters are sufﬁcient. We present a design example for the time-domain ﬁrst-order CDMA illustrating some of its fundamental properties as well as the equivalence to the frequency-domain alternative.},
	language = {en},
	author = {Buchris, Yaakov and Cohen, Israel and Benesty, Jacob},
	pages = {9},
	file = {Buchris et al. - Analysis and design of time-domain ﬁrst-order circ.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XRZQBXCL\\Buchris et al. - Analysis and design of time-domain ﬁrst-order circ.pdf:application/pdf},
}

@article{desloge_masking_2017,
	title = {Masking release for hearing-impaired listeners: {The} effect of increased audibility through reduction of amplitude variability},
	volume = {141},
	issn = {0001-4966},
	shorttitle = {Masking release for hearing-impaired listeners},
	url = {http://asa.scitation.org/doi/10.1121/1.4985186},
	doi = {10.1121/1.4985186},
	language = {en},
	number = {6},
	urldate = {2018-04-10},
	journal = {The Journal of the Acoustical Society of America},
	author = {Desloge, Joseph G. and Reed, Charlotte M. and Braida, Louis D. and Perez, Zachary D. and D'Aquila, Laura A.},
	month = jun,
	year = {2017},
	pages = {4452--4465},
	file = {Desloge et al. - 2017 - Masking release for hearing-impaired listeners Th.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\SMIAHHTC\\Desloge et al. - 2017 - Masking release for hearing-impaired listeners Th.pdf:application/pdf},
}

@article{greenberg_evaluation_2003,
	title = {Evaluation of array-processing algorithms for a headband hearing aid},
	volume = {113},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1536624},
	doi = {10.1121/1.1536624},
	language = {en},
	number = {3},
	urldate = {2018-04-10},
	journal = {The Journal of the Acoustical Society of America},
	author = {Greenberg, Julie E. and Desloge, Joseph G. and Zurek, Patrick M.},
	month = mar,
	year = {2003},
	pages = {1646--1657},
	file = {Greenberg et al. - 2003 - Evaluation of array-processing algorithms for a he.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JTQYYU2E\\Greenberg et al. - 2003 - Evaluation of array-processing algorithms for a he.pdf:application/pdf},
}

@article{mendonca_review_2014,
	title = {A review on auditory space adaptations to altered head-related cues},
	volume = {8},
	issn = {1662-453X},
	url = {http://journal.frontiersin.org/article/10.3389/fnins.2014.00219/abstract},
	doi = {10.3389/fnins.2014.00219},
	language = {en},
	urldate = {2018-04-10},
	journal = {Frontiers in Neuroscience},
	author = {Mendonça, Catarina},
	month = jul,
	year = {2014},
	pages = {14},
	file = {MendonÃ§a - 2014 - A review on auditory space adaptations to altered .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4GDZBM9X\\MendonÃ§a - 2014 - A review on auditory space adaptations to altered .pdf:application/pdf},
}

@article{hassager_role_2016,
	title = {The role of spectral detail in the binaural transfer function on perceived externalization in a reverberant environment},
	volume = {139},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4950847},
	doi = {10.1121/1.4950847},
	language = {en},
	number = {5},
	urldate = {2018-04-10},
	journal = {The Journal of the Acoustical Society of America},
	author = {Hassager, Henrik Gert and Gran, Fredrik and Dau, Torsten},
	month = may,
	year = {2016},
	pages = {2992--3000},
	file = {Hassager et al. - 2016 - The role of spectral detail in the binaural transf.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5LGE7RZC\\Hassager et al. - 2016 - The role of spectral detail in the binaural transf.pdf:application/pdf},
}

@article{courtois_effects_2018,
	title = {Effects of {Binaural} {Spatialization} in {Wireless} {Microphone} {Systems} for {Hearing} {Aids} on {Normal}-{Hearing} and {Hearing}-{Impaired} {Listeners}},
	volume = {22},
	issn = {2331-2165, 2331-2165},
	url = {http://journals.sagepub.com/doi/10.1177/2331216517753548},
	doi = {10.1177/2331216517753548},
	abstract = {Little is known about the perception of artificial spatial hearing by hearing-impaired subjects. The purpose of this study was to investigate how listeners with hearing disorders perceived the effect of a spatialization feature designed for wireless microphone systems. Forty listeners took part in the experiments. They were arranged in four groups: normal-hearing, moderate, severe, and profound hearing loss. Their performance in terms of speech understanding and speaker localization was assessed with diotic and binaural stimuli. The results of the speech intelligibility experiment revealed that the subjects presenting a moderate or severe hearing impairment better understood speech with the spatialization feature. Thus, it was demonstrated that the conventional diotic binaural summation operated by current wireless systems can be transformed to reproduce the spatial cues required to localize the speaker, without any loss of intelligibility. The speaker localization experiment showed that a majority of the hearing-impaired listeners had similar performance with natural and artificial spatial hearing, contrary to the normal-hearing listeners. This suggests that certain subjects with hearing impairment preserve their localization abilities with approximated generic head-related transfer functions in the frontal horizontal plane.},
	language = {en},
	urldate = {2018-04-10},
	journal = {Trends in Hearing},
	author = {Courtois, Gilles and Lissek, Hervé and Estoppey, Philippe and Oesch, Yves and Gigandet, Xavier},
	month = jan,
	year = {2018},
	pages = {233121651775354},
	file = {Courtois et al. - 2018 - Effects of Binaural Spatialization in Wireless Mic.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LNBA3KPU\\Courtois et al. - 2018 - Effects of Binaural Spatialization in Wireless Mic.pdf:application/pdf},
}

@phdthesis{courtois_spatial_2016,
	address = {Suisse},
	title = {Spatial hearing rendering in wireless microphone systems for binaural hearing aids},
	language = {en},
	school = {Ecole Polytechnique Fédérale de Lausanne},
	author = {Courtois, Gilles André},
	year = {2016},
	file = {Courtois - Spatial hearing rendering in wireless microphone s.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MNA6CZV9\\Courtois - Spatial hearing rendering in wireless microphone s.pdf:application/pdf},
}

@article{keating_developmental_2013,
	title = {Developmental plasticity of spatial hearing following asymmetric hearing loss: context-dependent cue integration and its clinical implications},
	volume = {7},
	issn = {1662-5137},
	shorttitle = {Developmental plasticity of spatial hearing following asymmetric hearing loss},
	url = {http://journal.frontiersin.org/article/10.3389/fnsys.2013.00123/abstract},
	doi = {10.3389/fnsys.2013.00123},
	abstract = {Under normal hearing conditions, comparisons of the sounds reaching each ear are critical for accurate sound localization. Asymmetric hearing loss should therefore degrade spatial hearing and has become an important experimental tool for probing the plasticity of the auditory system, both during development and adulthood. In clinical populations, hearing loss affecting one ear more than the other is commonly associated with otitis media with effusion, a disorder experienced by approximately 80\% of children before the age of two. Asymmetric hearing may also arise in other clinical situations, such as after unilateral cochlear implantation. Here, we consider the role played by spatial cue integration in sound localization under normal acoustical conditions. We then review evidence for adaptive changes in spatial hearing following a developmental hearing loss in one ear, and show that adaptation may be achieved either by learning a new relationship between the altered cues and directions in space or by changing the way different cues are integrated in the brain. We next consider developmental plasticity as a source of vulnerability, describing maladaptive effects of asymmetric hearing loss that persist even when normal hearing is provided. We also examine the extent to which the consequences of asymmetric hearing loss depend upon its timing and duration. Although much of the experimental literature has focused on the effects of a stable unilateral hearing loss, some of the most common hearing impairments experienced by children tend to ﬂuctuate over time. We therefore propose that there is a need to bridge this gap by investigating the effects of recurring hearing loss during development, and outline recent steps in this direction. We conclude by arguing that this work points toward a more nuanced view of developmental plasticity, in which plasticity may be selectively expressed in response to speciﬁc sensory contexts, and consider the clinical implications of this.},
	language = {en},
	urldate = {2018-04-11},
	journal = {Frontiers in Systems Neuroscience},
	author = {Keating, Peter and King, Andrew J.},
	year = {2013},
	file = {Keating et King - 2013 - Developmental plasticity of spatial hearing follow.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IPKAF9PY\\Keating et King - 2013 - Developmental plasticity of spatial hearing follow.pdf:application/pdf},
}

@article{marrone_effects_2008,
	title = {The effects of hearing loss and age on the benefit of spatial separation between multiple talkers in reverberant rooms},
	volume = {124},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2980441},
	doi = {10.1121/1.2980441},
	language = {en},
	number = {5},
	urldate = {2018-04-11},
	journal = {The Journal of the Acoustical Society of America},
	author = {Marrone, Nicole and Mason, Christine R. and Kidd, Gerald},
	month = nov,
	year = {2008},
	pages = {3064--3075},
	file = {Marrone et al. - 2008 - The effects of hearing loss and age on the benefit.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\B42LBQ7A\\Marrone et al. - 2008 - The effects of hearing loss and age on the benefit.pdf:application/pdf},
}

@article{glasberg_derivation_1990,
	title = {Derivation of auditory filter shapes from notched-noise data},
	volume = {47},
	issn = {03785955},
	url = {http://linkinghub.elsevier.com/retrieve/pii/037859559090170T},
	doi = {10.1016/0378-5955(90)90170-T},
	language = {en},
	number = {1-2},
	urldate = {2018-04-11},
	journal = {Hearing Research},
	author = {Glasberg, Brian R and Moore, Brian C.J},
	month = aug,
	year = {1990},
	pages = {103--138},
	file = {Glasberg et Moore - 1990 - Derivation of auditory filter shapes from notched-.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BRAD2S5S\\Glasberg et Moore - 1990 - Derivation of auditory filter shapes from notched-.pdf:application/pdf},
}

@article{keidser_nal-nl2_2011,
	title = {The {NAL}-{NL2} prescription procedure},
	volume = {1},
	issn = {2039-4349, 2039-4330},
	url = {http://www.audiologyresearch.org/index.php/audio/article/view/25},
	doi = {10.4081/audiores.2011.e24},
	language = {en},
	number = {1S},
	urldate = {2018-04-11},
	journal = {Audiology Research},
	author = {Keidser, G. and Dillon, H.R. and Flax, M. and Ching, T. and Brewer, S.},
	month = may,
	year = {2011},
	file = {Keidser et al. - 2011 - The NAL-NL2 prescription procedure.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QBVY4CUM\\Keidser et al. - 2011 - The NAL-NL2 prescription procedure.pdf:application/pdf},
}

@article{rakerd_localization_2010,
	title = {Localization of sound in rooms. {V}. {Binaural} coherence and human sensitivity to interaural time differences in noise},
	volume = {128},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3493447},
	doi = {10.1121/1.3493447},
	language = {en},
	number = {5},
	urldate = {2018-04-12},
	journal = {The Journal of the Acoustical Society of America},
	author = {Rakerd, Brad and Hartmann, William M.},
	month = nov,
	year = {2010},
	pages = {3052--3063},
	file = {Rakerd et Hartmann - 2010 - Localization of sound in rooms. V. Binaural cohere.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GWEP3G5V\\Rakerd et Hartmann - 2010 - Localization of sound in rooms. V. Binaural cohere.pdf:application/pdf},
}

@article{hartmann_binaural_2005,
	title = {Binaural {Coherence} in {Rooms}},
	volume = {91},
	abstract = {In a study of the binaural properties of rooms, binaural cross-correlation (CC) functions were measured in 19 one-third-octave bands, and broad band, using an artiﬁcial head and torso (KEMAR). For a ﬁxed source-receiver distance, twenty CC functions were collected as the source and receiver were moved to different locations in the room. The peaks of the CC functions were taken as measures of binaural coherence. Plots were made of binaural coherence as a function of frequency for ﬁve rooms and several source-receiver distances (3 m, 6 m, and 12 m). Attempts were made to interpret the coherence plots in terms of geometrical, material, and acoustical properties of the different rooms in order to develop some intuition about the statistical behavior of the coherence function in rooms. Correctly interpreted, in coordination with headphone lateralization studies, the measured values of binaural coherence are expected to indicate the utility of the steady-state interaural time difference cue in sound localization.},
	language = {en},
	journal = {Acta Acustica united with Acustica},
	author = {Hartmann, William M and Rakerd, Brad and Koller, Aaron},
	year = {2005},
	pages = {12},
	file = {Hartmann et al. - 2005 - Binaural Coherence in Rooms.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZMTXCBR8\\Hartmann et al. - 2005 - Binaural Coherence in Rooms.pdf:application/pdf;Hartmann et al. - 2005 - Binaural Coherence in Rooms.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CHM2JQD3\\Hartmann et al. - 2005 - Binaural Coherence in Rooms.pdf:application/pdf},
}

@inproceedings{young_woo_lee_parameter-based_2015,
	title = {Parameter-based binaural hearing aid algorithms to improve speech intelligibility and localization in complex environments},
	isbn = {978-1-4244-9271-8},
	url = {http://ieeexplore.ieee.org/document/7319658/},
	doi = {10.1109/EMBC.2015.7319658},
	abstract = {This paper presents new binaural enhancement and noise suppression algorithms for binaural hearing aids. To enhance interaural level difference (ILD) cues at low frequencies, which are usually small, interaural time difference (ITD) cues are estimated and transformed to ILDs. The binaural noise suppression algorithm consists of adaptive beamforming and a coherence-based suppression filter. The estimated phase and signal-to-noise ratio (SNR) at each hearing aid are used to perform the processing. The performance of the proposed methods was assessed using perceptual evaluation with hearing-impaired listeners and objective evaluation.},
	language = {en},
	urldate = {2018-04-12},
	publisher = {IEEE},
	author = {{Young Woo Lee} and Moore, Brian C. J.},
	month = aug,
	year = {2015},
	pages = {5585--5588},
	file = {Young Woo Lee et Moore - 2015 - Parameter-based binaural hearing aid algorithms to.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2F8V4XKS\\Young Woo Lee et Moore - 2015 - Parameter-based binaural hearing aid algorithms to.pdf:application/pdf},
}

@article{ernst_binaural_2018,
	title = {Binaural model-based dynamic-range compression},
	issn = {1499-2027, 1708-8186},
	url = {https://www.tandfonline.com/doi/full/10.1080/14992027.2018.1425554},
	doi = {10.1080/14992027.2018.1425554},
	abstract = {Objective: Binaural cues such as interaural level differences (ILDs) are used to organise auditory perception and to segregate sound sources in complex acoustical environments. In bilaterally fitted hearing aids, dynamic-range compression operating independently at each ear potentially alters these ILDs, thus distorting binaural perception and sound source segregation. Design: A binaurally-linked model-based fast-acting dynamic compression algorithm designed to approximate the normal-hearing basilar membrane (BM) input–output function in hearing-impaired listeners is suggested. A multi-center evaluation in comparison with an alternative binaural and two bilateral fittings was performed to assess the effect of binaural synchronisation on (a) speech intelligibility and (b) perceived quality in realistic conditions. Study sample: 30 and 12 hearing impaired (HI) listeners were aided individually with the algorithms for both experimental parts, respectively. Results: A small preference towards the proposed model-based algorithm in the direct quality comparison was found. However, no benefit of binaural-synchronisation regarding speech intelligibility was found, suggesting a dominant role of the better ear in all experimental conditions. Conclusion: The suggested binaural synchronisation of compression algorithms showed a limited effect on the tested outcome measures, however, linking could be situationally beneficial to preserve a natural binaural perception of the acoustical environment.},
	language = {en},
	urldate = {2018-04-12},
	journal = {International Journal of Audiology},
	author = {Ernst, Stephan M. A. and Kortlang, Steffen and Grimm, Giso and Bisitz, Thomas and Kollmeier, Birger and Ewert, Stephan D.},
	month = jan,
	year = {2018},
	pages = {1--12},
	file = {Ernst et al. - 2018 - Binaural model-based dynamic-range compression.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XSU3I3WD\\Ernst et al. - 2018 - Binaural model-based dynamic-range compression.pdf:application/pdf},
}

@article{strelcyk_effects_2014,
	title = {Effects of interferer facing orientation on speech perception by normal-hearing and hearing-impaired listeners},
	volume = {135},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4864786},
	doi = {10.1121/1.4864786},
	language = {en},
	number = {3},
	urldate = {2018-04-12},
	journal = {The Journal of the Acoustical Society of America},
	author = {Strelcyk, Olaf and Pentony, Shareka and Kalluri, Sridhar and Edwards, Brent},
	month = mar,
	year = {2014},
	pages = {1419--1432},
	file = {Strelcyk et al. - 2014 - Effects of interferer facing orientation on speech.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\W5FYA5K6\\Strelcyk et al. - 2014 - Effects of interferer facing orientation on speech.pdf:application/pdf},
}

@article{david_effect_2018,
	title = {Effect of age and hearing loss on auditory stream segregation of speech sounds},
	issn = {03785955},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0378595517305713},
	doi = {10.1016/j.heares.2018.03.017},
	abstract = {Segregating and understanding speech in complex environments is a major challenge for hearingimpaired (HI) listeners. It remains unclear to what extent these difﬁculties are dominated by direct interference, such as simultaneous masking, or by a failure of the mechanisms of stream segregation. This study compared older HI listeners' performance with that of young and older normal-hearing (NH) listeners in stream segregation tasks involving speech sounds. Listeners were presented with sequences of speech tokens, each consisting of a fricative consonant and a voiced vowel (CV). The CV tokens were concatenated into interleaved sequences that alternated in fundamental frequency (F0) and/or simulated vocal tract length (VTL). Each pair of interleaved sequences was preceded by a “word” consisting of two random tokens. The listeners were asked to indicate whether the word was present in the following interleaved sequences. The word, if present, occurred within one of the interleaved sequences, so that performance improved if the listeners were able to perceptually segregate the two sequences. Although HI listeners' identiﬁcation of the speech tokens in isolation was poorer than that of the NH listeners, HI listeners were generally able to use both F0 and VTL cues to segregate the interleaved sequences. The results suggest that the difﬁculties experienced by HI listeners in complex acoustic environments cannot be explained by a loss of basic stream segregation abilities.},
	language = {en},
	urldate = {2018-04-12},
	journal = {Hearing Research},
	author = {David, Marion and Tausend, Alexis N. and Strelcyk, Olaf and Oxenham, Andrew J.},
	month = mar,
	year = {2018},
	file = {David et al. - 2018 - Effect of age and hearing loss on auditory stream .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\U9FCSY8V\\David et al. - 2018 - Effect of age and hearing loss on auditory stream .pdf:application/pdf},
}

@article{westermann_influence_2015,
	title = {The influence of informational masking in reverberant, multi-talker environments},
	volume = {138},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4923449},
	doi = {10.1121/1.4923449},
	language = {en},
	number = {2},
	urldate = {2018-04-12},
	journal = {The Journal of the Acoustical Society of America},
	author = {Westermann, Adam and Buchholz, Jörg M.},
	month = aug,
	year = {2015},
	pages = {584--593},
	file = {Westermann et Buchholz - 2015 - The influence of informational masking in reverber.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YJCK7ZKI\\Westermann et Buchholz - 2015 - The influence of informational masking in reverber.pdf:application/pdf},
}

@article{srinivasan_role_2017,
	title = {The role of early and late reflections on spatial release from masking: {Effects} of age and hearing lossa)},
	volume = {141},
	issn = {0001-4966},
	shorttitle = {The role of early and late reflections on spatial release from masking},
	url = {http://aip.scitation.org/doi/10.1121/1.4973837},
	doi = {10.1121/1.4973837},
	abstract = {Early reﬂections have been linked to improved speech intelligibility, while later-arriving reverberant sound has been shown to limit speech understanding. Here, these effects were examined by artiﬁcially removing either early reﬂections or late reﬂections. Removing late reﬂections improved performance more for colocated than for spatially separated maskers. Results of a multiple regression analysis suggest that pure-tone average (PTA) is a signiﬁcant predictor of spatial release from masking (SRM) in all acoustic conditions. Controlling for the effects of PTA, age is a signiﬁcant predictor of SRM only when early reﬂections are absent.},
	language = {en},
	number = {3},
	urldate = {2018-04-12},
	journal = {The Journal of the Acoustical Society of America},
	author = {Srinivasan, Nirmal Kumar and Stansell, Meghan and Gallun, Frederick J.},
	month = mar,
	year = {2017},
	pages = {EL185--EL191},
	file = {Srinivasan et al. - 2017 - The role of early and late reflections on spatial .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KGESJCLV\\Srinivasan et al. - 2017 - The role of early and late reflections on spatial .pdf:application/pdf},
}

@inproceedings{calamia_conformal_2017,
	title = {A conformal, helmet-mounted microphone array for auditory situational awareness and hearing protection},
	isbn = {978-1-5386-1632-1},
	url = {http://ieeexplore.ieee.org/document/8170002/},
	doi = {10.1109/WASPAA.2017.8170002},
	abstract = {Hearing protectors often are considered detrimental to auditory situational awareness due in part to the distortions they impose on localization cues. The addition of a helmet or other head-worn equipment can exacerbate this problem by covering the ears and effectively changing the shape of the head. In an effort to design a hearing-protection system that maintains natural auditory localization performance, we have built a prototype conformal microphone array integrated into a helmet. The system involves beamforming for spatial selectivity, ﬁltering with head-related transfer functions to reintroduce spatial auditory cues to the beamformed data, dynamic range compression to attenuate potentially damaging sound levels, and insert earphones to deliver binaural audio and provide additional hearing protection. All directions are auralized simultaneously to provide full spatial coverage around the wearer. In this paper we describe the design, implementation, and testing of a prototype 32-channel system, including 3D modeling and simulations, signal-processing approaches, real-time processing, and localization performance.},
	language = {en},
	urldate = {2018-04-12},
	publisher = {IEEE},
	author = {Calamia, Paul and Davis, Shakti and Smalt, Christopher and Weston, Christine},
	month = oct,
	year = {2017},
	pages = {96--100},
	file = {Calamia et al. - 2017 - A conformal, helmet-mounted microphone array for a.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\B4CMQWRT\\Calamia et al. - 2017 - A conformal, helmet-mounted microphone array for a.pdf:application/pdf},
}

@article{marrone_tuning_2008,
	title = {Tuning in the spatial dimension: {Evidence} from a masked speech identification task},
	volume = {124},
	issn = {0001-4966},
	shorttitle = {Tuning in the spatial dimension},
	url = {http://asa.scitation.org/doi/10.1121/1.2945710},
	doi = {10.1121/1.2945710},
	language = {en},
	number = {2},
	urldate = {2018-04-13},
	journal = {The Journal of the Acoustical Society of America},
	author = {Marrone, Nicole and Mason, Christine R. and Kidd, Gerald},
	month = aug,
	year = {2008},
	pages = {1146--1158},
	file = {Marrone et al. - 2008 - Tuning in the spatial dimension Evidence from a m.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MB7BNJGI\\Marrone et al. - 2008 - Tuning in the spatial dimension Evidence from a m.pdf:application/pdf},
}

@article{levitt_transformed_1971,
	title = {Transformed {Up}‐{Down} {Methods} in {Psychoacoustics}},
	volume = {49},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1912375},
	doi = {10.1121/1.1912375},
	language = {en},
	number = {2B},
	urldate = {2018-04-13},
	journal = {The Journal of the Acoustical Society of America},
	author = {Levitt, H.},
	month = feb,
	year = {1971},
	pages = {467--477},
	file = {Levitt - 1971 - Transformed Up‐Down Methods in Psychoacoustics.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RM2WM57D\\Levitt - 1971 - Transformed Up‐Down Methods in Psychoacoustics.pdf:application/pdf},
}

@article{gatehouse_speech_2004,
	title = {The {Speech}, {Spatial} and {Qualities} of {Hearing} {Scale} ({SSQ})},
	volume = {43},
	issn = {1499-2027, 1708-8186},
	url = {https://www.tandfonline.com/doi/full/10.1080/14992020400050014},
	doi = {10.1080/14992020400050014},
	abstract = {The Speech, Spatial and Qualities of Hearing Scale (SSQ) is designed to measure a range of hearing disabilities across several domains. Particular attention is given to hearing speech in a variety of competing contexts, and to the directional, distance and movement components of spatial hearing. In addition, the abilities both to segregate sounds and to attend to simultaneous speech streams are assessed, reflecting the reality of hearing in the everyday world. Qualities of hearing experience include ease of listening, and the naturalness, clarity and identifiability of different speakers, different musical pieces and instruments, and different everyday sounds. Application of the SSQ to 153 new clinic clients prior to hearing aid fitting showed that the greatest difficulty was experienced with simultaneous speech streams, ease of listening, listening in groups and in noise, and judging distance and movement. SSQ ratings were compared with an independent measure of handicap. After differences in hearing level were controlled for, it was found that identification, attention and effort problems, as well as spatial hearing problems, feature prominently in the disability–handicap relationship, along with certain features of speech hearing. The results implicate aspects of temporal and spatial dynamics of hearing disability in the experience of handicap. The SSQ shows promise as an instrument for evaluating interventions of various kinds, particularly (but not exclusively) those that implicate binaural function.},
	language = {en},
	number = {2},
	urldate = {2018-04-13},
	journal = {International Journal of Audiology},
	author = {Gatehouse, Stuart and Noble, William},
	month = jan,
	year = {2004},
	pages = {85--99},
	file = {Gatehouse et Noble - 2004 - The Speech, Spatial and Qualities of Hearing Scale.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NL9RSJP3\\Gatehouse et Noble - 2004 - The Speech, Spatial and Qualities of Hearing Scale.pdf:application/pdf},
}

@article{arbogast_effect_2002,
	title = {The effect of spatial separation on informational and energetic masking of speech},
	volume = {112},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1510141},
	doi = {10.1121/1.1510141},
	language = {en},
	number = {5},
	urldate = {2018-04-13},
	journal = {The Journal of the Acoustical Society of America},
	author = {Arbogast, Tanya L. and Mason, Christine R. and Kidd, Gerald},
	month = nov,
	year = {2002},
	pages = {2086--2098},
	file = {Arbogast et al. - 2002 - The effect of spatial separation on informational .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JINSUNP7\\Arbogast et al. - 2002 - The effect of spatial separation on informational .pdf:application/pdf},
}

@article{arbogast_effect_2005,
	title = {The effect of spatial separation on informational masking of speech in normal-hearing and hearing-impaired listeners},
	volume = {117},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1861598},
	doi = {10.1121/1.1861598},
	language = {en},
	number = {4},
	urldate = {2018-04-13},
	journal = {The Journal of the Acoustical Society of America},
	author = {Arbogast, Tanya L. and Mason, Christine R. and Kidd, Gerald},
	month = apr,
	year = {2005},
	pages = {2169--2180},
	file = {Arbogast et al. - 2005 - The effect of spatial separation on informational .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\F2KFWRYR\\Arbogast et al. - 2005 - The effect of spatial separation on informational .pdf:application/pdf},
}

@article{kortlang_auditory_2016,
	title = {Auditory {Model}-{Based} {Dynamic} {Compression} {Controlled} by {Subband} {Instantaneous} {Frequency} and {Speech} {Presence} {Probability} {Estimates}},
	volume = {24},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/7499883/},
	doi = {10.1109/TASLP.2016.2584705},
	abstract = {Sensorineural hearing loss typically results in elevated thresholds and steepened loudness growth signiﬁcantly conditioned by a damage of outer hair cells (OHC). In hearing aids, ampliﬁcation and dynamic compression aim at widening the limited available dynamic range. However, speech perception particularly in complex acoustic scenes often remains difﬁcult. Here, a physiologically motivated, fast acting, model-based dynamic compression algorithm (MDC) is introduced which aims at restoring the behaviorally estimated basilar membrane input–output (BM I/O) function in normal-hearing listeners. A system-speciﬁc gain prescription rule is suggested, based on the same model BM I/O function and a behavioral estimate of the individual OHC loss. Cochlear off-frequency component suppression is mimicked using an instantaneous frequency (IF) estimate. Increased loudness as a consequence of widened ﬁlters in the impaired system is considered in a further compensation stage. In an extended version, a subband estimate of the speech presence probability (MDC+SPP) additionally provides speech-selective ampliﬁcation in stationary noise. Instrumental evaluation revealed that the IF control enhances the spectral contrast of vowels and beneﬁts in quality predictions at higher signal-to-noise ratios (SNRs) were observed. Compared with a conventional multiband dynamic compressor, MDC achieved objective quality and intelligibility beneﬁts for a competing talker at lower SNRs. MDC+SPP outperformed the conventional compressor in the quality predictions and reached comparable instrumental speech intelligibility as achieved with linear ampliﬁcation. The proposed algorithm provides a ﬁrst promising basis for auditory model-based compression with signal-typeand bandwidth-dependent gains.},
	language = {en},
	number = {10},
	urldate = {2018-04-16},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Kortlang, Steffen and Grimm, Giso and Hohmann, Volker and Kollmeier, Birger and Ewert, Stephan D.},
	month = oct,
	year = {2016},
	pages = {1759--1772},
	file = {Kortlang et al. - 2016 - Auditory Model-Based Dynamic Compression Controlle.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MK4RSRIT\\Kortlang et al. - 2016 - Auditory Model-Based Dynamic Compression Controlle.pdf:application/pdf},
}

@article{van_den_bogaert_horizontal_2006,
	title = {Horizontal localization with bilateral hearing aids: {Without} is better than with},
	volume = {119},
	issn = {0001-4966},
	shorttitle = {Horizontal localization with bilateral hearing aids},
	url = {http://asa.scitation.org/doi/10.1121/1.2139653},
	doi = {10.1121/1.2139653},
	language = {en},
	number = {1},
	urldate = {2018-04-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Van den Bogaert, Tim and Klasen, Thomas J. and Moonen, Marc and Van Deun, Lieselot and Wouters, Jan},
	month = jan,
	year = {2006},
	pages = {515--526},
	file = {Van den Bogaert et al. - 2006 - Horizontal localization with bilateral hearing aid.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JVUEELIN\\Van den Bogaert et al. - 2006 - Horizontal localization with bilateral hearing aid.pdf:application/pdf},
}

@article{wang_speaking_2018,
	title = {Speaking rhythmically improves speech recognition under “cocktail-party” conditions},
	volume = {143},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.5030518},
	doi = {10.1121/1.5030518},
	abstract = {This study examines whether speech rhythm affects speech recognition under “cocktail-party” conditions. Against a two-talker masker, but not a speech-spectrum noise masker, recognition of the last (third) keyword in a normal rhythmic sentence was signiﬁcantly better than that of the ﬁrst keyword. However, this word-position-related speech-recognition improvement disappeared for rhythmically hybrid target sentences that were constructed by grouping parts from different sentences with different artiﬁcially modulated rhythms (rates) (fast, normal, or slow). Thus, the normal rhythm with a constant rate plays a role in improving speech recognition against informational speech masking, probably through a build-up of temporal prediction for target words.},
	language = {en},
	number = {4},
	urldate = {2018-04-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Wang, Mengyuan and Kong, Lingzhi and Zhang, Changxin and Wu, Xihong and Li, Liang},
	month = apr,
	year = {2018},
	pages = {EL255--EL259},
	file = {Wang et al. - 2018 - Speaking rhythmically improves speech recognition .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MCP74UDJ\\Wang et al. - 2018 - Speaking rhythmically improves speech recognition .pdf:application/pdf},
}

@article{best_better-ear_2015,
	title = {Better-ear glimpsing in hearing-impaired listeners},
	volume = {137},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4907737},
	doi = {10.1121/1.4907737},
	abstract = {When competing speech sounds are spatially separated, listeners can make use of the ear with the better target-to-masker ratio. Recent studies showed that listeners with normal hearing are able to efﬁciently make use of this “better-ear,” even when it alternates between left and right ears at different times in different frequency bands, which may contribute to the ability to listen in spatialized speech mixtures. In the present study, better-ear glimpsing in listeners with bilateral sensorineural hearing impairment, who perform poorly in spatialized speech mixtures, was investigated. The results suggest that this deﬁcit is not related to better-ear glimpsing.},
	language = {en},
	number = {2},
	urldate = {2018-04-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Best, Virginia and Mason, Christine R. and Kidd, Gerald and Iyer, Nandini and Brungart, Douglas S.},
	month = feb,
	year = {2015},
	pages = {EL213--EL219},
	file = {Best et al. - 2015 - Better-ear glimpsing in hearing-impaired listeners.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\D2V2KS39\\Best et al. - 2015 - Better-ear glimpsing in hearing-impaired listeners.pdf:application/pdf},
}

@article{brungart_better-ear_2012,
	title = {Better-ear glimpsing efficiency with symmetrically-placed interfering talkers},
	volume = {132},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4747005},
	doi = {10.1121/1.4747005},
	language = {en},
	number = {4},
	urldate = {2018-04-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Brungart, Douglas S. and Iyer, Nandini},
	month = oct,
	year = {2012},
	pages = {2545--2556},
	file = {Brungart et Iyer - 2012 - Better-ear glimpsing efficiency with symmetrically.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\FHZEZST7\\Brungart et Iyer - 2012 - Better-ear glimpsing efficiency with symmetrically.pdf:application/pdf},
}

@article{glyde_importance_2013,
	title = {The importance of interaural time differences and level differences in spatial release from masking},
	volume = {134},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4812441},
	doi = {10.1121/1.4812441},
	abstract = {Numerous studies have described improvements in speech understanding when interaural time differences (ITDs) and interaural level differences (ILDs) are present. The present study aimed to investigate whether either cue in isolation can elicit spatial release from masking (SRM) in a speech-on-speech masking paradigm with maskers positioned symmetrically around the listener. Twelve adults were tested using three presentations of the Listening in Spatialized Noise–Sentences Test, with each presentation modiﬁed to contain different interaural cues in the stimuli. Results suggest that ILDs provide a similar amount of SRM as ITDs and ILDs combined. ITDs alone provide signiﬁcantly less beneﬁt.},
	language = {en},
	number = {2},
	urldate = {2018-04-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Glyde, Helen and Buchholz, Jörg M. and Dillon, Harvey and Cameron, Sharon and Hickson, Louise},
	month = aug,
	year = {2013},
	pages = {EL147--EL152},
	file = {Glyde et al. - 2013 - The importance of interaural time differences and .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XPX589LP\\Glyde et al. - 2013 - The importance of interaural time differences and .pdf:application/pdf},
}

@article{glyde_effect_2013,
	title = {The effect of better-ear glimpsing on spatial release from masking},
	volume = {134},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4817930},
	doi = {10.1121/1.4817930},
	language = {en},
	number = {4},
	urldate = {2018-04-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Glyde, Helen and Buchholz, Jörg and Dillon, Harvey and Best, Virginia and Hickson, Louise and Cameron, Sharon},
	month = oct,
	year = {2013},
	pages = {2937--2945},
	file = {Glyde et al. - 2013 - The effect of better-ear glimpsing on spatial rele.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ATZICPIE\\Glyde et al. - 2013 - The effect of better-ear glimpsing on spatial rele.pdf:application/pdf},
}

@article{beutelmann_prediction_2006,
	title = {Prediction of speech intelligibility in spatial noise and reverberation for normal-hearing and hearing-impaired listeners},
	volume = {120},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2202888},
	doi = {10.1121/1.2202888},
	language = {en},
	number = {1},
	urldate = {2018-04-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Beutelmann, Rainer and Brand, Thomas},
	month = jul,
	year = {2006},
	pages = {331--342},
	file = {Beutelmann et Brand - 2006 - Prediction of speech intelligibility in spatial no.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PE77VP4I\\Beutelmann et Brand - 2006 - Prediction of speech intelligibility in spatial no.pdf:application/pdf},
}

@article{rana_better-ear_2016,
	title = {Better-ear glimpsing at low frequencies in normal-hearing and hearing-impaired listeners},
	volume = {140},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4961006},
	doi = {10.1121/1.4961006},
	language = {en},
	number = {2},
	urldate = {2018-04-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Rana, Baljeet and Buchholz, Jörg M.},
	month = aug,
	year = {2016},
	pages = {1192--1205},
	file = {Rana et Buchholz - 2016 - Better-ear glimpsing at low frequencies in normal-.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BWT7J2JD\\Rana et Buchholz - 2016 - Better-ear glimpsing at low frequencies in normal-.pdf:application/pdf},
}

@article{cameron_development_2007,
	title = {Development of the {Listening} in {Spatialized} {Noise}-{Sentences} {Test} ({LISN}-{S}):},
	volume = {28},
	issn = {0196-0202},
	shorttitle = {Development of the {Listening} in {Spatialized} {Noise}-{Sentences} {Test} ({LISN}-{S})},
	url = {https://insights.ovid.com/crossref?an=00003446-200704000-00007},
	doi = {10.1097/AUD.0b013e318031267f},
	language = {en},
	number = {2},
	urldate = {2018-04-27},
	journal = {Ear and Hearing},
	author = {Cameron, Sharon and Dillon, Harvey},
	month = apr,
	year = {2007},
	pages = {196--211},
	file = {Cameron et Dillon - 2007 - Development of the Listening in Spatialized Noise-.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZU7Y8RI2\\Cameron et Dillon - 2007 - Development of the Listening in Spatialized Noise-.pdf:application/pdf},
}

@article{glyde_problems_2011,
	title = {Problems {Hearing} in {Noise} in {Older} {Adults}: {A} {Review} of {Spatial} {Processing} {Disorder}},
	volume = {15},
	issn = {1084-7138},
	shorttitle = {Problems {Hearing} in {Noise} in {Older} {Adults}},
	url = {http://journals.sagepub.com/doi/10.1177/1084713811424885},
	doi = {10.1177/1084713811424885},
	abstract = {Difficulty understanding speech in background noise, even with amplification to restore audibility, is a common problem for hearing-impaired individuals and is especially frequent in older adults. Despite the debilitating nature of the problem the cause is not yet completely clear. This review considers the role of spatial processing ability in understanding speech in noise, highlights the potential impact of disordered spatial processing, and attempts to establish if aging leads to reduced spatial processing ability. Evidence supporting and opposing the hypothesis that spatial processing is disordered among the aging population is presented.With a few notable exceptions, spatial processing ability was shown to be reduced in an older population in comparison to young adults, leading to poorer speech understanding in noise. However, it is argued that to conclude aging negatively effects spatial processing ability may be oversimplified or even premature given potentially confounding factors such as cognitive ability and hearing impairment. Further research is required to determine the effect of aging and hearing impairment on spatial processing and to investigate possible remediation options for spatial processing disorder.},
	language = {en},
	number = {3},
	urldate = {2018-04-27},
	journal = {Trends in Amplification},
	author = {Glyde, Helen and Hickson, Louise and Cameron, Sharon and Dillon, Harvey},
	month = sep,
	year = {2011},
	pages = {116--126},
	file = {Glyde et al. - 2011 - Problems Hearing in Noise in Older Adults A Revie.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QBZUX9NP\\Glyde et al. - 2011 - Problems Hearing in Noise in Older Adults A Revie.pdf:application/pdf},
}

@phdthesis{leclere_towards_2015,
	address = {Lyon},
	type = {Acoustique, {Psychoacoustique}},
	title = {Towards a binaural model for predicting speech intelligibility among competing voices in rooms},
	language = {fr},
	school = {Université de Lyon},
	author = {Leclère, Thibaud},
	year = {2015},
	keywords = {psychoacoustic, binaural, speech, rooms},
	file = {Leclère - Towards a binaural model for predicting speech int.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZPN4NTG3\\Leclère - Towards a binaural model for predicting speech int.pdf:application/pdf},
}

@article{jansen_comparison_2012,
	title = {Comparison of three types of {French} speech-in-noise tests: {A} multi-center study},
	volume = {51},
	issn = {1499-2027, 1708-8186},
	shorttitle = {Comparison of three types of {French} speech-in-noise tests},
	url = {http://www.tandfonline.com/doi/full/10.3109/14992027.2011.633568},
	doi = {10.3109/14992027.2011.633568},
	abstract = {Objective: To compare results on the everyday sentence test ‘FIST’, the new closed-set sentence test ‘FrMatrix’, and the digit triplet screening test ‘FrDigit3’. Design: First, the FrMatrix was developed and normative values were obtained. Subsequently, speech reception thresholds (SRTs) for the three types of tests were gathered at four study centers representing different geographic regions in Belgium and France. Study sample: Fifty-seven normal-hearing listeners took part in the normative study of the FrMatrix, and 118 subjects, with a wide range of hearing thresholds, participated in the comparative study. Results: Homogenizing the individual words of the FrMatrix with regard to their intelligibility resulted in a reference SRT of Ϫ 6.0 (Ϯ 0.6) dB SNR and slope at the SRT of 14.0 \%/dB. The within-subject variability was only 0.4 dB. Comparison of the three tests showed high correlations between the SRTs mutually (Ͼ 0.81). The FrMatrix had the highest discriminative power, both in stationary and in ﬂuctuating noise. For all three tests, differences across the participating study centers were small and not signiﬁcant. Conclusions: The FIST, the FrMatrix, and the FrDigit3 provide similar results and reliably evaluate speech recognition performance in noise both in normal-hearing and hearing-impaired listeners.},
	language = {en},
	number = {3},
	urldate = {2018-05-02},
	journal = {International Journal of Audiology},
	author = {Jansen, Sofie and Luts, Heleen and Wagener, Kirsten Carola and Kollmeier, Birger and Del Rio, Matthieu and Dauman, René and James, Chris and Fraysse, Bernard and Vormès, Emilie and Frachet, Bruno and Wouters, Jan and van Wieringen, Astrid},
	month = mar,
	year = {2012},
	pages = {164--173},
	file = {Jansen et al. - 2012 - Comparison of three types of French speech-in-nois.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\DUQTHTXF\\Jansen et al. - 2012 - Comparison of three types of French speech-in-nois.pdf:application/pdf},
}

@article{van_den_bogaert_sound_2011,
	title = {Sound source localization using hearing aids with microphones placed behind-the-ear, in-the-canal, and in-the-pinna},
	volume = {50},
	issn = {1499-2027, 1708-8186},
	url = {http://www.tandfonline.com/doi/full/10.3109/14992027.2010.537376},
	doi = {10.3109/14992027.2010.537376},
	abstract = {Objective: The effect of different commercial hearing aids on the ability to resolve front-back confusions and on sound localization in the frontal horizontal and vertical plane was studied. Design: Commercial hearing aids with a microphone placed in-the-ear-canal (ITC), behind-the-ear (BTE), and in-the-pinna (ITP) were evaluated in the frontal and full horizontal plane, and in the frontal vertical plane. Study Sample: A group of 13 hearing-impaired subjects evaluated the hearing aids. Nine normal-hearing listeners were used as a reference group. Results and Conclusions: Differences in sound localization in the front-back dimension were found for different hearing aids. A large inter-subject variability was found during the front-back and elevation experiments. With ITP or ITC microphones, almost all natural spectral information was preserved. One of the BTE hearing aids, which is equipped with a directional microphone conﬁguration, generated a sufﬁcient amount of spectral cues to allow front-back discrimination. No signiﬁcant effect of hearing aids on elevation performance in the frontal vertical plane was observed. Hearing-impaired subjects reached the same performance with and without the different hearing aids. In the unaided condition, a frequency-speciﬁc audibility correction was applied. Some of the hearing-impaired listeners reached normal hearing performance with this correction.},
	language = {en},
	number = {3},
	urldate = {2018-05-02},
	journal = {International Journal of Audiology},
	author = {Van den Bogaert, Tim and Carette, Evelyne and Wouters, Jan},
	month = mar,
	year = {2011},
	pages = {164--176},
	file = {Van den Bogaert et al. - 2011 - Sound source localization using hearing aids with .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TTYR9QMV\\Van den Bogaert et al. - 2011 - Sound source localization using hearing aids with .pdf:application/pdf},
}

@article{begault_techniques_1992,
	title = {Techniques and {Applications} for {Binaural} {Sound} {Manipulation}},
	volume = {2},
	issn = {1050-8414, 1532-7108},
	language = {en},
	number = {1},
	urldate = {2018-05-03},
	journal = {The International Journal of Aviation Psychology},
	author = {Begault, Durand R. and Wenzel, Elizabeth M.},
	month = jan,
	year = {1992},
	pages = {1--22},
	file = {Begault et Wenzel - 1992 - Techniques and Applications for Binaural Sound Man.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NE2DVMSL\\Begault et Wenzel - 1992 - Techniques and Applications for Binaural Sound Man.pdf:application/pdf},
}

@misc{noauthor_internationale-matrixtests.pdf_nodate,
	title = {Internationale-{Matrixtests}.pdf},
	file = {Internationale-Matrixtests.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\C5D9QIY6\\Internationale-Matrixtests.pdf:application/pdf},
}

@article{rhebergen_characterizing_2014,
	title = {Characterizing the {Speech} {Reception} {Threshold} in hearing-impaired listeners in relation to masker type and masker level},
	volume = {135},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4864301},
	doi = {10.1121/1.4864301},
	language = {en},
	number = {3},
	urldate = {2018-05-03},
	journal = {The Journal of the Acoustical Society of America},
	author = {Rhebergen, Koenraad S. and Pool, Ruben E. and Dreschler, Wouter A.},
	month = mar,
	year = {2014},
	pages = {1491--1505},
	file = {Rhebergen et al. - 2014 - Characterizing the Speech Reception Threshold in h.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4KQVA624\\Rhebergen et al. - 2014 - Characterizing the Speech Reception Threshold in h.pdf:application/pdf},
}

@article{francart_perception_nodate,
	title = {Perception of binaural localization cues with combined electric and acoustic hearing},
	language = {nl},
	author = {Francart, Tom},
	pages = {245},
	file = {Francart - Perception of binaural localization cues with comb.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4Z846E9B\\Francart - Perception of binaural localization cues with comb.pdf:application/pdf},
}

@article{kates_principles_2005,
	title = {Principles of {Digital} {Dynamic}-{Range} {Compression}},
	volume = {9},
	issn = {1084-7138},
	url = {http://journals.sagepub.com/doi/10.1177/108471380500900202},
	doi = {10.1177/108471380500900202},
	language = {en},
	number = {2},
	urldate = {2018-05-21},
	journal = {Trends in Amplification},
	author = {Kates, James M.},
	month = mar,
	year = {2005},
	pages = {45--76},
	file = {Kates - 2005 - Principles of Digital Dynamic-Range Compression.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KNY9DAMV\\Kates - 2005 - Principles of Digital Dynamic-Range Compression.pdf:application/pdf},
}

@article{kates_multichannel_2005,
	title = {Multichannel {Dynamic}-{Range} {Compression} {Using} {Digital} {Frequency} {Warping}},
	volume = {2005},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.3003},
	doi = {10.1155/ASP.2005.3003},
	abstract = {A multichannel dynamic-range compressor system using digital frequency warping is described. A frequency-warped ﬁlter is realized by replacing the ﬁlter unit delays with all-pass ﬁlters. The appropriate design of the frequency warping gives a nonuniform frequency representation very close to the auditory Bark scale. The warped compressor is shown to have substantially reduced group delay in comparison with a conventional design having comparable frequency resolution. The warped compressor, however, has more delay at low than at high frequencies, which can lead to perceptible changes in the signal. The detection threshold for the compressor group delay was determined as a function of the number of all-pass ﬁlter sections in cascade needed for a detectible change in signal quality. The test signals included clicks, vowels, and speech, and results are presented for both normal-hearing and hearing-impaired subjects. Thresholds for clicks are lower than thresholds for vowels, and hearing-impaired subjects have higher thresholds than normal-hearing listeners. A frequency-warped compressor using a cascade of 31 all-pass ﬁlter sections oﬀers a combination of low overall delay, good frequency resolution, and imperceptible frequency-dependent delay eﬀects for most listening conditions.},
	language = {en},
	number = {18},
	urldate = {2018-05-21},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Kates, James M. and Arehart, Kathryn Hoberg},
	month = dec,
	year = {2005},
	file = {Kates et Arehart - 2005 - Multichannel Dynamic-Range Compression Using Digit.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5SYEI6U5\\Kates et Arehart - 2005 - Multichannel Dynamic-Range Compression Using Digit.pdf:application/pdf},
}

@article{grimm_toolbox_2018,
	title = {A toolbox for rendering virtual acoustic environments in the context of audiology},
	url = {http://arxiv.org/abs/1804.11300},
	abstract = {A toolbox for creation and rendering of dynamic virtual acoustic environments (TASCAR) that allows direct user interaction was developed for application in hearing aid research and audiology. This technical paper describes the general software structure and the time-domain simulation methods, i.e., transmission model, image source model, and render formats, used to produce virtual acoustic environments with moving objects. Implementation-specific properties are described, and the computational performance of the system was measured as a function of simulation complexity. Results show that on commercially available commonly used hardware the simulation of several hundred virtual sound sources is possible in the time domain.},
	language = {en},
	urldate = {2018-05-21},
	journal = {arXiv:1804.11300 [cs, eess]},
	author = {Grimm, Giso and Luberadzka, Joanna and Hohmann, Volker},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.11300},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Grimm et al. - 2018 - A toolbox for rendering virtual acoustic environme.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\FN7ZYDBN\\Grimm et al. - 2018 - A toolbox for rendering virtual acoustic environme.pdf:application/pdf},
}

@inproceedings{otoole_virtual_2014,
	title = {Virtual 5.1 {Surround} {Sound} {Localization} using {Head}-{Tracking} {Devices}},
	isbn = {978-1-84919-924-7},
	url = {http://digital-library.theiet.org/content/conferences/10.1049/cp.2014.0656},
	doi = {10.1049/cp.2014.0656},
	abstract = {We investigated the impact of exploratory head movements on sound localization accuracy using real and virtual 5.1 loudspeaker arrays. Head orientation data in the horizontal plane was provided either by the Microsoft Kinect face-tracking or Oculus Rift’s built-in Inertial Measurement Unit (IMU) which resulted in signiﬁcantly diﬀerent precision and accuracy of measurements. In both cases, results suggest improvements in virtual source localization accuracy in the front and rear quadrants.},
	language = {en},
	urldate = {2018-06-06},
	publisher = {Institution of Engineering and Technology},
	author = {O'Toole, B.C. and O'Sullivan, L. and Kelly, I.J. and Boland, F. and Gorzel, M. and Kearney, G.},
	year = {2014},
	pages = {41--46},
	file = {O'Toole et al. - 2014 - Virtual 5.1 Surround Sound Localization using Head.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7HYGV2Z4\\O'Toole et al. - 2014 - Virtual 5.1 Surround Sound Localization using Head.pdf:application/pdf},
}

@phdthesis{bahu_localisation_2016,
	address = {Paris, France},
	title = {Localisation auditive en contexte de synthèse binaurale non-individuelle},
	school = {Université Pierre et Marie Curie - Paris VI},
	author = {Bahu, Hélène},
	month = dec,
	year = {2016},
	file = {bahu2016phd.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\DEEFXPFI\\bahu2016phd.pdf:application/pdf},
}

@article{kates_dynamic_2018,
	title = {The dynamic gammawarp auditory filterbank},
	volume = {143},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.5027827},
	doi = {10.1121/1.5027827},
	language = {en},
	number = {3},
	urldate = {2018-06-12},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kates, James M. and Prabhu, Shashidhar},
	month = mar,
	year = {2018},
	pages = {1603--1612},
	file = {Kates et Prabhu - 2018 - The dynamic gammawarp auditory filterbank.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GE89F2TB\\Kates et Prabhu - 2018 - The dynamic gammawarp auditory filterbank.pdf:application/pdf},
}

@article{smith_bark_1999,
	title = {Bark and {ERB} {Bilinear} {Transforms}},
	volume = {7},
	abstract = {Use of a bilinear conformal map to achieve a frequency warping nearly identical to that of the Bark frequency scale is described. Because the map takes the unit circle to itself, its form is that of the transfer function of a ﬁrst-order allpass ﬁlter. Since it is a ﬁrst-order map, it preserves the model order of rational systems, making it a valuable frequency warping technique for use in audio ﬁlter design. A closed-form weighted-equation-error method is derived that computes the optimal mapping coefﬁcient as a function of sampling rate, and the solution is shown to be generally indistinguishable from the optimal least-squares solution. The optimal Chebyshev mapping is also found to be essentially identical to the optimal least-squares 0 solution. The expression 0:8517 [arctan(0:06583fs)]1=2 0:916 is shown to accurately approximate the optimal allpass coefﬁcient as a function of sampling rate fs in kHz for sampling rates greater than 1 kHz. A ﬁlter design example is included that illustrates improvements due to carrying out the design over a Bark scale. Corresponding results are also given and compared for approximating the related “equivalent rectangular bandwidth (ERB) scale” of Moore and Glasberg using a ﬁrst-order allpass transformation. Due to the higher frequency resolution called for by the ERB scale, particularly at low frequencies, the ﬁrst-order conformal map is less able to follow the desired mapping, and the error is two to three times greater than the Bark-scale case, depending on the sampling rate.},
	language = {en},
	number = {6},
	journal = {IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING},
	author = {Smith, Julius O},
	year = {1999},
	pages = {12},
	file = {Smith - 1999 - Bark and ERB Bilinear Transforms.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\38T2CBMF\\Smith - 1999 - Bark and ERB Bilinear Transforms.pdf:application/pdf},
}

@article{oppenheim_discrete_1972,
	title = {Discrete {Representation} of {Signals}},
	volume = {60},
	number = {6},
	journal = {Proceedings of the IEEE},
	author = {Oppenheim, Alan V. and Johnson, Donald H.},
	month = jun,
	year = {1972},
	file = {oppenheim1972.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XMKZUGQS\\oppenheim1972.pdf:application/pdf},
}

@article{oppenheim_computation_1971,
	title = {Computation of {Spectra} with {Unequal} {Resolution} {Using} the {Fast} {Fourier} {Transform}},
	journal = {Proceedings of the IEEE},
	author = {Oppenheim, Alan V. and Johnson, Donald H.},
	year = {1971},
	file = {oppenheim1971.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3822N7TX\\oppenheim1971.pdf:application/pdf},
}

@book{hansler_speech_2008,
	address = {Berlin},
	series = {Springer series on signals and communication technology},
	title = {Speech and audio processing in adverse environments},
	isbn = {978-3-540-70601-4 978-3-540-70602-1},
	language = {en},
	publisher = {Springer},
	editor = {Hänsler, Eberhard and Schmidt, Gerhard},
	year = {2008},
	note = {OCLC: 244628140},
	file = {Hänsler et Schmidt - 2008 - Speech and audio processing in adverse environment.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2N343E8E\\Hänsler et Schmidt - 2008 - Speech and audio processing in adverse environment.pdf:application/pdf},
}

@article{oberem_intentional_2018,
	title = {Intentional switching in auditory selective attention: {Exploring} attention shifts with different reverberation times},
	volume = {359},
	issn = {03785955},
	shorttitle = {Intentional switching in auditory selective attention},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0378595517303222},
	doi = {10.1016/j.heares.2017.12.013},
	abstract = {Using a well-established binaural-listening paradigm the ability to intentionally switch auditory selective attention was examined under anechoic, low reverberation (0.8 s) and high reverberation (1.75 s) conditions. Twenty-three young, normal-hearing subjects were tested in a within-subject design to analyze inﬂuences of the reverberation times. Spoken word pairs by two speakers were presented simultaneously to subjects from two of eight azimuth positions. The stimuli consisted of a single number word, (i.e., 1 to 9), followed by either the direction “UP” or “DOWN” in German. Guided by a visual cue prior to auditory stimulus onset indicating the position of the target speaker, subjects were asked to identify whether the target number was numerically smaller or greater than ﬁve and to categorize the direction of the second word. Switch costs, (i.e. reaction time differences between a position switch of the target relative to a position repetition), were larger under the high reverberation condition. Furthermore, the error rates were highly dependent on reverberant energy and reverberation interacted with the congruence effect, (i.e. stimuli spoken by target and distractor may evoke the same answer (congruent) or different answers (incongruent)), indicating larger congruence effects under higher reverberation times.},
	language = {en},
	urldate = {2018-06-12},
	journal = {Hearing Research},
	author = {Oberem, Josefa and Seibold, Julia and Koch, Iring and Fels, Janina},
	month = mar,
	year = {2018},
	pages = {32--39},
	file = {Oberem et al. - 2018 - Intentional switching in auditory selective attent.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PWR7XMJX\\Oberem et al. - 2018 - Intentional switching in auditory selective attent.pdf:application/pdf},
}

@phdthesis{ayllon_speech_2013,
	address = {Alcalá de Henares, Spain},
	type = {{PhD} {Thesis}},
	title = {Speech {Enhancement} {Algorithms} for {Audiological} {Applications}},
	language = {en},
	school = {Universidad de Alcalá},
	author = {Ayllon, David},
	month = oct,
	year = {2013},
	file = {lvarez - Speech Enhancement Algorithms for Audiological App.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Y4R6RL7T\\lvarez - Speech Enhancement Algorithms for Audiological App.pdf:application/pdf},
}

@inproceedings{ayllon_machine_2016,
	title = {A machine learning approach for computationally and energy efficient speech enhancement in binaural hearing aids},
	isbn = {978-1-4799-9988-0},
	url = {http://ieeexplore.ieee.org/document/7472932/},
	doi = {10.1109/ICASSP.2016.7472932},
	abstract = {A binaural speech enhancement algorithm that combines superdirective beamforming with time-frequency (TF) masking is proposed. Supervised machine learning is used to design a speech/noise classiﬁer that estimates the ideal binary mask (IBM), which is further softened to reduce musical noise. The method is energy-efﬁcient in two ways: the computational complexity is limited and the wireless data transmission optimized. The experimental work demonstrates the ability of the method to increase the intelligibility of speech corrupted by different types of noise in low SNR scenarios.},
	language = {en},
	urldate = {2018-07-03},
	publisher = {IEEE},
	author = {Ayllon, David and Gil-Pita, Roberto and Rosa-Zurera, Manuel},
	month = mar,
	year = {2016},
	pages = {6515--6519},
	file = {Ayllon et al. - 2016 - A machine learning approach for computationally an.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3GA4RZLT\\Ayllon et al. - 2016 - A machine learning approach for computationally an.pdf:application/pdf},
}

@inproceedings{ayllon_improving_2017,
	title = {Improving {Speech} {Intelligibility} in {Binaural} {Hearing} {Aids} by {Estimating} a {Time}-{Frequency} {Mask} with a {Weighted} {Least} {Squares} {Classifier}},
	url = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/0771.html},
	doi = {10.21437/Interspeech.2017-771},
	abstract = {An efﬁcient algorithm for speech enhancement in binaural hearing aids is proposed. The algorithm is based on the estimation of a time-frequency mask using supervised machine learning. The standard least-squares linear classiﬁer is reformulated to optimize a metric related to speech/noise separation. The method is energy-efﬁcient in two ways: the computational complexity is limited and the wireless data transmission optimized. The ability of the algorithm to enhance speech contaminated with different types of noise and low SNR has been evaluated. Objective measures of speech intelligibility and speech quality demonstrate that the algorithm increments both the hearing comfort and speech understanding of the user. These results are supported by subjective listening tests.},
	language = {en},
	urldate = {2018-07-03},
	booktitle = {Interspeech},
	publisher = {ISCA},
	author = {Ayllón, David and Gil-Pita, Roberto and Rosa-Zurera, Manuel},
	month = aug,
	year = {2017},
	pages = {191--195},
	file = {Ayllón et al. - 2017 - Improving Speech Intelligibility in Binaural Heari.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2YSBRY8W\\Ayllón et al. - 2017 - Improving Speech Intelligibility in Binaural Heari.pdf:application/pdf},
}

@inproceedings{ayllon_optimum_2011,
	title = {Optimum microphone array for monaural and binaural in-the-canal hearing aids},
	isbn = {978-1-4577-2105-2 978-1-4577-2104-5 978-1-4577-2103-8},
	url = {http://ieeexplore.ieee.org/document/6135974/},
	doi = {10.1109/CAMSAP.2011.6135974},
	abstract = {Modern hearing aids include signal processing algorithms to improve the speech intelligibility by means of a microphone array, which can be monaural or binaural. In both cases, the signals are distorted by the well-known headshadow effect that must be considered in the design. The main goal of this paper is to ﬁnd the best microphone array conﬁguration for in-the-canal hearing aids, constrained by the reduced dimension of such devices, in order to maximize the output gain and intelligibility when the array is steered to the desired direction. For this purpose, a total of 12 different array arrangements are compared, in terms of the array gain and intelligibility obtained by a Minimum Variance Distortionless Response (MVDR) beamformer, which also considers the headshadow effect. The highest gain and intelligibility are obtained by a binaural array composed by a total of 8 microphones, 4 in diamond-shaped alignment in each ear. The results show the level of improvement achieved by increasing the number of microphones as well as using binaural arrays, giving rise to a compromise between the complexity of the array and the desired enhancement.},
	language = {en},
	urldate = {2018-07-03},
	publisher = {IEEE},
	author = {Ayllon, David and Gil-Pita, Roberto and Rosa-Zurera, Manuel},
	month = dec,
	year = {2011},
	pages = {177--180},
	file = {Ayllon et al. - 2011 - Optimum microphone array for monaural and binaural.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RM4QJGFU\\Ayllon et al. - 2011 - Optimum microphone array for monaural and binaural.pdf:application/pdf},
}

@phdthesis{marquardt_development_2015,
	address = {Oldenburg},
	title = {Development and evaluation of psychoacoustically motivated binaural noise reduction and cue preservation techniques},
	language = {en},
	school = {Von der Fakultät für Medizin und Gesundheitswissenschaften der Carl von Ossietzky Universität Oldenburg},
	author = {Marquardt, Daniel},
	month = nov,
	year = {2015},
	file = {Marquardt - Development and evaluation of psychoacoustically m.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AKUSRP69\\Marquardt - Development and evaluation of psychoacoustically m.pdf:application/pdf},
}

@phdthesis{kearney_auditory_2010,
	address = {Dublin},
	title = {Auditory {Scene} {Synthesis} using {Virtual} {Acoustic} {Recording} and {Reproduction}},
	school = {Trinity College Dublin},
	author = {Kearney, Gavin},
	month = mar,
	year = {2010},
	file = {GKearneyPhDThesis.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LJ5MD2W9\\GKearneyPhDThesis.pdf:application/pdf},
}

@article{mccreery_evidence-based_2012,
	title = {An {Evidence}-{Based} {Systematic} {Review} of {Amplitude} {Compression} in {Hearing} {Aids} for {School}-{Age} {Children} {With} {Hearing} {Loss}},
	volume = {21},
	issn = {1059-0889},
	url = {http://aja.pubs.asha.org/article.aspx?doi=10.1044/1059-0889(2012/12-0013)},
	doi = {10.1044/1059-0889(2012/12-0013)},
	abstract = {Purpose: Two clinical questions were developed: one addressing the comparison of linear amplification with compression limiting to linear amplification with peak clipping, and the second comparing wide dynamic range compression with linear amplification for outcomes of audibility, speech recognition, speech and language, and self- or parent report in children with hearing loss. Method: Twenty-six databases were systematically searched for studies addressing a clinical question and meeting all inclusion criteria. Studies were evaluated for methodological quality, and effect sizes were reported or calculated when possible. Results: The literature search resulted in the inclusion of 8 studies. All 8 studies included comparisons of wide dynamic range compression to linear amplification, and 2 of the 8 studies provided comparisons of compression limiting versus peak clipping. Conclusions: Moderate evidence from the included studies demonstrated that audibility was improved and speech recognition was either maintained or improved with wide dynamic range compression as compared with linear amplification. No significant differences were observed between compression limiting and peak clipping on outcomes (i.e., speech recognition and self-/parent report) reported across the 2 studies. Preference ratings appear to be influenced by participant characteristics and environmental factors. Further research is needed before conclusions can confidently be drawn.},
	language = {en},
	number = {2},
	urldate = {2018-07-10},
	journal = {American Journal of Audiology},
	author = {McCreery, Ryan W. and Venediktov, Rebecca A. and Coleman, Jaumeiko J. and Leech, Hillary M.},
	month = dec,
	year = {2012},
	pages = {269},
	file = {McCreery et al. - 2012 - An Evidence-Based Systematic Review of Amplitude C.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4M6SHA4W\\McCreery et al. - 2012 - An Evidence-Based Systematic Review of Amplitude C.pdf:application/pdf},
}

@article{mccreery_evidence-based_2012-1,
	title = {An {Evidence}-{Based} {Systematic} {Review} of {Frequency} {Lowering} in {Hearing} {Aids} for {School}-{Age} {Children} {With} {Hearing} {Loss}},
	volume = {21},
	issn = {1059-0889},
	url = {http://aja.pubs.asha.org/article.aspx?doi=10.1044/1059-0889(2012/12-0015)},
	doi = {10.1044/1059-0889(2012/12-0015)},
	abstract = {Purpose: We developed 1 clinical question for this review, which addressed the comparison of hearing aids using frequency lowering compared to conventional processing amplification for outcomes of audibility, speech recognition, speech and language, and self- or parent-report for children with hearing loss. Method: We systematically searched 26 databases for studies addressing a clinical question and meeting all inclusion criteria. We evaluated studies for methodological quality and reported or calculated effect sizes when possible. Results: The literature search resulted in the inclusion of 5 studies. We implemented several different frequencylowering strategies across studies; 2 studies used nonlinear frequency compression, 2 used frequency transposition, and 1 used frequency compression with dynamic consonant boost. Conclusions: Whereas methodological limitations of the included studies preclude the formulation of strong conclusions, findings were generally positive across frequency-lowering strategies and outcomes. Additional high-quality research is needed in this area.},
	language = {en},
	number = {2},
	urldate = {2018-07-10},
	journal = {American Journal of Audiology},
	author = {McCreery, Ryan W. and Venediktov, Rebecca A. and Coleman, Jaumeiko J. and Leech, Hillary M.},
	month = dec,
	year = {2012},
	pages = {313},
	file = {McCreery et al. - 2012 - An Evidence-Based Systematic Review of Frequency L.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AJ6QFTL6\\McCreery et al. - 2012 - An Evidence-Based Systematic Review of Frequency L.pdf:application/pdf},
}

@article{mccreery_evidence-based_2012-2,
	title = {An {Evidence}-{Based} {Systematic} {Review} of {Directional} {Microphones} and {Digital} {Noise} {Reduction} {Hearing} {Aids} in {School}-{Age} {Children} {With} {Hearing} {Loss}},
	volume = {21},
	issn = {1059-0889},
	url = {http://aja.pubs.asha.org/article.aspx?doi=10.1044/1059-0889(2012/12-0014)},
	doi = {10.1044/1059-0889(2012/12-0014)},
	abstract = {Purpose: The purpose of this evidence-based systematic review was to evaluate the efficacy of digital noise reduction and directional microphones for outcome measures of audibility, speech recognition, speech and language, and self- or parent-report in pediatric hearing aid users. Method: The authors searched 26 databases for experimental studies published after 1980 addressing one or more clinical questions and meeting all inclusion criteria. The authors evaluated studies for methodological quality and reported or calculated p values and effect sizes when possible. Results: A systematic search of the literature resulted in the inclusion of 4 digital noise reduction and 7 directional microphone studies (in 9 journal articles) that addressed speech recognition, speech and language, and /or selfor parent-report outcomes. No digital noise reduction or directional microphone studies addressed audibility outcomes. Conclusions: On the basis of a moderate level of evidence, digital noise reduction was not found to improve or degrade speech understanding. Additional research is needed before conclusions can be drawn regarding the impact of digital noise reduction on important speech, language, hearing, and satisfaction outcomes. Moderate evidence also indicates that directional microphones resulted in improved speech recognition in controlled optimal settings; however, additional research is needed to determine the effectiveness of directional microphones in actual everyday listening environments.},
	language = {en},
	number = {2},
	urldate = {2018-07-10},
	journal = {American Journal of Audiology},
	author = {McCreery, Ryan W. and Venediktov, Rebecca A. and Coleman, Jaumeiko J. and Leech, Hillary M.},
	month = dec,
	year = {2012},
	pages = {295},
	file = {McCreery et al. - 2012 - An Evidence-Based Systematic Review of Directional.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\C6A8SMLV\\McCreery et al. - 2012 - An Evidence-Based Systematic Review of Directional.pdf:application/pdf},
}

@article{mehraei_influence_2018,
	title = {Influence of talker discontinuity on cortical dynamics of auditory spatial attention},
	volume = {179},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811918305743},
	doi = {10.1016/j.neuroimage.2018.06.067},
	language = {en},
	urldate = {2018-07-10},
	journal = {NeuroImage},
	author = {Mehraei, Golbarg and Shinn-Cunningham, Barbara and Dau, Torsten},
	month = oct,
	year = {2018},
	pages = {548--556},
	file = {Mehraei et al. - 2018 - Influence of talker discontinuity on cortical dyna.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WR8SKMN7\\Mehraei et al. - 2018 - Influence of talker discontinuity on cortical dyna.pdf:application/pdf},
}

@inproceedings{singh_fast_2015,
	address = {New Delhi, India},
	title = {Fast {Inversion} of {Positive} {Definite} {Hermitian} {Matrices} {Using} {Real} {Inverse} {Operations}},
	booktitle = {2015 {Annual} {IEEE} {India} {Conference} ({INDICON})},
	author = {Singh, Abhishek Kumar},
	month = dec,
	year = {2015},
	file = {singh2015.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7JGT9S5F\\singh2015.pdf:application/pdf},
}

@techreport{peterson_adaptive_1989,
	address = {Research Laboratory of Electronics Massachusetts Institute of Technology Cambridge, MA 02139 USA},
	type = {technical},
	title = {Adaptive {Array} {Processing} for {Multiple} {Microphone} {Hearing} {Aids}},
	number = {541},
	institution = {Massachusetts Institute of Technology},
	author = {Peterson, Patrick M.},
	month = feb,
	year = {1989},
	file = {peterson89_mit_techreport.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AC65WZ49\\peterson89_mit_techreport.pdf:application/pdf},
}

@misc{doclo_binaural_2006,
	title = {Binaural noise reduction for hearing aids},
	language = {en},
	author = {Doclo, Simon and Moonen, Marc and Wouters, Jan},
	year = {2006},
	file = {Doclo et al. - Binaural noise reduction for hearing aids.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\D8CR9K7Z\\Doclo et al. - Binaural noise reduction for hearing aids.pdf:application/pdf},
}

@inproceedings{worley_auditory_2002,
	address = {Kyoto, Japan},
	title = {Auditory {Attention} {Based} on {Differences} in {Median} {Vertical} {Plane} {Position}},
	language = {en},
	booktitle = {Proceedings of the 2002 {International} {Conference} on {Auditory} {Display}},
	author = {Worley, J. W. and Darwin, C. J.},
	month = jul,
	year = {2002},
	pages = {5},
	file = {Bn - 2002 - J.W. Worley & C.J. Darwin.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RJUW7CRD\\Bn - 2002 - J.W. Worley & C.J. Darwin.pdf:application/pdf},
}

@article{aaronson_release_2009,
	title = {Release from speech-on-speech masking in a front-and-back geometry},
	volume = {125},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3050276},
	doi = {10.1121/1.3050276},
	language = {en},
	number = {3},
	urldate = {2018-08-21},
	journal = {The Journal of the Acoustical Society of America},
	author = {Aaronson, Neil L. and Rakerd, Brad and Hartmann, William M.},
	month = mar,
	year = {2009},
	pages = {1636--1648},
	file = {Aaronson et al. - 2009 - Release from speech-on-speech masking in a front-a.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6VR223X3\\Aaronson et al. - 2009 - Release from speech-on-speech masking in a front-a.pdf:application/pdf},
}

@article{martin_spatial_2012,
	title = {Spatial release from speech-on-speech masking in the median sagittal plane},
	volume = {131},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3669994},
	doi = {10.1121/1.3669994},
	language = {en},
	number = {1},
	urldate = {2018-08-21},
	journal = {The Journal of the Acoustical Society of America},
	author = {Martin, Russell L. and McAnally, Ken I. and Bolia, Robert S. and Eberle, Geoff and Brungart, Douglas S.},
	month = jan,
	year = {2012},
	pages = {378--385},
	file = {Martin et al. - 2012 - Spatial release from speech-on-speech masking in t.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IZ3J8TSN\\Martin et al. - 2012 - Spatial release from speech-on-speech masking in t.pdf:application/pdf},
}

@article{best_spatial_2011,
	title = {Spatial release from masking in normally hearing and hearing-impaired listeners as a function of the temporal overlap of competing talkers},
	volume = {129},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3533733},
	doi = {10.1121/1.3533733},
	language = {en},
	number = {3},
	urldate = {2018-08-21},
	journal = {The Journal of the Acoustical Society of America},
	author = {Best, Virginia and Mason, Christine R. and Kidd, Gerald},
	month = mar,
	year = {2011},
	pages = {1616--1625},
	file = {Best et al. - 2011 - Spatial release from masking in normally hearing a.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\57LKBRRQ\\Best et al. - 2011 - Spatial release from masking in normally hearing a.pdf:application/pdf},
}

@article{ellinger_role_2017,
	title = {The role of interaural differences on speech intelligibility in complex multi-talker environmentsa)},
	volume = {141},
	issn = {0001-4966},
	url = {http://aip.scitation.org/doi/10.1121/1.4976113},
	doi = {10.1121/1.4976113},
	abstract = {Interaural differences in time (ITDs) and interaural differences in level (ILDs) contribute to a listener’s ability to achieve spatial release from masking (SRM), and help to improve speech intelligibility in noisy environments. In this study, the extent to which ITDs and ILDs contribute to SRM and the relationships with aging and hearing loss were examined. SRM was greatest when stimuli were presented with consistent ITD and ILD, relative to ITD or ILD alone, all of which produced greater SRM than when ITD and ILD cues were in conﬂict with each other. This pattern was independent of age and hearing loss.},
	language = {en},
	number = {2},
	urldate = {2018-08-22},
	journal = {The Journal of the Acoustical Society of America},
	author = {Ellinger, Rachel L. and Jakien, Kasey M. and Gallun, Frederick J.},
	month = feb,
	year = {2017},
	pages = {EL170--EL176},
	file = {Ellinger et al. - 2017 - The role of interaural differences on speech intel.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\SM7VSJCH\\Ellinger et al. - 2017 - The role of interaural differences on speech intel.pdf:application/pdf},
}

@article{may_signal--noise-ratio-aware_2018,
	title = {Signal-to-{Noise}-{Ratio}-{Aware} {Dynamic} {Range} {Compression} in {Hearing} {Aids}},
	volume = {22},
	issn = {2331-2165, 2331-2165},
	url = {http://journals.sagepub.com/doi/10.1177/2331216518790903},
	doi = {10.1177/2331216518790903},
	abstract = {Fast-acting dynamic range compression is a level-dependent amplification scheme which aims to restore audibility for hearingimpaired listeners. However, when being applied to noisy speech at positive signal-to-noise ratios (SNRs), the gain function typically changes rapidly over time as it is driven by the short-term fluctuations of the speech signal. This leads to an amplification of the noise components in the speech gaps, which reduces the output SNR and distorts the acoustic properties of the background noise. An adaptive compression scheme is proposed here which utilizes information about the SNR in different frequency channels to adaptively change the characteristics of the compressor. Specifically, fast-acting compression is applied to speech-dominated time-frequency (T-F) units where the SNR is high, while slow-acting compression is used to effectively linearize the processing for noise-dominated T-F units where the SNR is low. A systematic evaluation of this SNRaware compression scheme showed that the effective compression of speech components embedded in noise was similar to that of a conventional fast-acting system, whereas natural fluctuations in the background noise were preserved in a similar way as when a slow-acting compressor was applied.},
	language = {en},
	urldate = {2018-08-22},
	journal = {Trends in Hearing},
	author = {May, Tobias and Kowalewski, Borys and Dau, Torsten},
	month = jan,
	year = {2018},
	pages = {1--12},
	file = {May et al. - 2018 - Signal-to-Noise-Ratio-Aware Dynamic Range Compress.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GJKSXH2Y\\May et al. - 2018 - Signal-to-Noise-Ratio-Aware Dynamic Range Compress.pdf:application/pdf},
}

@book{middlebrooks_auditory_2017,
	address = {Cham},
	series = {Springer {Handbook} of {Auditory} {Research}},
	title = {The {Auditory} system at the cocktail party},
	isbn = {978-3-319-51660-8 978-3-319-51662-2},
	language = {en},
	number = {Volume 60},
	publisher = {Springer International Publishing},
	editor = {Middlebrooks, John C. and Simon, Jonathan Z. and Popper, Arthur N. and Fay, Richard R.},
	year = {2017},
	note = {OCLC: 990257975},
	file = {Middlebrooks et al. - 2017 - The Auditory system at the cocktail party.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QCAB2Y62\\Middlebrooks et al. - 2017 - The Auditory system at the cocktail party.pdf:application/pdf},
}

@article{wan_application_2010,
	title = {Application of an extended equalization-cancellation model to speech intelligibility with spatially distributed maskers},
	volume = {128},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3502458},
	doi = {10.1121/1.3502458},
	language = {en},
	number = {6},
	urldate = {2018-08-23},
	journal = {The Journal of the Acoustical Society of America},
	author = {Wan, Rui and Durlach, Nathaniel I. and Colburn, H. Steven},
	month = dec,
	year = {2010},
	pages = {3678--3690},
	file = {Wan et al. - 2010 - Application of an extended equalization-cancellati.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JPTJ5C6G\\Wan et al. - 2010 - Application of an extended equalization-cancellati.pdf:application/pdf},
}

@article{wan_application_2014,
	title = {Application of a short-time version of the {Equalization}-{Cancellation} model to speech intelligibility experiments with speech maskers},
	volume = {136},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4884767},
	doi = {10.1121/1.4884767},
	language = {en},
	number = {2},
	urldate = {2018-08-23},
	journal = {The Journal of the Acoustical Society of America},
	author = {Wan, Rui and Durlach, Nathaniel I. and Colburn, H. Steven},
	month = aug,
	year = {2014},
	pages = {768--776},
	file = {Wan et al. - 2014 - Application of a short-time version of the Equaliz.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\I58AYIDW\\Wan et al. - 2014 - Application of a short-time version of the Equaliz.pdf:application/pdf},
}

@article{edmonds_spatial_2006,
	title = {The spatial unmasking of speech: {Evidence} for better-ear listening},
	volume = {120},
	issn = {0001-4966},
	shorttitle = {The spatial unmasking of speech},
	url = {http://asa.scitation.org/doi/10.1121/1.2228573},
	doi = {10.1121/1.2228573},
	language = {en},
	number = {3},
	urldate = {2018-08-23},
	journal = {The Journal of the Acoustical Society of America},
	author = {Edmonds, Barrie A. and Culling, John F.},
	month = sep,
	year = {2006},
	pages = {1539--1545},
	file = {Edmonds et Culling - 2006 - The spatial unmasking of speech Evidence for bett.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VWVYAA7N\\Edmonds et Culling - 2006 - The spatial unmasking of speech Evidence for bett.pdf:application/pdf},
}

@article{cosentino_model_2014,
	title = {A model that predicts the binaural advantage to speech intelligibility from the mixed target and interferer signals},
	volume = {135},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4861239},
	doi = {10.1121/1.4861239},
	language = {en},
	number = {2},
	urldate = {2018-08-23},
	journal = {The Journal of the Acoustical Society of America},
	author = {Cosentino, Stefano and Marquardt, Torsten and McAlpine, David and Culling, John F. and Falk, Tiago H.},
	month = feb,
	year = {2014},
	pages = {796--807},
	file = {Cosentino et al. - 2014 - A model that predicts the binaural advantage to sp.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZQEZQEHC\\Cosentino et al. - 2014 - A model that predicts the binaural advantage to sp.pdf:application/pdf},
}

@inproceedings{favre-felix_real-time_2017,
	address = {Jeju Island, South Korea},
	title = {Real-time estimation of eye gaze by in-ear electrodes},
	isbn = {978-1-5090-2809-2},
	url = {http://ieeexplore.ieee.org/document/8037754/},
	doi = {10.1109/EMBC.2017.8037754},
	abstract = {Cognitive control of a hearing aid is the topic for several ongoing studies. The relevance of these studies should be seen in the light of inadequate steering of current hearing aids. While most studies are concerned with auditory attention tracking from the electroencephalogram (EEG), a complimentary approach may be to use visual attention tracking to steer the devices. Visual attention may be characterized by gaze direction, which can be obtained by electrooculography (EOG). EOG may be recorded from electrodes placed in the ear canal, termed EarEOG. To test the comparison of conventional EOG and EarEOG recordings, we conducted two experiments with six subjects. In the first experiment, the subjects were instructed to follow a moving dot on the screen moving in large saccades. In the second experiment, there were five large targets, and within each target, the dot had minor movements. When comparing conventional EOG and EarEOG, correlations of 0.9 and 0.91 with standard deviations of 0.02 were obtained for the two experiments respectively. To assess the feasibility of using EarEOG in real-time, correlation between EarEOG and the timecourse of the dot position was performed. When both signals were filtered with the same real-time applicable filter, correlations of 0.83 and 0.85 with standard deviations of 0.09 and 0.05 were found respectively to the two experiments. In conclusion, this study provides motivational aspects of using EarEOG to estimate eye gaze, as well as it identifies important future challenges in real-time applications to steer external devices such as a hearing aid.},
	language = {en},
	urldate = {2018-08-24},
	booktitle = {2017 39th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	publisher = {IEEE},
	author = {Favre-Felix, A. and Graversen, C. and Dau, T. and Lunner, T.},
	month = jul,
	year = {2017},
	pages = {4086--4089},
	file = {Favre-Felix et al. - 2017 - Real-time estimation of eye gaze by in-ear electro.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AS3CW7DR\\Favre-Felix et al. - 2017 - Real-time estimation of eye gaze by in-ear electro.pdf:application/pdf},
}

@inproceedings{casebeer_multi-view_2018,
	address = {Tokyo, Japan},
	title = {Multi-{View} {Networks} for {Denoising} of {Arbitrary} {Numbers} of {Channels}},
	abstract = {We propose a set of denoising neural networks capable of operating on an arbitrary number of channels at runtime, irrespective of how many channels they were trained on. We coin the proposed models multi-view networks since they operate using multiple views of the same data. We explore two such architectures and show how they outperform traditional denoising models in multi-channel scenarios. Additionally, we demonstrate how multi-view networks can leverage information provided by additional recordings to make better predictions, and how they are able to generalize to a number of recordings not seen in training.},
	language = {en},
	booktitle = {{IEEE} {International} {Workshop} on {Acoustic} {Signal} {Enhancement}},
	author = {Casebeer, Jonah and Luc, Brian and Smaragdis, Paris},
	year = {2018},
	pages = {5},
	file = {Casebeer et al. - MULTI-VIEW NETWORKS FOR DENOISING OF ARBITRARY NUM.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ENBFIPXX\\Casebeer et al. - MULTI-VIEW NETWORKS FOR DENOISING OF ARBITRARY NUM.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997},
	pages = {173--1780},
	file = {hochreiter1997lstm.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZX72WRDC\\hochreiter1997lstm.pdf:application/pdf},
}

@inproceedings{sarver_application_2011,
	address = {Paris, France},
	title = {Application of {Non}-{Negative} {Matrix} {Factorization} to {Signal}-{Adaptive} {Audio} {Effects}},
	abstract = {This paper proposes novel audio effects based on manipulating an audio signal in a representation domain provided by non-negative matrix factorization (NMF). Critical-band magnitude spectrograms Y of sounds are ﬁrst factorized into a product of two lower-rank matrices so that Y ≈ BG. The parameter matrices B and G are then processed in order to achieve the desired effect. Three classes of effects were investigated: 1) dynamic range compression (or expansion) of the component spectra or gains, 2) effects based on rank-ordering the components (colums of B and the corresponding rows of G) according to acoustic features extracted from them, and then weighting each component according to its rank, and 3) distortion effects based on controlling the amount of components (and thus the reconstruction error) in the above linear approximation. The subjective quality of the effects was assessed in a listening test.},
	language = {en},
	booktitle = {Proc. of the 14th {International} {Conference} on {Digital} {Audio} {Effects}},
	author = {Sarver, Ryan and Klapuri, Anssi},
	month = sep,
	year = {2011},
	pages = {4},
	file = {Sarver et Klapuri - 2011 - Centre for Digital Music Queen Mary University of .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Q542PKUM\\Sarver et Klapuri - 2011 - Centre for Digital Music Queen Mary University of .pdf:application/pdf},
}

@inproceedings{heymann_neural_2016,
	address = {Shanghai},
	title = {Neural network based spectral mask estimation for acoustic beamforming},
	isbn = {978-1-4799-9988-0},
	url = {http://ieeexplore.ieee.org/document/7471664/},
	doi = {10.1109/ICASSP.2016.7471664},
	abstract = {We present a neural network based approach to acoustic beamforming. The network is used to estimate spectral masks from which the Cross-Power Spectral Density matrices of speech and noise are estimated, which in turn are used to compute the beamformer coefﬁcients. The network training is independent of the number and the geometric conﬁguration of the microphones. We further show that it is possible to train the network on clean speech only, avoiding the need for stereo data with separated speech and noise. Two types of networks are evaluated. One small feed-forward network with only one hidden layer and one more elaborated bi-directional Long Short-Term Memory network. We compare our system with different parametric approaches to mask estimation and using different beamforming algorithms. We show that our system yields superior results, both in terms of perceptual speech quality and with respect to speech recognition error rate. The results for the simple feed-forward network are especially encouraging considering its low computational requirements.},
	language = {en},
	urldate = {2018-09-12},
	booktitle = {2016 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Heymann, Jahn and Drude, Lukas and Haeb-Umbach, Reinhold},
	month = mar,
	year = {2016},
	pages = {196--200},
	file = {Heymann et al. - 2016 - Neural network based spectral mask estimation for .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2DJJWTDB\\Heymann et al. - 2016 - Neural network based spectral mask estimation for .pdf:application/pdf},
}

@inproceedings{perotin_multichannel_2017,
	title = {Multichannel speech separation with recurrent neural networks from high-order ambisonics recordings},
	abstract = {We present a source separation system for high-order ambisonics (HOA) contents. We derive a multichannel spatial ﬁlter from a mask estimated by a long short-term memory (LSTM) recurrent neural network. We combine one channel of the mixture with the outputs of basic HOA beamformers as inputs to the LSTM, assuming that we know the directions of arrival of the directional sources. In our experiments, the speech of interest can be corrupted either by diffuse noise or by an equally loud competing speaker. We show that adding as input the output of the beamformer steered toward the competing speech in addition to that of the beamformer steered toward the target speech brings signiﬁcant improvements in terms of word error rate.},
	language = {en},
	booktitle = {hal- 01699759v2},
	author = {Perotin, Lauréline and Serizel, Romain and Vincent, Emmanuel and Guérin, Alexandre},
	year = {2017},
	pages = {6},
	file = {Perotin et al. - Multichannel speech separation with recurrent neur.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QTZ3X4U2\\Perotin et al. - Multichannel speech separation with recurrent neur.pdf:application/pdf},
}

@inproceedings{tran_vu_blind_2010,
	address = {Dallas, TX, USA},
	title = {Blind speech separation employing directional statistics in an {Expectation} {Maximization} framework},
	isbn = {978-1-4244-4295-9},
	url = {http://ieeexplore.ieee.org/document/5495994/},
	doi = {10.1109/ICASSP.2010.5495994},
	abstract = {In this paper we propose to employ directional statistics in a complex vector space to approach the problem of blind speech separation in the presence of spatially correlated noise. We interpret the values of the short time Fourier transform of the microphone signals to be draws from a mixture of complex Watson distributions, a probabilistic model which naturally accounts for spatial aliasing. The parameters of the density are related to the a priori source probabilities, the power of the sources and the transfer function ratios from sources to sensors. Estimation formulas are derived for these parameters by employing the Expectation Maximization (EM) algorithm. The E-step corresponds to the estimation of the source presence probabilities for each time-frequency bin, while the M-step leads to a maximum signal-to-noise ratio (MaxSNR) beamformer in the presence of uncertainty about the source activity. Experimental results are reported for an implementation in a generalized sidelobe canceller (GSC) like spatial beamforming conﬁguration for 3 speech sources with signiﬁcant coherent noise in reverberant environments, demonstrating the usefulness of the novel modeling framework.},
	language = {en},
	urldate = {2018-09-17},
	booktitle = {2010 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Tran Vu, Dang Hai and Haeb-Umbach, Reinhold},
	year = {2010},
	pages = {241--244},
	file = {Tran Vu et Haeb-Umbach - 2010 - Blind speech separation employing directional stat.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TDVJ728L\\Tran Vu et Haeb-Umbach - 2010 - Blind speech separation employing directional stat.pdf:application/pdf},
}

@article{glorot_understanding_nodate,
	title = {Understanding the difﬁculty of training deep feedforward neural networks},
	abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
	language = {en},
	author = {Glorot, Xavier and Bengio, Yoshua},
	pages = {8},
	file = {Glorot et Bengio - Understanding the difﬁculty of training deep feedf.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9RVWQZRJ\\Glorot et Bengio - Understanding the difﬁculty of training deep feedf.pdf:application/pdf},
}

@article{chen_large-scale_2016,
	title = {Large-scale training to increase speech intelligibility for hearing-impaired listeners in novel noises},
	volume = {139},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4948445},
	doi = {10.1121/1.4948445},
	language = {en},
	number = {5},
	urldate = {2018-09-17},
	journal = {The Journal of the Acoustical Society of America},
	author = {Chen, Jitong and Wang, Yuxuan and Yoho, Sarah E. and Wang, DeLiang and Healy, Eric W.},
	month = may,
	year = {2016},
	pages = {2604--2612},
	file = {Chen et al. - 2016 - Large-scale training to increase speech intelligib.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UEBZKBKY\\Chen et al. - 2016 - Large-scale training to increase speech intelligib.pdf:application/pdf},
}

@article{healy_algorithm_2013,
	title = {An algorithm to improve speech recognition in noise for hearing-impaired listeners},
	volume = {134},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4820893},
	doi = {10.1121/1.4820893},
	language = {en},
	number = {4},
	urldate = {2018-09-17},
	journal = {The Journal of the Acoustical Society of America},
	author = {Healy, Eric W. and Yoho, Sarah E. and Wang, Yuxuan and Wang, DeLiang},
	month = oct,
	year = {2013},
	pages = {3029--3038},
	file = {Healy et al. - 2013 - An algorithm to improve speech recognition in nois.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BDTTB5EG\\Healy et al. - 2013 - An algorithm to improve speech recognition in nois.pdf:application/pdf},
}

@article{nugraha_multichannel_2016,
	title = {Multichannel {Audio} {Source} {Separation} {With} {Deep} {Neural} {Networks}},
	volume = {24},
	url = {http://ieeexplore.ieee.org/document/7492604/},
	doi = {10.1109/TASLP.2016.2580946},
	abstract = {This article addresses the problem of multichannel audio source separation. We propose a framework where deep neural networks (DNNs) are used to model the source spectra and combined with the classical multichannel Gaussian model to exploit the spatial information. The parameters are estimated in an iterative expectation-maximization (EM) fashion and used to derive a multichannel Wiener ﬁlter. We present an extensive experimental study to show the impact of different design choices on the performance of the proposed technique. We consider different cost functions for the training of DNNs, namely the probabilistically motivated Itakura-Saito divergence, and also Kullback-Leibler, Cauchy, mean squared error, and phase-sensitive cost functions. We also study the number of EM iterations and the use of multiple DNNs, where each DNN aims to improve the spectra estimated by the preceding EM iteration. Finally, we present its application to a speech enhancement problem. The experimental results show the beneﬁt of the proposed multichannel approach over a single-channel DNNbased approach and the conventional multichannel nonnegative matrix factorization based iterative EM algorithm.},
	language = {en},
	urldate = {2018-09-19},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Nugraha, Aditya Arie and Liutkus, Antoine and Vincent, Emmanuel},
	month = sep,
	year = {2016},
	pages = {1652--1664},
	file = {Nugraha et al. - 2016 - Multichannel Audio Source Separation With Deep Neu.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PTLRNKDY\\Nugraha et al. - 2016 - Multichannel Audio Source Separation With Deep Neu.pdf:application/pdf},
}

@inproceedings{mars_frequency-invariant_2014,
	address = {Chiang Mai, Thailand},
	title = {A frequency-invariant fixed beamformer for speech enhancement},
	isbn = {978-616-361-823-8},
	url = {http://ieeexplore.ieee.org/document/7041712/},
	doi = {10.1109/APSIPA.2014.7041712},
	abstract = {Fixed beamformers maintain response that is independent of the signal and interference statistics. Frequencyinvariant  xed beamformers can achieve constant beamwidth across all frequencies which results in lower signal distortion and they have lower computation complexity compared to its adaptive counterpart. However, unlike data-dependent beamformers, their sidelobe attenuation is poor with respect to the direction of the interferences. In this paper we propose a method to improve the sidelobe attenuation while retaining the advantages of lower computational complexity and frequency-invariance. This is achieved by introducing frequency-invariant nulls in the interference directions. We also show how the weights for each null direction can be combined with the  xed beamformer to form the effective weights of the proposed beamformer.},
	language = {en},
	urldate = {2018-10-30},
	booktitle = {Signal and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA}), 2014 {Asia}-{Pacific}},
	publisher = {IEEE},
	author = {Mars, Rohith and Reju, V. G. and Khong, Andy W. H.},
	month = dec,
	year = {2014},
	pages = {1--6},
	file = {Mars et al. - 2014 - A frequency-invariant fixed beamformer for speech .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EHESC6LL\\Mars et al. - 2014 - A frequency-invariant fixed beamformer for speech .pdf:application/pdf},
}

@article{zhao_subband_2011,
	title = {Subband design of fixed wideband beamformers based on the least squares approach},
	volume = {91},
	issn = {01651684},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0165168410003695},
	doi = {10.1016/j.sigpro.2010.09.016},
	abstract = {Subband method is an effective way to reduce the computational complexity of a wideband system and in this paper we study the subband design problem for ﬁxed wideband beamformers, with an emphasis on the design of frequency invariant beamformers (FIBs). We ﬁrst express the equivalent fullband beam response as a function of the subband beamformer coefﬁcients and then formulate the design problem based on the least squares approach. One direct least squares formulation is ﬁrst proposed for the design of a general wideband beamformer, and then extended to the FIB design case, followed by three further variations.},
	language = {en},
	number = {4},
	urldate = {2018-10-30},
	journal = {Signal Processing},
	author = {Zhao, Yong and Liu, Wei and Langley, Richard J.},
	month = apr,
	year = {2011},
	pages = {1060--1065},
	file = {Zhao et al. - 2011 - Subband design of fixed wideband beamformers based.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ITFQWNYT\\Zhao et al. - 2011 - Subband design of fixed wideband beamformers based.pdf:application/pdf},
}

@article{nelder_simplex_1965,
	title = {A {Simplex} {Method} for {Function} {Minimization}},
	volume = {7},
	issn = {0010-4620, 1460-2067},
	url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/7.4.308},
	doi = {10.1093/comjnl/7.4.308},
	language = {en},
	number = {4},
	urldate = {2018-11-09},
	journal = {The Computer Journal},
	author = {Nelder, J. A. and Mead, R.},
	month = jan,
	year = {1965},
	pages = {308--313},
	file = {Nelder et Mead - 1965 - A Simplex Method for Function Minimization.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AIE4QEVD\\Nelder et Mead - 1965 - A Simplex Method for Function Minimization.pdf:application/pdf},
}

@book{noauthor_compression_2017,
	title = {The {Compression} {Handbook} - {Fourth} {Edition}},
	language = {en},
	year = {2017},
	file = {The Compression Handbook - Fourth Edition.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3XYUQBLM\\The Compression Handbook - Fourth Edition.pdf:application/pdf},
}

@article{chowdhury_integrating_nodate,
	title = {Integrating {Signal} {Processing} {Modules} of {Hearing} {Aids} into a {Real}-{Time} {Smartphone} {App}},
	abstract = {This paper presents the integration of three major modules of the signal processing pipeline that go into a typical digital hearing aid as a real-time smartphone app. These modules include voice activity detection, noise reduction, and compression. The steps taken to allow the real-time implementation of this integration or signal processing pipeline are discussed. These steps can be utilized to create similar signal processing pipelines or integrated apps to evaluate hearing improvement algorithms. The real-time characteristics of the developed integrated app are reported as well as an objective evaluation of its noise reduction.},
	language = {en},
	author = {Chowdhury, Tahsin A and Sehgal, Abhishek and Kehtarnavaz, Nasser},
	pages = {4},
	file = {Chowdhury et al. - Integrating Signal Processing Modules of Hearing A.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6ZZNTCIJ\\Chowdhury et al. - Integrating Signal Processing Modules of Hearing A.pdf:application/pdf},
}

@book{kelley_iterative_1999,
	title = {Iterative {Methods} for {Optimization}},
	isbn = {978-0-89871-433-3 978-1-61197-092-0},
	url = {http://epubs.siam.org/doi/book/10.1137/1.9781611970920},
	language = {en},
	urldate = {2018-11-09},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Kelley, C. T.},
	month = jan,
	year = {1999},
	doi = {10.1137/1.9781611970920},
	file = {Kelley - 1999 - Iterative Methods for Optimization.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\R73C558K\\Kelley - 1999 - Iterative Methods for Optimization.pdf:application/pdf},
}

@book{coello_evolutionary_2007,
	title = {Evolutionary {Algorithms} for {Solving} {Multi}-{Objective} {Problems} {Second} {Edition}},
	language = {en},
	publisher = {Springer},
	author = {Coello, Carlos A Coello and Lamont, Gary B and Van Veldhuizen, David A},
	year = {2007},
	file = {Coello et al. - Evolutionary Algorithms for Solving Multi-Objectiv.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7JYNABEL\\Coello et al. - Evolutionary Algorithms for Solving Multi-Objectiv.pdf:application/pdf},
}

@book{zavalishin_art_2015,
	title = {The {Art} of {VA} {Filter} {Design}},
	author = {Zavalishin, Vadim},
	month = jul,
	year = {2015},
	file = {VAFilterDesign_1.1.1.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CISEX4DT\\VAFilterDesign_1.1.1.pdf:application/pdf},
}

@book{madisetti_digital_1998,
	address = {Boca Raton, Fla},
	series = {The electrical engineering handbook series},
	title = {The digital signal processing handbook},
	isbn = {978-0-8493-8572-8},
	language = {en},
	publisher = {CRC Press [u.a.]},
	editor = {Madisetti, Vijay K. and Williams, Douglas B.},
	year = {1998},
	note = {OCLC: 36977280},
	file = {Madisetti et Williams - 1998 - The digital signal processing handbook.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PNKWSJVK\\Madisetti et Williams - 1998 - The digital signal processing handbook.pdf:application/pdf},
}

@book{liu_wideband_2010,
	address = {Chichester, UK},
	title = {Wideband {Beamforming}},
	isbn = {978-0-470-66117-8 978-0-470-71392-1},
	url = {http://doi.wiley.com/10.1002/9780470661178},
	language = {en},
	urldate = {2018-11-09},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Liu, Wei and Weiss, Stephan},
	month = mar,
	year = {2010},
	doi = {10.1002/9780470661178},
	file = {Liu et Weiss - 2010 - Wideband Beamforming.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PNMUILLD\\Liu et Weiss - 2010 - Wideband Beamforming.pdf:application/pdf},
}

@unpublished{jutten_detection_2007,
	type = {Cours},
	title = {Détection, {Estimation}, {Information}},
	abstract = {Cours de troisième année du département 3i
Options Images et Signaux et Automatique},
	author = {Jutten, Chritian},
	month = sep,
	year = {2007},
	file = {Detection_Estimation_Information.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\99ZDBVJX\\Detection_Estimation_Information.pdf:application/pdf},
}

@article{hazan_clear_2018,
	title = {Clear speech adaptations in spontaneous speech produced by young and older adults},
	volume = {144},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.5053218},
	doi = {10.1121/1.5053218},
	language = {en},
	number = {3},
	urldate = {2018-11-14},
	journal = {The Journal of the Acoustical Society of America},
	author = {Hazan, Valerie and Tuomainen, Outi and Kim, Jeesun and Davis, Christopher and Sheffield, Benjamin and Brungart, Douglas},
	month = sep,
	year = {2018},
	pages = {1331--1346},
	file = {Hazan et al. - 2018 - Clear speech adaptations in spontaneous speech pro.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3YVGIKSD\\Hazan et al. - 2018 - Clear speech adaptations in spontaneous speech pro.pdf:application/pdf},
}

@inproceedings{doclo_theorical_2006,
	title = {Theorical analysis of binaural cue preservation using multi-channel {Wiener} filtering and interaural transfer functions},
	abstract = {In this paper a theoretical analysis of the binaural cue preservation of the multi-channel Wiener ﬁlter (MWF) is performed. We will prove that in the case of a single speech source the MWF perfectly preserves the binaural cues of the speech component, but changes the binaural cues of the noise component to the cues of the speech component. In addition, we show that by extending the MWF cost function with terms related to the interaural transfer function it is possible to preserve the binaural cues of both the speech and the noise component, without considerably reducing the noise reduction performance.},
	language = {en},
	booktitle = {Proc. {Int}. {Workshop} {Acoust}. {Echo} {Noise} {Control} ({IWAENC})},
	author = {Doclo, Simon and Klasen, Thomas J and Van den Bogaert, Tim and Wouters, Jan and Moonen, Marc},
	month = sep,
	year = {2006},
	pages = {4},
	file = {Doclo et Klasen - 2006 - THEORETICAL ANALYSIS OF BINAURAL CUE PRESERVATION .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZTFCGRQE\\Doclo et Klasen - 2006 - THEORETICAL ANALYSIS OF BINAURAL CUE PRESERVATION .pdf:application/pdf},
}

@inproceedings{klasen_binaural_2006,
	address = {Toulouse, France},
	title = {Binaural {Multi}-{Channel} {Wiener} {Filtering} for {Hearing} {Aids}: {Preserving} {Interaural} {Time} and {Level} {Differences}},
	volume = {5},
	isbn = {978-1-4244-0469-8},
	shorttitle = {Binaural {Multi}-{Channel} {Wiener} {Filtering} for {Hearing} {Aids}},
	url = {http://ieeexplore.ieee.org/document/1661233/},
	doi = {10.1109/ICASSP.2006.1661233},
	abstract = {This paper presents an extension of the binaural multi-channel Wiener ﬁltering algorithm discussed in [1]. The goal of this paper is to preserve both the interaural time difference (ITD) and interaural level difference (ILD) of the speech and noise components. This is done by extending the cost function to incorporate terms for the interaural transfer functions (ITF) of the speech and noise components. Using weights, the emphasis on the preservation of the ITFs can be controlled in addition to the emphasis on noise reduction. Adapting these parameters allows one to preserve the ITFs of the speech and noise component, and therefore ITD and ILD cues, while enhancing the signal-to-noise ratio.},
	language = {en},
	urldate = {2018-11-14},
	booktitle = {2006 {IEEE} {International} {Conference} on {Acoustics} {Speed} and {Signal} {Processing} {Proceedings}},
	publisher = {IEEE},
	author = {Klasen, T.J. and Doclo, S. and Van den Bogaert, T. and Moonen, M. and Wouters, J.},
	year = {2006},
	pages = {V--145--V--148},
	file = {Klasen et al. - 2006 - Binaural Multi-Channel Wiener Filtering for Hearin.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RIZ6I5Q5\\Klasen et al. - 2006 - Binaural Multi-Channel Wiener Filtering for Hearin.pdf:application/pdf},
}

@article{gosling_optimal_2018,
	title = {Optimal {Binaural} {LCMV} {Beamforming} in {Complex} {Acoustic} {Scenarios}: {Theoretical} and {Practical} {Insights}},
	shorttitle = {Optimal {Binaural} {LCMV} {Beamforming} in {Complex} {Acoustic} {Scenarios}},
	url = {http://arxiv.org/abs/1807.04636},
	abstract = {Binaural beamforming algorithms for head-mounted assistive listening devices are crucial to improve speech quality and speech intelligibility in noisy environments, while maintaining the spatial impression of the acoustic scene. While the well-known BMVDR beamformer is able to preserve the binaural cues of one desired source, the BLCMV beamformer uses additional constraints to also preserve the binaural cues of interfering sources. In this paper, we provide theoretical and practical insights on how to optimally set the interference scaling parameters in the BLCMV beamformer for an arbitrary number of interfering sources. In addition, since in practice only a limited temporal observation interval is available to estimate all required beamformer quantities, we provide an experimental evaluation in a complex acoustic scenario using measured impulse responses from hearing aids in a cafeteria for different observation intervals. The results show that even rather short observation intervals are sufficient to achieve a decent noise reduction performance and that a proposed threshold on the optimal interference scaling parameters leads to smaller binaural cue errors in practice.},
	language = {en},
	urldate = {2018-11-19},
	journal = {arXiv:1807.04636 [cs, eess]},
	author = {Gößling, N. and Marquardt, D. and Merks, I. and Zhang, T. and Doclo, S.},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.04636},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Gößling et al. - 2018 - Optimal Binaural LCMV Beamforming in Complex Acous.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\8MADNKML\\Gößling et al. - 2018 - Optimal Binaural LCMV Beamforming in Complex Acous.pdf:application/pdf},
}

@inproceedings{koutrouvelis_binaural_2017,
	address = {Kos, Greece},
	title = {Binaural beamforming using pre-determined relative acoustic transfer functions},
	isbn = {978-0-9928626-7-1},
	url = {http://ieeexplore.ieee.org/document/8081157/},
	doi = {10.23919/EUSIPCO.2017.8081157},
	abstract = {Binaural beamformers (BFs) aim to reduce the output noise power while simultaneously preserving the binaural cues of all sources. Typically, the latter is accomplished via constraints relating the output and input interaural transfer functions (ITFs). The ITF is a function of the corresponding relative acoustic transfer function (RATF), which implies that RATF estimates of all sources in the acoustic scene are required. Here, we propose an alternative way to approximately preserve the binaural cues of the entire acoustic scene without estimating RATFs. We propose to preserve the binaural cues of all sources with a set of ﬁxed pre-determined RATFs distributed around the head. Two recently proposed binaural BFs are evaluated in the context of using pre-determined RATFs and compared to the binaural minimum variance distrortionless response BF which can only preserve the binaural cues of the target.},
	language = {en},
	urldate = {2018-11-19},
	booktitle = {2017 25th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	publisher = {IEEE},
	author = {Koutrouvelis, Andreas I. and Hendriks, Richard C. and Heusdens, Richard and Jensen, Jesper and Guo, Meng},
	month = aug,
	year = {2017},
	pages = {1--5},
	file = {Koutrouvelis et al. - 2017 - Binaural beamforming using pre-determined relative.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UJ6TGZ3L\\Koutrouvelis et al. - 2017 - Binaural beamforming using pre-determined relative.pdf:application/pdf},
}

@article{asad_robust_nodate,
	title = {A {Robust} {Binaural} {Linearly} {Constrained} {Minimum} {Variance} with {Spatial} {Cues} {Preservation} for {Hearing} {Aids} {Beamforming}},
	abstract = {In this work, a robust binaural beamforming algorithm for hearing aid applications is introduced. The robust binaural beamforming algorithm has two main components: a Robust Binaural Linearly Constrained Minimum Variance (BLCMV) based on imposing two constraints around the estimated direction of the target signal, and a post processor based on a complex coherence for classification, selection, and mixing of binaural signals at each time-frequency bin. The robust BLCMV provides a good level of noise reduction and low level of target distortion under realistic conditions. The post processor enhances the beamformer abilities to preserve the binaural cues for the diffuse-like background noise and the directional interferers, while keeping a good level of noise reduction. The algorithm introduced does not require knowledge of the interfering sources’ directions nor the second order statistics of the noise-only components. The introduced algorithm requires an estimate of the target speaker direction, but it is designed to be robust to some deviation from the estimated direction. Comparing with a recently proposed state of the art method, comprehensive evaluations have been performed under complex acoustic scenarios generated in both anechoic and reverberant environments, considering a mismatch between the estimated and the true direction of arrivals for the target speakers as well as for the interferers. Mismatch between the anechoic propagation models used for the design and the reverberant propagation models used to generate the directional signals is also considered. The results illustrate the robustness of the proposed algorithm to mismatches generated from the direction of arrivals and from the reverberation.},
	language = {en},
	author = {As’ad, H and Bouchard, M and Kamkar-Parsi, H},
	pages = {12},
	file = {As’ad et al. - A Robust Binaural Linearly Constrained Minimum Var.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GRDKDHHA\\As’ad et al. - A Robust Binaural Linearly Constrained Minimum Var.pdf:application/pdf},
}

@phdthesis{farmani_informed_2017,
	title = {Informed {Sound} {Source} {Localization} for {Hearing} {Aid} {Applications}},
	url = {http://vbn.aau.dk/da/publications/informed-sound-source-localization-for-hearing-aid-applications(a6730185-1a22-4ac2-876e-2d05923fec69).html},
	language = {en},
	urldate = {2018-11-19},
	school = {Aalborg University},
	author = {Farmani, Mojtaba},
	year = {2017},
	doi = {10.5278/vbn.phd.tech.00009},
	file = {Farmani - 2017 - Informed Sound Source Localization for Hearing Aid.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\56NNR5SD\\Farmani - 2017 - Informed Sound Source Localization for Hearing Aid.pdf:application/pdf},
}

@inproceedings{corey_dynamic_2017,
	address = {New Paltz, NY},
	title = {Dynamic range compression for noisy mixtures using source separation and beamforming},
	isbn = {978-1-5386-1632-1},
	url = {http://ieeexplore.ieee.org/document/8170041/},
	doi = {10.1109/WASPAA.2017.8170041},
	abstract = {Dynamic range compression is widely used in digital hearing aids, but performs poorly in noisy conditions with multiple sources. We propose a system that combines source separation, compression, and beamforming to compress each source independently. We derive an expression for a time-varying weighted multichannel Wiener ﬁlter that performs both beamforming and compression. Experiments using recorded speech and behind-the-ear hearing aid impulse responses suggest that the combined system provides more accurate dynamic range compression than a conventional compressor in the presence of competing speech and background noise.},
	language = {en},
	urldate = {2018-11-19},
	booktitle = {2017 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
	publisher = {IEEE},
	author = {Corey, Ryan M. and Singer, Andrew C.},
	month = oct,
	year = {2017},
	pages = {289--293},
	file = {Corey et Singer - 2017 - Dynamic range compression for noisy mixtures using.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5ZLCNAEH\\Corey et Singer - 2017 - Dynamic range compression for noisy mixtures using.pdf:application/pdf},
}

@article{koutrouvelis_convex_2018,
	title = {A {Convex} {Approximation} of the {Relaxed} {Binaural} {Beamforming} {Optimization} {Problem}},
	issn = {2329-9290, 2329-9304},
	url = {http://arxiv.org/abs/1805.01692},
	doi = {10.1109/TASLP.2018.2878618},
	abstract = {The recently proposed relaxed binaural beamforming (RBB) optimization problem provides a ﬂexible trade-off between noise suppression and binaural-cue preservation of the sound sources in the acoustic scene. It minimizes the output noise power, under the constraints which guarantee that the target remains unchanged after processing and the binaural-cue distortions of the acoustic sources will be less than a user-deﬁned threshold. However, the RBB problem is a computationally demanding non-convex optimization problem. The only existing suboptimal method which approximately solves the RBB is a successive convex optimization (SCO) method which, typically, requires to solve multiple convex optimization problems per frequency bin, in order to converge. Convergence is achieved when all constraints of the RBB optimization problem are satisﬁed. In this paper, we propose a semi-deﬁnite convex relaxation (SDCR) of the RBB optimization problem. The proposed suboptimal SDCR method solves a single convex optimization problem per frequency bin, resulting in a much lower computational complexity than the SCO method. Unlike the SCO method, the SDCR method does not guarantee user-controlled upper-bounded binaural-cue distortions. To tackle this problem we also propose a suboptimal hybrid method which combines the SDCR and SCO methods. Instrumental measures combined with a listening test show that the SDCR and hybrid methods achieve signiﬁcantly lower computational complexity than the SCO method, and in most cases better trade-off between predicted intelligibility and binaural-cue preservation than the SCO method.},
	language = {en},
	urldate = {2018-11-19},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Koutrouvelis, Andreas I. and Hendriks, Richard C. and Heusdens, Richard and Jensen, Jesper},
	year = {2018},
	note = {arXiv: 1805.01692},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Information Theory},
	pages = {1--1},
	file = {Koutrouvelis et al. - 2018 - A Convex Approximation of the Relaxed Binaural Bea.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HAK7WCNK\\Koutrouvelis et al. - 2018 - A Convex Approximation of the Relaxed Binaural Bea.pdf:application/pdf},
}

@inproceedings{koutrouvelis_improved_2016,
	address = {Shanghai},
	title = {Improved multi-microphone noise reduction preserving binaural cues},
	isbn = {978-1-4799-9988-0},
	url = {http://ieeexplore.ieee.org/document/7471717/},
	doi = {10.1109/ICASSP.2016.7471717},
	abstract = {We propose a new multi-microphone noise reduction technique for binaural cue preservation of the desired source and the interferers. This method is based on the linearly constrained minimum variance (LCMV) framework, where the constraints are used for the binaural cue preservation of the desired source and of multiple interferers. In this framework there is a trade-off between noise reduction and binaural cue preservation. The more constraints the LCMV uses for preserving binaural cues, the less degrees of freedom can be used for noise suppression. The recently presented binaural LCMV (BLCMV) method and the optimal BLCMV (OBLCMV) method require two constraints per interferer and introduce an additional interference rejection parameter. This unnecessarily reduces the degrees of freedom, available for noise reduction, and negatively inﬂuences the trade-off between noise reduction and binaural cue preservation. With the proposed method, binaural cue preservation is obtained using just a single constraint per interferer without the need of an interference rejection parameter. The proposed method can simultaneously achieve noise reduction and perfect binaural cue preservation of more than twice as many interferers as the BLCMV, while the OBLCMV can preserve the binaural cues of only one interferer.},
	language = {en},
	urldate = {2018-11-19},
	booktitle = {2016 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Koutrouvelis, Andreas I. and Hendriks, Richard C. and Jensen, Jesper and Heusdens, Richard},
	month = mar,
	year = {2016},
	pages = {460--464},
	file = {Koutrouvelis et al. - 2016 - Improved multi-microphone noise reduction preservi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YAHPPV3X\\Koutrouvelis et al. - 2016 - Improved multi-microphone noise reduction preservi.pdf:application/pdf},
}

@inproceedings{markovich-golan_weighted_2012,
	address = {Eilat, Israel},
	title = {A weighted multichannel {Wiener} filter for multiple sources scenarios},
	isbn = {978-1-4673-4681-8 978-1-4673-4682-5 978-1-4673-4680-1},
	url = {http://ieeexplore.ieee.org/document/6376958/},
	doi = {10.1109/EEEI.2012.6376958},
	language = {en},
	urldate = {2018-11-20},
	booktitle = {2012 {IEEE} 27th {Convention} of {Electrical} and {Electronics} {Engineers} in {Israel}},
	publisher = {IEEE},
	author = {Markovich-Golan, Shmulik and Gannot, Sharon and Cohen, Israel},
	month = nov,
	year = {2012},
	pages = {1--5},
	file = {Markovich-Golan et al. - 2012 - A weighted multichannel Wiener filter for multiple.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3NNUC3XZ\\Markovich-Golan et al. - 2012 - A weighted multichannel Wiener filter for multiple.pdf:application/pdf},
}

@article{hu_local_2016,
	title = {A local representation of the head-related transfer function},
	volume = {140},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4962805},
	doi = {10.1121/1.4962805},
	abstract = {Spatial descriptions of the head-related transfer function (HRTF) using spherical harmonics, which is commonly used for the purpose, consider all directions simultaneously. However, in perceptual studies, it is necessary to model HRTFs with different angular resolutions at different directions. To this end, an alternative spatial representation of the HRTF, based on local analysis functions, is introduced. The proposal is shown to have the potential to describe the local features of the HRTF. This is veriﬁed by comparing the reconstruction error achieved by the proposal to that of the spherical harmonic decomposition when reconstructing the HRTF inside a spherical cap.},
	language = {en},
	number = {3},
	urldate = {2018-11-20},
	journal = {The Journal of the Acoustical Society of America},
	author = {Hu, Shichao and Trevino, Jorge and Salvador, Cesar and Sakamoto, Shuichi and Li, Junfeng and Suzuki, Yôiti},
	month = sep,
	year = {2016},
	pages = {EL285--EL290},
	file = {Hu et al. - 2016 - A local representation of the head-related transfe.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GXA2EPJV\\Hu et al. - 2016 - A local representation of the head-related transfe.pdf:application/pdf},
}

@inproceedings{asad_binaural_2017,
	address = {Kuala Lumpur},
	title = {Binaural beamforming with spatial cues preservation for hearing aids in real-life complex acoustic environments},
	isbn = {978-1-5386-1542-3},
	url = {http://ieeexplore.ieee.org/document/8282250/},
	doi = {10.1109/APSIPA.2017.8282250},
	abstract = {This work is introducing novel binaural beamforming algorithms for hearing aids, with a good trade-off between noise reduction and the preservation of the binaural cues for different types of sources (directional interfering talker sources, diffuse-like background noise). In the proposed methods, no knowledge of the interfering talkers’ direction or the second order statistics of the noise-only components is required. Different classification decisions are considered in the timefrequency domain based on the power, the power difference, and the complex coherence of different available signals. Simulations are performed using signals recorded from multichannel binaural hearing aids, to validate the performance of the proposed algorithms under different acoustic scenarios and using different microphone configurations. For the simulations performed in this paper, a good knowledge of the target direction and propagation model is assumed. For hearing aids, this assumption is typically more realistic than the assumption of knowing the direction and propagation model of the interferer talkers. The comparison of the performance results is done with other algorithms that don't require information on the directions or statistics of the interfering talker sources and the background noise. The results indicate that the proposed algorithms can either provide nearly the same noise reduction as classical beamformers but with improved noise binaural cues preservation, or they can produce a good trade-off between noise reduction and noise binaural cues preservation.},
	language = {en},
	urldate = {2018-11-21},
	booktitle = {2017 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	publisher = {IEEE},
	author = {As'ad, Hala and Bouchard, Martin and Kamkar-Parsi, Homayoun},
	month = dec,
	year = {2017},
	pages = {1390--1399},
	file = {As'ad et al. - 2017 - Binaural beamforming with spatial cues preservatio.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\8IPS788G\\As'ad et al. - 2017 - Binaural beamforming with spatial cues preservatio.pdf:application/pdf},
}

@inproceedings{erdogan_improved_2016,
	address = {San Francisco, USA},
	title = {Improved {MVDR} {Beamforming} {Using} {Single}-{Channel} {Mask} {Prediction} {Networks}},
	url = {http://www.isca-speech.org/archive/Interspeech_2016/abstracts/0552.html},
	doi = {10.21437/Interspeech.2016-552},
	abstract = {Recent studies on multi-microphone speech databases indicate that it is beneﬁcial to perform beamforming to improve speech recognition accuracies, especially when there is a high level of background noise. Minimum variance distortionless response (MVDR) beamforming is an important beamforming method that performs quite well for speech recognition purposes especially if the steering vector is known. However, steering the beamformer to focus on speech in unknown acoustic conditions remains a challenging problem. In this study, we use singlechannel speech enhancement deep networks to form masks that can be used for noise spatial covariance estimation, which steers the MVDR beamforming toward the speech. We analyze how mask prediction affects performance and also discuss various ways to use masks to obtain the speech and noise spatial covariance estimates in a reliable way. We show that using a single mask across microphones for covariance prediction with minima-limited post-masking yields the best result in terms of signal-level quality measures and speech recognition word error rates in a mismatched training condition.},
	language = {en},
	urldate = {2018-11-21},
	booktitle = {Interspeech},
	author = {Erdogan, Hakan and Hershey, John R. and Watanabe, Shinji and Mandel, Michael I. and Roux, Jonathan Le},
	month = sep,
	year = {2016},
	pages = {1981--1985},
	file = {Erdogan et al. - 2016 - Improved MVDR Beamforming Using Single-Channel Mas.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XD24BKJH\\Erdogan et al. - 2016 - Improved MVDR Beamforming Using Single-Channel Mas.pdf:application/pdf},
}

@phdthesis{koutrouvelis_multi-microphone_2018,
	title = {Multi-{Microphone} {Noise} {Reduction} for {Hearing} {Assistive} {Devices}},
	url = {http://resolver.tudelft.nl/uuid:cdb32aa2-9ca4-448c-a8a0-63f458c375ff},
	language = {en},
	urldate = {2018-11-27},
	school = {Delft University of Technology},
	author = {Koutrouvelis, A.},
	year = {2018},
	doi = {10.4233/uuid:cdb32aa2-9ca4-448c-a8a0-63f458c375ff},
	file = {Koutrouvelis - 2018 - Multi-Microphone Noise Reduction for Hearing Assis.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LPLE8PML\\Koutrouvelis - 2018 - Multi-Microphone Noise Reduction for Hearing Assis.pdf:application/pdf},
}

@article{ben-hur_spectral_2017,
	title = {Spectral equalization in binaural signals represented by order-truncated spherical harmonics},
	volume = {141},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4983652},
	doi = {10.1121/1.4983652},
	language = {en},
	number = {6},
	urldate = {2018-12-03},
	journal = {The Journal of the Acoustical Society of America},
	author = {Ben-Hur, Zamir and Brinkmann, Fabian and Sheaffer, Jonathan and Weinzierl, Stefan and Rafaely, Boaz},
	month = jun,
	year = {2017},
	pages = {4087--4096},
	file = {Ben-Hur et al. - 2017 - Spectral equalization in binaural signals represen.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9SN6YIMB\\Ben-Hur et al. - 2017 - Spectral equalization in binaural signals represen.pdf:application/pdf},
}

@article{hu_modeling_2019,
	title = {Modeling head-related transfer functions with spherical wavelets},
	volume = {146},
	issn = {0003682X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0003682X18301397},
	doi = {10.1016/j.apacoust.2018.10.026},
	abstract = {The head-related transfer function (HRTF) describes the sound transmission characteristics from a sound source to a listener’s ears. Recently, spherical harmonic decomposition has been extensively used for modeling the HRTF spatial patterns. Despite its advantage of approximating the coarse structure of HRTF spatial variations with modeling up to a low order, there are still some limitations since spherical harmonics take signiﬁcant values in all directions. First, rapidly changing HRTF spatial variations in some local regions may require modeling up to a rather high order; this is not wise in terms of the modeling efﬁciency. Second, the expansion coefﬁcients of the spherical harmonics describe the spatial frequency of the target dataset in all directions, and thus have difﬁculties in revealing the direction dependent HRTF characteristics. In this study, a method for locally modeling HRTF spatial patterns is proposed based on spherical wavelets, which take signiﬁcant values only over a local region on the sphere. Results of numerical experiments show that our proposed method yields smaller approximation errors than the conventional method when representing HRTFs inside the local regions under evaluation. Furthermore, the expansion coefﬁcients in the proposed method could well correspond to the HRTF local features on the sphere, which makes it a useful tool for the analysis and visualization of HRTF spatial patterns.},
	language = {en},
	urldate = {2018-12-04},
	journal = {Applied Acoustics},
	author = {Hu, Shichao and Trevino, Jorge and Salvador, César and Sakamoto, Shuichi and Suzuki, Yôiti},
	month = mar,
	year = {2019},
	pages = {81--88},
	file = {Hu et al. - 2019 - Modeling head-related transfer functions with sphe.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3BGMLSBR\\Hu et al. - 2019 - Modeling head-related transfer functions with sphe.pdf:application/pdf},
}

@article{favre-felix_improving_2018,
	title = {Improving {Speech} {Intelligibility} by {Hearing} {Aid} {Eye}-{Gaze} {Steering}: {Conditions} {With} {Head} {Fixated} in a {Multitalker} {Environment}},
	volume = {22},
	issn = {2331-2165, 2331-2165},
	shorttitle = {Improving {Speech} {Intelligibility} by {Hearing} {Aid} {Eye}-{Gaze} {Steering}},
	url = {http://journals.sagepub.com/doi/10.1177/2331216518814388},
	doi = {10.1177/2331216518814388},
	abstract = {The behavior of a person during a conversation typically involves both auditory and visual attention. Visual attention implies that the person directs his or her eye gaze toward the sound target of interest, and hence, detection of the gaze may provide a steering signal for future hearing aids. The steering could utilize a beamformer or the selection of a specific audio stream from a set of remote microphones. Previous studies have shown that eye gaze can be measured through electrooculography (EOG). To explore the precision and real-time feasibility of the methodology, seven hearing-impaired persons were tested, seated with their head fixed in front of three targets positioned at À30 , 0 , and þ30  azimuth. Each target presented speech from the Danish DAT material, which was available for direct input to the hearing aid using head-related transfer functions. Speech intelligibility was measured in three conditions: a reference condition without any steering, a condition where eye gaze was estimated from EOG measures to select the desired audio stream, and an ideal condition with steering based on an eye-tracking camera. The ‘‘EOG-steering’’ improved the sentence correct score compared with the ‘‘no-steering’’ condition, although the performance was still significantly lower than the ideal condition with the eye-tracking camera. In conclusion, eye-gaze steering increases speech intelligibility, although real-time EOG-steering still requires improvements of the signal processing before it is feasible for implementation in a hearing aid.},
	language = {en},
	urldate = {2018-12-19},
	journal = {Trends in Hearing},
	author = {Favre-Félix, Antoine and Graversen, Carina and Hietkamp, Renskje K. and Dau, Torsten and Lunner, Thomas},
	month = jan,
	year = {2018},
	file = {Favre-Félix et al. - 2018 - Improving Speech Intelligibility by Hearing Aid Ey.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RDJPTDHE\\Favre-Félix et al. - 2018 - Improving Speech Intelligibility by Hearing Aid Ey.pdf:application/pdf},
}

@article{rana_effect_2018,
	title = {Effect of improving audibility on better-ear glimpsing using non-linear amplification},
	volume = {144},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.5083823},
	doi = {10.1121/1.5083823},
	language = {en},
	number = {6},
	urldate = {2019-01-15},
	journal = {The Journal of the Acoustical Society of America},
	author = {Rana, Baljeet and Buchholz, Jörg M.},
	month = dec,
	year = {2018},
	pages = {3465--3474},
	file = {Rana et Buchholz - 2018 - Effect of improving audibility on better-ear glimp.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HBUNXZS9\\Rana et Buchholz - 2018 - Effect of improving audibility on better-ear glimp.pdf:application/pdf},
}

@phdthesis{rehr_robust_2018,
	address = {Hambourg},
	title = {Robust {Speech} {Enhancement} {Using} {Statistical} {Signal} {Processing} and {Machine}-{Learning}},
	language = {de},
	school = {Universität Hambourg},
	author = {Rehr, Robert},
	year = {2018},
	file = {Rehr - Robust Speech Enhancement Using Statistical Signal.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\P48ADX7K\\Rehr - Robust Speech Enhancement Using Statistical Signal.pdf:application/pdf},
}

@misc{kreutz-delgado_complex_2009,
	title = {The {Complex} {Gradient} {Operator} and the {CR}-{Calculus}},
	url = {http://arxiv.org/abs/0906.4835},
	abstract = {A thorough discussion and development of the calculus of real-valued functions of complex-valued vectors is given using the framework of the Wirtinger Calculus. The presented material is suitable for exposition in an introductory Electrical Engineering graduate level course on the use of complex gradients and complex Hessian matrices, and has been successfully used in teaching at UC San Diego. Going beyond the commonly encountered treatments of the first-order complex vector calculus, second-order considerations are examined in some detail filling a gap in the pedagogic literature.},
	language = {en},
	urldate = {2019-01-25},
	author = {Kreutz-Delgado, Ken},
	month = jun,
	year = {2009},
	note = {arXiv: 0906.4835},
	keywords = {Mathematics - Complex Variables, Mathematics - Optimization and Control},
	file = {Kreutz-Delgado - 2009 - The Complex Gradient Operator and the CR-Calculus.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5VDDI52Y\\Kreutz-Delgado - 2009 - The Complex Gradient Operator and the CR-Calculus.pdf:application/pdf},
}

@phdthesis{olsson_algorithms_2007,
	type = {{PhD} {Thesis}},
	title = {Algorithms for {Source} {Separation} with {Cocktail} {Party} {Applications}},
	school = {DTU},
	author = {Olsson, Rasmus Kongsgaard},
	year = {2007},
	file = {imm5270(1).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3QLQUMGP\\imm5270(1).pdf:application/pdf},
}

@article{spagnol_structural_2010,
	title = {Structural {Modeling} {Of} {Pinna}-{Related} {Transfer} {Functions}},
	abstract = {This paper faces the general problem of modeling pinnarelated transfer functions (PRTFs) for 3-D sound rendering. Following a structural modus operandi, we exploit an algorithm for the decomposition of PRTFs into ear resonances and frequency notches due to reﬂections over pinna cavities in order to deliver a method to extract the frequencies of the most important spectral notches. Ray-tracing analysis reveals a convincing correspondence between extracted frequencies and pinna cavities of a bunch of subjects. We then propose a model for PRTF synthesis which allows to control separately the evolution of resonances and spectral notches through the design of two distinct ﬁlter blocks. The resulting model is suitable for future integration into a structural head-related transfer function model, and for parametrization over anthropometrical measurements of a wide range of subjects.},
	language = {en},
	journal = {Sound and Music Computing Conference},
	author = {Spagnol, Simone and Geronazzo, Michele and Avanzini, Federico},
	year = {2010},
	pages = {422--428},
	file = {Spagnol et al. - Structural Modeling Of Pinna-Related Transfer Func.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BJP29KPB\\Spagnol et al. - Structural Modeling Of Pinna-Related Transfer Func.pdf:application/pdf},
}

@article{stone_side_2004,
	title = {Side effects of fast-acting dynamic range compression that affect intelligibility in a competing speech task},
	volume = {116},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1784447},
	doi = {10.1121/1.1784447},
	language = {en},
	number = {4},
	urldate = {2019-01-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Stone, Michael A. and Moore, Brian C. J.},
	month = oct,
	year = {2004},
	pages = {2311--2323},
	file = {Stone et Moore - 2004 - Side effects of fast-acting dynamic range compress.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2AY2569Z\\Stone et Moore - 2004 - Side effects of fast-acting dynamic range compress.pdf:application/pdf},
}

@article{souza_measuring_2006,
	title = {Measuring the acoustic effects of compression amplification on speech in noise},
	volume = {119},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2108861},
	doi = {10.1121/1.2108861},
	language = {en},
	number = {1},
	urldate = {2019-01-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Souza, Pamela E. and Jenstad, Lorienne M. and Boike, Kumiko T.},
	month = jan,
	year = {2006},
	pages = {41--44},
	file = {Souza et al. - 2006 - Measuring the acoustic effects of compression ampl.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LIPLL2MJ\\Souza et al. - 2006 - Measuring the acoustic effects of compression ampl.pdf:application/pdf},
}

@article{stone_effects_2008,
	title = {Effects of spectro-temporal modulation changes produced by multi-channel compression on intelligibility in a competing-speech task},
	volume = {123},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2821969},
	doi = {10.1121/1.2821969},
	language = {en},
	number = {2},
	urldate = {2019-01-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Stone, Michael A. and Moore, Brian C. J.},
	month = feb,
	year = {2008},
	pages = {1063--1076},
	file = {Stone et Moore - 2008 - Effects of spectro-temporal modulation changes pro.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VXA83BH6\\Stone et Moore - 2008 - Effects of spectro-temporal modulation changes pro.pdf:application/pdf},
}

@inproceedings{markovich_comparison_2008,
	address = {Eilat, Israel},
	title = {A comparison between alternative beamforming strategies for interference cancelation in noisy and reverberant environment},
	isbn = {978-1-4244-2481-8},
	url = {http://ieeexplore.ieee.org/document/4736688/},
	doi = {10.1109/EEEI.2008.4736688},
	abstract = {In speech communication systems the received microphone signals are often degraded by competing speakers, noise signals and room reverberation. Microphone arrays are commonly utilized to enhance the desired speech signal. In this paper two important design criteria, namely the Minimum Variance Distortionless Response (MVDR) and the Linearly Constrained Minimum Variance (LCMV) beamformers, are explored. These structures differ in their treatment of the interference sources. Experimental results using simulated reverberant enclosure are used for comparing the two strategies. It is shown that the LCMV beamformer outperforms the MVDR beamformer provided that the acoustic environment is time-invariant.},
	language = {en},
	urldate = {2019-02-01},
	booktitle = {2008 {IEEE} 25th {Convention} of {Electrical} and {Electronics} {Engineers} in {Israel}},
	publisher = {IEEE},
	author = {Markovich, Shmulik and Gannot, Sharon and Cohen, Israel},
	month = mar,
	year = {2008},
	pages = {203--207},
	file = {Markovich et al. - 2008 - A comparison between alternative beamforming strat.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\L7QR56CA\\Markovich et al. - 2008 - A comparison between alternative beamforming strat.pdf:application/pdf},
}

@article{souden_study_2010,
	title = {A {Study} of the {LCMV} and {MVDR} {Noise} {Reduction} {Filters}},
	volume = {58},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/5482097/},
	doi = {10.1109/TSP.2010.2051803},
	abstract = {In real-world environments, the signals captured by a set of microphones in a speech communication system are mixtures of the desired signal, interference, and ambient noise. A promising solution for proper speech acquisition (with reduced noise and interference) in this context consists in using the linearly constrained minimum variance (LCMV) beamformer to reject the interference, reduce the overall mixture energy, and preserve the target signal. The minimum variance distortionless response beamformer (MVDR) is also commonly known to reduce the interferenceplus-noise energy without distorting the desired signal. In either case, it is of paramount importance to accurately quantify the achieved noise and interference reduction. Indeed, it is quite reasonable to ask, for instance, about the price that has to be paid in order to achieve total removal of the interference without distorting the target signal when using the LCMV. Besides, it is fundamental to understand the effect of the MVDR on both noise and interference. In this correspondence, we investigate the performance of the MVDR and LCMV beamformers when the interference and ambient noise coexist with the target source. We demonstrate a new relationship between both ﬁlters in which the MVDR is decomposed into the LCMV and a matched ﬁlter (MVDR solution in the absence of interference). Both components are properly weighted to achieve maximum interference-plusnoise reduction. We investigate the performance of the MVDR, LCMV, and matched ﬁlters and elaborate new closed-form expressions for their output signal-to-interference ratio (SIR) and output signal-to-noise ratio (SNR). We theoretically demonstrate the tradeoff that has to be made between noise reduction and interference rejection. In fact, the total removal of the interference may severely amplify the residual ambient noise. Conversely, totally focussing on noise reduction leads to increased level of residual interference. The proposed study is ﬁnally supported by several numerical examples.},
	language = {en},
	number = {9},
	urldate = {2019-02-01},
	journal = {IEEE Transactions on Signal Processing},
	author = {Souden, Mehrez and Benesty, Jacob and Affes, Sofiène},
	month = sep,
	year = {2010},
	pages = {4925--4935},
	file = {Souden et al. - 2010 - A Study of the LCMV and MVDR Noise Reduction Filte.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Z386G5VC\\Souden et al. - 2010 - A Study of the LCMV and MVDR Noise Reduction Filte.pdf:application/pdf},
}

@incollection{benesty_mvdr_2010,
	address = {Berlin, Heidelberg},
	title = {The {MVDR} {Beamformer} for {Speech} {Enhancement}},
	volume = {3},
	isbn = {978-3-642-11129-7 978-3-642-11130-3},
	url = {http://link.springer.com/10.1007/978-3-642-11130-3_9},
	abstract = {The minimum variance distortionless response (MVDR) beamformer is widely studied in the area of speech enhancement and can be used for both speech dereverberation and noise reduction1. This chapter summarizes some new insights into the MVDR beamformer. SpeciØcally, the local and global behaviour of the MVDR beamformer is analysed, diÆerent forms of the MVDR beamformer and relations between the MVDR and other optimal beamformers are discussed. In addition, the tradeoÆ between dereverberation and noise reduction is analysed. This analysis is done for a mixture of coherent and non-coherent noise Øelds and entirely non-coherent noise Øelds. It is shown that maximum noise reduction is achieved when the MVDR beamformer is used for noise reduction only. The amount of noise reduction that is sacriØced when complete dereverberation is required depends on the direct-to-reverberation ratio of the acoustic impulse response between the source and the reference microphone. The performance evaluation demonstrates the tradeoÆ between dereverberation and noise reduction.},
	language = {en},
	urldate = {2019-02-01},
	booktitle = {Speech {Processing} in {Modern} {Communication}},
	publisher = {Springer Berlin Heidelberg},
	author = {Habets, Emanuël A. P. and Benesty, Jacob and Gannot, Sharon and Cohen, Israel},
	editor = {Benesty, Jacob and Kellermann, Walter and Cohen, Israel and Benesty, Jacob and Gannot, Sharon},
	year = {2010},
	doi = {10.1007/978-3-642-11130-3_9},
	pages = {225--254},
	file = {Habets et al. - 2010 - The MVDR Beamformer for Speech Enhancement.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RBXPCQYR\\Habets et al. - 2010 - The MVDR Beamformer for Speech Enhancement.pdf:application/pdf},
}

@article{raghu_expressive_2016,
	title = {On the {Expressive} {Power} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1606.05336},
	abstract = {We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings can be summarized as follows: (1) The complexity of the computed function grows exponentially with depth. (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights. (3) Regularizing on trajectory length (trajectory regularization) is a simpler alternative to batch normalization, with the same performance.},
	language = {en},
	urldate = {2019-02-13},
	journal = {arXiv:1606.05336 [cs, stat]},
	author = {Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.05336},
	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Raghu et al. - 2016 - On the Expressive Power of Deep Neural Networks.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\C2H6WGDF\\Raghu et al. - 2016 - On the Expressive Power of Deep Neural Networks.pdf:application/pdf},
}

@article{srivastava_dropout:_nodate,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overﬁtting}},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	language = {en},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	pages = {30},
	file = {Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4E76L9WZ\\Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf},
}

@book{vasiliev_python_2019,
	address = {Place of publication not identified},
	title = {Python {Deep} {Learning}: exploring deep learning techniques, neural network.},
	isbn = {978-1-78934-846-0},
	shorttitle = {{PYTHON} {DEEP} {LEARNING} -},
	language = {en},
	publisher = {PACKT Publishing Limited},
	author = {Vasiliev, Ivan and Slater, Daniel and Spacagna, Gianmario and Roelants, Peter and Zocca, Valentino},
	year = {2019},
	note = {OCLC: 1073089207},
	file = {2019 - PYTHON DEEP LEARNING - exploring deep learning te.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NUESG54Y\\2019 - PYTHON DEEP LEARNING - exploring deep learning te.pdf:application/pdf},
}

@book{pulkki_communication_2015,
	address = {Aalto University, Finland},
	title = {Communication {Acoustics}},
	language = {en},
	publisher = {Wiley},
	author = {Pulkki, Ville and Karjalainen, Matti},
	year = {2015},
	file = {Pulkki - Communication Acoustics.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EGP4CMVA\\Pulkki - Communication Acoustics.pdf:application/pdf},
}

@book{van_dijk_physiology_2014,
	series = {[{Advances} in {Experimental} {Medicine} and {Biology} 894]},
	title = {Physiology, {Psychoacoustics} and {Cognition} in {Normal} and {Impaired} {Hearing}},
	publisher = {Springer},
	author = {van Dijk, Pim and Başkent, Deniz and Gaudrain, Etienne and de Kleine, Emile and Wagner, Anita and Lanting, Cris},
	year = {2014},
	file = {[Advances in Experimental Medicine and Biology 894] Pim van Dijk, Deniz Başkent, Etienne Gaudrain, Emile de Kleine, Anita Wagner, Cris Lanting (eds.) - Physiology, Psychoacoustics and Cognition in Normal and Impaired .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MY27GFIJ\\[Advances in Experimental Medicine and Biology 894] Pim van Dijk, Deniz Başkent, Etienne Gaudrain, Emile de Kleine, Anita Wagner, Cris Lanting (eds.) - Physiology, Psychoacoustics and Cognition in Normal and Impaired .pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	title = {Deep learning: adaptive computation and machine learning},
	publisher = {The MIT press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	file = {[Adaptive Computation and Machine Learning series] Bengio, Yoshua_ Courville, Aaron_ Goodfellow, Ian J - Deep learning_ adaptive computation and machine learning (2016, The MIT Press).pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7WW8D3TW\\[Adaptive Computation and Machine Learning series] Bengio, Yoshua_ Courville, Aaron_ Goodfellow, Ian J - Deep learning_ adaptive computation and machine learning (2016, The MIT Press).pdf:application/pdf},
}

@article{howard_mobilenets:_2017,
	title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
	shorttitle = {{MobileNets}},
	url = {http://arxiv.org/abs/1704.04861},
	abstract = {We present a class of efﬁcient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks. We introduce two simple global hyperparameters that efﬁciently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classiﬁcation. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, ﬁnegrain classiﬁcation, face attributes and large scale geo-localization.},
	language = {en},
	urldate = {2019-02-14},
	journal = {arXiv:1704.04861 [cs]},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04861},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IEYWGDAU\\Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:application/pdf},
}

@inproceedings{yu_permutation_2017,
	address = {New Orleans, LA},
	title = {Permutation invariant training of deep models for speaker-independent multi-talker speech separation},
	isbn = {978-1-5090-4117-6},
	url = {http://ieeexplore.ieee.org/document/7952154/},
	doi = {10.1109/ICASSP.2017.7952154},
	abstract = {We propose a novel deep learning training criterion, named permutation invariant training (PIT), for speaker independent multi-talker speech separation, commonly known as the cocktail-party problem. Oifferent from the multi-dass regression technique and the deep dustering (OPCL) technique, our novel approach minimizes the separation error directly. This strategy effectively solves the longlasting label permutation problem, that has prevented progress on deep learning based techniques for speech separation. We evaluated PIT on the WSJO and Oanish mixed-speech separation tasks and found that it compares favorably to non-negative matrix factorization (NMF), computational auditory scene analysis (CASA), and OPCL and generalizes weH over unseen speakers and languages. Since PIT is simple to implement and can be easily integrated and combined with other advanced techniques, we believe improvements built upon PIT can eventuaHy solve the cocktail-party problem.},
	language = {en},
	urldate = {2019-02-14},
	booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Yu, Dong and Kolbaek, Morten and Tan, Zheng-Hua and Jensen, Jesper},
	month = mar,
	year = {2017},
	pages = {241--245},
	file = {Yu et al. - 2017 - Permutation invariant training of deep models for .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\753KR9B9\\Yu et al. - 2017 - Permutation invariant training of deep models for .pdf:application/pdf},
}

@book{hastie_elements_nodate,
	edition = {Second Edition},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}},
	language = {en},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	file = {Tibshirani et Friedman - Valerie and Patrick Hastie.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AW6KFWVU\\Tibshirani et Friedman - Valerie and Patrick Hastie.pdf:application/pdf},
}

@incollection{makino_introduction_2018,
	address = {Cham},
	title = {An {Introduction} to {Multichannel} {NMF} for {Audio} {Source} {Separation}},
	isbn = {978-3-319-73030-1 978-3-319-73031-8},
	url = {http://link.springer.com/10.1007/978-3-319-73031-8_4},
	abstract = {This chapter introduces multichannel nonnegative matrix factorization (NMF) methods for audio source separation. All the methods and some of their extensions are introduced within a more general local Gaussian modeling (LGM) framework. These methods are very attractive since allow combining spatial and spectral cues in a joint and principal way, but also are natural extensions and generalizations of many single-channel NMF-based methods to the multichannel case. The chapter introduces the spectral (NMF-based) and spatial models, as well as the way to combine them within the LGM framework. Model estimation criteria and algorithms are described as well, while going deeper into details of some of them.},
	language = {en},
	urldate = {2019-02-21},
	booktitle = {Audio {Source} {Separation}},
	publisher = {Springer International Publishing},
	author = {Ozerov, Alexey and Févotte, Cédric and Vincent, Emmanuel},
	editor = {Makino, Shoji},
	year = {2018},
	doi = {10.1007/978-3-319-73031-8_4},
	pages = {73--94},
	file = {Ozerov et al. - 2018 - An Introduction to Multichannel NMF for Audio Sour.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5LH7U3WC\\Ozerov et al. - 2018 - An Introduction to Multichannel NMF for Audio Sour.pdf:application/pdf},
}

@article{bramslow_improving_2018,
	title = {Improving competing voices segregation for hearing impaired listeners using a low-latency deep neural network algorithm},
	volume = {144},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.5045322},
	doi = {10.1121/1.5045322},
	language = {en},
	number = {1},
	urldate = {2019-02-21},
	journal = {The Journal of the Acoustical Society of America},
	author = {Bramsløw, Lars and Naithani, Gaurav and Hafez, Atefeh and Barker, Tom and Pontoppidan, Niels Henrik and Virtanen, Tuomas},
	month = jul,
	year = {2018},
	pages = {172--185},
	file = {Bramsløw et al. - 2018 - Improving competing voices segregation for hearing.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\M842J4JY\\Bramsløw et al. - 2018 - Improving competing voices segregation for hearing.pdf:application/pdf},
}

@inproceedings{naithani_low_2017,
	address = {New Paltz, NY},
	title = {Low latency sound source separation using convolutional recurrent neural networks},
	isbn = {978-1-5386-1632-1},
	url = {http://ieeexplore.ieee.org/document/8169997/},
	doi = {10.1109/WASPAA.2017.8169997},
	abstract = {Deep neural networks (DNN) have been successfully employed for the problem of monaural sound source separation achieving state-of-the-art results. In this paper, we propose using convolutional recurrent neural network (CRNN) architecture for tackling this problem. We focus on a scenario where low algorithmic delay (≤ 10 ms) is paramount, and relatively little training data is available. We show that the proposed architecture can achieve slightly better performance as compared to feedforward DNNs and long short-term memory (LSTM) networks. In addition to reporting separation performance metrics (i.e., source to distortion ratios), we also report extended short term objective intelligibility (ESTOI) scores which better predict intelligibility performance in presence of non-stationary interferers.},
	language = {en},
	urldate = {2019-02-21},
	booktitle = {2017 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
	publisher = {IEEE},
	author = {Naithani, Gaurav and Barker, Tom and Parascandolo, Giambattista and Bramslow, Lars and Pontoppidan, Niels Henrik and Virtanen, Tuomas},
	month = oct,
	year = {2017},
	pages = {71--75},
	file = {Naithani et al. - 2017 - Low latency sound source separation using convolut.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9XNPHX8X\\Naithani et al. - 2017 - Low latency sound source separation using convolut.pdf:application/pdf},
}

@article{adavanne_direction_2017,
	title = {Direction of arrival estimation for multiple sound sources using convolutional recurrent neural network},
	url = {http://arxiv.org/abs/1710.10059},
	abstract = {This paper proposes a deep neural network for estimating the directions of arrival (DOA) of multiple sound sources. The proposed stacked convolutional and recurrent neural network (DOAnet) generates a spatial pseudo-spectrum (SPS) along with the DOA estimates in both azimuth and elevation. We avoid any explicit feature extraction step by using the magnitudes and phases of the spectrograms of all the channels as input to the network. The proposed DOAnet is evaluated by estimating the DOAs of multiple concurrently present sources in anechoic, matched and unmatched reverberant conditions. The results show that the proposed DOAnet is capable of estimating the number of sources and their respective DOAs with good precision and generate SPS with high signal-to-noise ratio.},
	language = {en},
	urldate = {2019-02-21},
	journal = {arXiv:1710.10059 [cs, eess]},
	author = {Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.10059},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Adavanne et al. - 2017 - Direction of arrival estimation for multiple sound.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2IATKVNF\\Adavanne et al. - 2017 - Direction of arrival estimation for multiple sound.pdf:application/pdf},
}

@inproceedings{nikunen_multichannel_2014,
	address = {Florence, Italy},
	title = {Multichannel audio separation by direction of arrival based spatial covariance model and non-negative matrix factorization},
	isbn = {978-1-4799-2893-4},
	url = {http://ieeexplore.ieee.org/document/6854892/},
	doi = {10.1109/ICASSP.2014.6854892},
	abstract = {This paper studies multichannel audio separation using non-negative matrix factorization (NMF) combined with a new model for spatial covariance matrices (SCM). The proposed model for SCMs is parameterized by source direction of arrival (DoA) and its parameters can be optimized to yield a spatially coherent solution over frequencies thus avoiding permutation ambiguity and spatial aliasing. The model constrains the estimation of SCMs to a set of geometrically possible solutions. Additionally we present a method for using a priori DoA information of the sources extracted blindly from the mixture for the initialization of the parameters of the proposed model. The simulations show that the proposed algorithm exceeds the separation quality of existing spatial separation methods.},
	language = {en},
	urldate = {2019-02-21},
	booktitle = {2014 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Nikunen, Joonas and Virtanen, Tuomas},
	month = may,
	year = {2014},
	pages = {6677--6681},
	file = {Nikunen et Virtanen - 2014 - Multichannel audio separation by direction of arri.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VP648YI4\\Nikunen et Virtanen - 2014 - Multichannel audio separation by direction of arri.pdf:application/pdf},
}

@article{bai_empirical_2018,
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN.},
	language = {en},
	urldate = {2019-02-22},
	journal = {arXiv:1803.01271 [cs]},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.01271},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GUM2XRVW\\Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf:application/pdf},
}

@inproceedings{zhang_binaural_2017,
	title = {Binaural {Reverberant} {Speech} {Separation} {Based} on {Deep} {Neural} {Networks}},
	url = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/0297.html},
	doi = {10.21437/Interspeech.2017-297},
	abstract = {Supervised learning has exhibited great potential for speech separation in recent years. In this paper, we focus on separating target speech in reverberant conditions from binaural inputs using supervised learning. Specifically, deep neural network (DNN) is constructed to map from both spectral and spatial features to a training target. For spectral features extraction, we first convert binaural inputs into a single signal by applying a fixed beamformer. A new spatial feature is proposed and extracted to complement spectral features. The training target is the recently suggested ideal ratio mask (IRM). Systematic evaluations and comparisons show that the proposed system achieves good separation performance and substantially outperforms existing algorithms under challenging multisource and reverberant environments.},
	language = {en},
	urldate = {2019-02-22},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Zhang, Xueliang and Wang, DeLiang},
	month = aug,
	year = {2017},
	pages = {2018--2022},
	file = {Zhang et Wang - 2017 - Binaural Reverberant Speech Separation Based on De.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZN3J92Z5\\Zhang et Wang - 2017 - Binaural Reverberant Speech Separation Based on De.pdf:application/pdf},
}

@article{zhang_deep_2017,
	title = {Deep {Learning} {Based} {Binaural} {Speech} {Separation} in {Reverberant} {Environments}},
	volume = {25},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/7886357/},
	doi = {10.1109/TASLP.2017.2687104},
	abstract = {Speech signal is usually degraded by room reverberation and additive noises in real environments. This paper focuses on separating target speech signal in reverberant conditions from binaural inputs. Binaural separation is formulated as a supervised learning problem, and we employ deep learning to map from both spatial and spectral features to a training target. With binaural inputs, we ﬁrst apply a ﬁxed beamformer and then extract several spectral features. A new spatial feature is proposed and extracted to complement the spectral features. The training target is the recently suggested ideal ratio mask. Systematic evaluations and comparisons show that the proposed system achieves very good separation performance and substantially outperforms related algorithms under challenging multisource and reverberant environments.},
	language = {en},
	number = {5},
	urldate = {2019-02-22},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Zhang, Xueliang and Wang, DeLiang},
	month = may,
	year = {2017},
	pages = {1075--1084},
	file = {Zhang et Wang - 2017 - Deep Learning Based Binaural Speech Separation in .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Z48HH7C5\\Zhang et Wang - 2017 - Deep Learning Based Binaural Speech Separation in .pdf:application/pdf},
}

@book{reiss_audio_2014,
	edition = {1},
	title = {Audio {Effects}: {Theory}, {Implementation} and {Application}},
	isbn = {978-1-4665-6029-1},
	shorttitle = {Audio {Effects}},
	url = {https://www.taylorfrancis.com/books/9781466560291},
	language = {en},
	urldate = {2019-03-22},
	publisher = {CRC Press},
	author = {Reiss, Joshua D. and McPherson, Andrew P.},
	month = oct,
	year = {2014},
	doi = {10.1201/b17593},
	file = {Reiss - 2014 - Audio Effects Theory, Implementation and Applicat.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HFY59GXS\\Reiss - 2014 - Audio Effects Theory, Implementation and Applicat.pdf:application/pdf},
}

@misc{noauthor_juce_2015,
	title = {Juce {Instruction}},
	publisher = {Aalto University},
	year = {2015},
	file = {juce_instructions.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NUDQ4HQP\\juce_instructions.pdf:application/pdf},
}

@article{warsitz_blind_2007,
	title = {Blind {Acoustic} {Beamforming} {Based} on {Generalized} {Eigenvalue} {Decomposition}},
	volume = {15},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/4244540/},
	doi = {10.1109/TASL.2007.898454},
	abstract = {Maximizing the output signal-to-noise ratio (SNR) of a sensor array in the presence of spatially colored noise leads to a generalized eigenvalue problem. While this approach has extensively been employed in narrowband (antenna) array beamforming, it is typically not used for broadband (microphone) array beamforming due to the uncontrolled amount of speech distortion introduced by a narrowband SNR criterion. In this paper, we show how the distortion of the desired signal can be controlled by a single-channel post-ﬁlter, resulting in a performance comparable to the generalized minimum variance distortionless response beamformer, where arbitrary transfer functions relate the source and the microphones. Results are given both for directional and diffuse noise. A novel gradient ascent adaptation algorithm is presented, and its good convergence properties are experimentally revealed by comparison with alternatives from the literature. A key feature of the proposed beamformer is that it operates blindly, i.e., it neither requires knowledge about the array geometry nor an explicit estimation of the transfer functions from source to sensors or the direction-of-arrival.},
	language = {en},
	number = {5},
	urldate = {2019-04-01},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	author = {Warsitz, Ernst and Haeb-Umbach, Reinhold},
	month = jul,
	year = {2007},
	pages = {1529--1539},
	file = {Warsitz et Haeb-Umbach - 2007 - Blind Acoustic Beamforming Based on Generalized Ei.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\DLL7M56U\\Warsitz et Haeb-Umbach - 2007 - Blind Acoustic Beamforming Based on Generalized Ei.pdf:application/pdf},
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classiﬁcation model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a signiﬁcant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classiﬁcation: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	language = {en},
	urldate = {2019-04-05},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Machine Learning},
	file = {Ioffe et Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EM2F94UX\\Ioffe et Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf},
}

@patent{mesgarani_systems_nodate,
	title = {Systems and {Methods} for {Speech} {Separation} and {Neural} {Decoding} of {Attentional} {Selection} in {Multi}-{Speaker} {Environments}},
	nationality = {USA},
	number = {US 2019 / 0066713 A1},
	author = {Mesgarani, Nima and Luo, Yi and O'Sullivan, James and Chen, Zhuo},
	pages = {69},
	file = {US20190066713A1.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9642PP95\\US20190066713A1.pdf:application/pdf},
}

@article{lagrange_source_nodate,
	title = {Source {Separation} {Methods}  for under-determined sound mixtures},
	language = {en},
	journal = {Blind Source Separation.},
	author = {Lagrange, Mathieu},
	pages = {33},
	file = {Lagrange - Source Separation Methods  for under-determined so.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\INASKV3X\\Lagrange - Source Separation Methods  for under-determined so.pdf:application/pdf},
}

@article{kolbaek_joint_2017,
	title = {Joint {Separation} and {Denoising} of {Noisy} {Multi}-talker {Speech} using {Recurrent} {Neural} {Networks} and {Permutation} {Invariant} {Training}},
	url = {http://arxiv.org/abs/1708.09588},
	abstract = {In this paper we propose to use utterance-level Permutation Invariant Training (uPIT) for speaker independent multi-talker speech separation and denoising, simultaneously. Speciﬁcally, we train deep bi-directional Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) using uPIT, for single-channel speaker independent multi-talker speech separation in multiple noisy conditions, including both synthetic and real-life noise signals. We focus our experiments on generalizability and noise robustness of models that rely on various types of a priori knowledge e.g. in terms of noise type and number of simultaneous speakers.},
	language = {en},
	urldate = {2019-04-10},
	journal = {arXiv:1708.09588 [cs, eess]},
	author = {Kolbæk, Morten and Yu, Dong and Tan, Zheng-Hua and Jensen, Jesper},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.09588},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Kolbæk et al. - 2017 - Joint Separation and Denoising of Noisy Multi-talk.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\DLE3PA7S\\Kolbæk et al. - 2017 - Joint Separation and Denoising of Noisy Multi-talk.pdf:application/pdf},
}

@incollection{vincent_speech_2015,
	address = {Cham},
	title = {Speech {Enhancement} with {LSTM} {Recurrent} {Neural} {Networks} and its {Application} to {Noise}-{Robust} {ASR}},
	volume = {9237},
	isbn = {978-3-319-22481-7 978-3-319-22482-4},
	url = {http://link.springer.com/10.1007/978-3-319-22482-4_11},
	abstract = {We evaluate some recent developments in recurrent neural network (RNN) based speech enhancement in the light of noise-robust automatic speech recognition (ASR). The proposed framework is based on Long Short-Term Memory (LSTM) RNNs which are discriminatively trained according to an optimal speech reconstruction objective. We demonstrate that LSTM speech enhancement, even when used ‘na¨ıvely’ as front-end processing, delivers competitive results on the CHiME-2 speech recognition task. Furthermore, simple, feature-level fusion based extensions to the framework are proposed to improve the integration with the ASR back-end. These yield a best result of 13.76 \% average word error rate, which is, to our knowledge, the best score to date.},
	language = {en},
	urldate = {2019-04-10},
	booktitle = {Latent {Variable} {Analysis} and {Signal} {Separation}},
	publisher = {Springer International Publishing},
	author = {Weninger, Felix and Erdogan, Hakan and Watanabe, Shinji and Vincent, Emmanuel and Le Roux, Jonathan and Hershey, John R. and Schuller, Björn},
	editor = {Vincent, Emmanuel and Yeredor, Arie and Koldovský, Zbyněk and Tichavský, Petr},
	year = {2015},
	doi = {10.1007/978-3-319-22482-4_11},
	pages = {91--99},
	file = {Weninger et al. - 2015 - Speech Enhancement with LSTM Recurrent Neural Netw.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\W2MP25MP\\Weninger et al. - 2015 - Speech Enhancement with LSTM Recurrent Neural Netw.pdf:application/pdf},
}

@inproceedings{erdogan_phase-sensitive_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178061/},
	doi = {10.1109/ICASSP.2015.7178061},
	abstract = {Separation of speech embedded in non-stationary interference is a challenging problem that has recently seen dramatic improvements using deep network-based methods. Previous work has shown that estimating a masking function to be applied to the noisy spectrum is a viable approach that can be improved by using a signalapproximation based objective function. Better modeling of dynamics through deep recurrent networks has also been shown to improve performance. Here we pursue both of these directions. We develop a phase-sensitive objective function based on the signal-to-noise ratio (SNR) of the reconstructed signal, and show that in experiments it yields uniformly better results in terms of signal-to-distortion ratio (SDR). We also investigate improvements to the modeling of dynamics, using bidirectional recurrent networks, as well as by incorporating speech recognition outputs in the form of alignment vectors concatenated with the spectral input features. Both methods yield further improvements, pointing to tighter integration of recognition with separation as a promising future direction.},
	language = {en},
	urldate = {2019-04-10},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Erdogan, Hakan and Hershey, John R. and Watanabe, Shinji and Le Roux, Jonathan},
	month = apr,
	year = {2015},
	pages = {708--712},
	file = {Erdogan et al. - 2015 - Phase-sensitive and recognition-boosted speech sep.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6Z92MEIV\\Erdogan et al. - 2015 - Phase-sensitive and recognition-boosted speech sep.pdf:application/pdf},
}

@incollection{brown_separation_2005,
	address = {Berlin/Heidelberg},
	title = {Separation of {Speech} by {Computational} {Auditory} {Scene} {Analysis}},
	isbn = {978-3-540-24039-6},
	url = {http://link.springer.com/10.1007/3-540-27489-8_16},
	abstract = {The term auditory scene analysis (ASA) refers to the ability of human listeners to form perceptual representations of the constituent sources in an acoustic mixture, as in the well-known ‘cocktail party’ eﬀect. Accordingly, computational auditory scene analysis (CASA) is the ﬁeld of study which attempts to replicate ASA in machines. Some CASA systems are closely modelled on the known stages of auditory processing, whereas others adopt a more functional approach. However, all are broadly based on the principles underlying the perception and organisation of sound by human listeners, and in this respect they diﬀer from ICA and other approaches to sound separation. In this paper, we review the principles underlying ASA and show how they can be implemented in CASA systems. We also consider the link between CASA and automatic speech recognition, and draw distinctions between the CASA and ICA approaches.},
	language = {en},
	urldate = {2019-04-10},
	booktitle = {Speech {Enhancement}},
	publisher = {Springer-Verlag},
	author = {Brown, Guy J. and Wang, DeLiang},
	year = {2005},
	doi = {10.1007/3-540-27489-8_16},
	pages = {371--402},
	file = {Brown et Wang - 2005 - Separation of Speech by Computational Auditory Sce.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\45IMHF7M\\Brown et Wang - 2005 - Separation of Speech by Computational Auditory Sce.pdf:application/pdf},
}

@inproceedings{xia_using_2017,
	address = {Kuala Lumpur},
	title = {Using optimal ratio mask as training target for supervised speech separation},
	isbn = {978-1-5386-1542-3},
	url = {http://ieeexplore.ieee.org/document/8282021/},
	doi = {10.1109/APSIPA.2017.8282021},
	abstract = {Supervised speech separation uses supervised learning algorithms to learn a mapping from an input noisy signal to an output target. With the fast development of deep learning, supervised separation has become the most important direction in speech separation area in recent years. For the supervised algorithm, training target has a significant impact on the performance. Ideal ratio mask is a commonly used training target, which can improve the speech intelligibility and quality of the separated speech. However, it does not take into account the correlation between noise and clean speech. In this paper, we use the optimal ratio mask as the training target of the deep neural network (DNN) for speech separation. The experiments are carried out under various noise environments and signal to noise ratio (SNR) conditions. The results show that the optimal ratio mask outperforms other training targets in general.},
	language = {en},
	urldate = {2019-04-10},
	booktitle = {2017 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	publisher = {IEEE},
	author = {Xia, Shasha and Li, Hao and Zhang, Xueliang},
	month = dec,
	year = {2017},
	pages = {163--166},
	file = {Xia et al. - 2017 - Using optimal ratio mask as training target for su.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BTWB5RI2\\Xia et al. - 2017 - Using optimal ratio mask as training target for su.pdf:application/pdf},
}

@inproceedings{leglaive_semi-supervised_2019,
	address = {Brighton, United Kingdom},
	title = {Semi-supervised multichannel speech enhancement with variational autoencoders and non-negative matrix factorization},
	abstract = {In this paper we address speaker-independent multichannel speech enhancement in unknown noisy environments. Our work is based on a well-established multichannel local Gaussian modeling framework. We propose to use a neural network for modeling the speech spectro-temporal content. The parameters of this supervised model are learned using the framework of variational autoencoders. The noisy recording environment is supposed to be unknown, so the noise spectro-temporal modeling remains unsupervised and is based on non-negative matrix factorization (NMF). We develop a Monte Carlo expectation-maximization algorithm and we experimentally show that the proposed approach outperforms its NMF-based counterpart, where speech is modeled using supervised NMF.},
	language = {en},
	booktitle = {{ICASSP} 2019 - {IEEE} {International} {Conference} on {Acoustics} {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Leglaive, Simon and Girin, Laurent and Horaud, Radu},
	month = may,
	year = {2019},
	pages = {6},
	file = {Leglaive et al. - Semi-supervised multichannel speech enhancement wi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TKQN2EE4\\Leglaive et al. - Semi-supervised multichannel speech enhancement wi.pdf:application/pdf},
}

@inproceedings{leglaive_speech_2019,
	address = {Brighton, United Kingdom},
	title = {Speech enhancement with variational autoencoders and alpha-stable distributions},
	abstract = {This paper focuses on single-channel semi-supervised speech enhancement. We learn a speaker-independent deep generative speech model using the framework of variational autoencoders. The noise model remains unsupervised because we do not assume prior knowledge of the noisy recording environment. In this context, our contribution is to propose a noise model based on alpha-stable distributions, instead of the more conventional Gaussian non-negative matrix factorization approach found in previous studies. We develop a Monte Carlo expectation-maximization algorithm for estimating the model parameters at test time. Experimental results show the superiority of the proposed approach both in terms of perceptual quality and intelligibility of the enhanced speech signal.},
	language = {en},
	booktitle = {{ICASSP} 2019 - {IEEE} {International} {Conference} on {Acoustics} {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Leglaive, Simon and Simsekli, Umut and Liutkus, Antoine and Girin, Laurent and Horaud, Radu},
	month = may,
	year = {2019},
	pages = {6},
	file = {Leglaive et al. - Speech enhancement with variational autoencoders a.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7FT3EP76\\Leglaive et al. - Speech enhancement with variational autoencoders a.pdf:application/pdf},
}

@phdthesis{leglaive_modeles_2017,
	address = {Paris, France},
	title = {Modèles de mélange pour la séparation multicanale de sources sonores en milieu reverberant},
	language = {fr},
	school = {Télécom ParisTech},
	author = {Leglaive, Simon},
	month = dec,
	year = {2017},
	file = {Leglaive - Modèles de mélange pour la séparation multicanale .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CRVBZQAF\\Leglaive - Modèles de mélange pour la séparation multicanale .pdf:application/pdf},
}

@book{loizou_speech_2013,
	edition = {2nd},
	title = {Speech {Enhancement}},
	language = {en},
	publisher = {CRC Press},
	author = {Loizou, Philipos C},
	year = {2013},
	file = {Loizou - Speech Enhancement.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CY5EYU94\\Loizou - Speech Enhancement.pdf:application/pdf},
}

@inproceedings{fontaine_explaining_2017,
	address = {New Paltz, NY},
	title = {Explaining the parameterized wiener filter with alpha-stable processes},
	isbn = {978-1-5386-1632-1},
	url = {http://ieeexplore.ieee.org/document/8169993/},
	doi = {10.1109/WASPAA.2017.8169993},
	abstract = {This paper introduces a new method for single-channel denoising that sheds new light on classical early developments on this topic that occurred in the 70’s and 80’s with Wiener ﬁltering and spectral subtraction. Operating both in the short-time Fourier transform domain, these methods consist in estimating the power spectral density (PSD) of the noise without speech. Then, the clean speech signal is obtained by manipulating the corrupted time-frequency bins thanks to these noise PSD estimates. Theoretically grounded when using power spectra, these methods were subsequently generalized to magnitude spectra, or shown to yield better performance by weighting the PSDs in the so-called parameterized Wiener ﬁlter. Both these strategies were long considered ad-hoc. To the best of our knowledge, while we recently proposed an interpretation of magnitude processing, there is still no theoretical result that would justify the better performance of parameterized Wiener ﬁlters. Here, we show how the α-stable probabilistic model for waveforms naturally leads to these weighted ﬁlters and we provide a grounded and fast algorithm to enhance corrupted audio that compares favorably with classical denoising methods.},
	language = {en},
	urldate = {2019-04-12},
	booktitle = {2017 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
	publisher = {IEEE},
	author = {Fontaine, Mathieu and Liutkus, Antoine and Girin, Laurent and Badeau, Roland},
	month = oct,
	year = {2017},
	pages = {51--55},
	file = {Fontaine et al. - 2017 - Explaining the parameterized wiener filter with al.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HBGUEECA\\Fontaine et al. - 2017 - Explaining the parameterized wiener filter with al.pdf:application/pdf},
}

@inproceedings{yang_deep_2018,
	address = {Bali},
	title = {Deep learning-based speech presence probability estimation for noise {PSD} estimation in single-channel speech enhancement},
	isbn = {978-1-5386-5689-1},
	url = {https://ieeexplore.ieee.org/document/8372770/},
	doi = {10.1109/ICSIGSYS.2018.8372770},
	abstract = {In single-channel speech enhancement, it is essential to determine noise reduction factors to successfully remove noise while minimizing speech distortion. These factors are typically set by a function of noise power spectral density (PSD) in timefrequency domain, and the state-of-the-art algorithm also introduces additional processes to estimate speech presence probability (SPP) to further enhance the estimation. Due to many tuning parameters, however, it is not easy to implement an algorithm that reliably estimates SPP in noise varying environment. We proposed a combination of deep learning network and an effective training method to enhance the performance of the SPP estimation module. The proposed approach is regarded as a hybrid approach, with the noise reduction factor still estimated by the conventional statistic-based single channel enhancement algorithms. The advantages and disadvantages of the proposed approach compared to deep learning approach of single channel speech enhancement are also investigated.},
	language = {en},
	urldate = {2019-04-12},
	booktitle = {2018 {International} {Conference} on {Signals} and {Systems} ({ICSigSys})},
	publisher = {IEEE},
	author = {Yang, Haemin and Choe, Soyeon and Kim, Keulbit and Kang, Hong-Goo},
	month = may,
	year = {2018},
	pages = {267--270},
	file = {Yang et al. - 2018 - Deep learning-based speech presence probability es.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IHDA9FXX\\Yang et al. - 2018 - Deep learning-based speech presence probability es.pdf:application/pdf},
}

@article{ji_robust_2017,
	title = {Robust noise power spectral density estimation for binaural speech enhancement in time-varying diffuse noise field},
	volume = {2017},
	issn = {1687-4722},
	url = {https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-017-0122-4},
	doi = {10.1186/s13636-017-0122-4},
	abstract = {In speech enhancement, noise power spectral density (PSD) estimation plays a key role in determining appropriate de-nosing gains. In this paper, we propose a robust noise PSD estimator for binaural speech enhancement in timevarying noise environments. First, it is shown that the noise PSD can be numerically obtained using an eigenvalue of the input covariance matrix. A simplified estimator is then derived through an approximation process, so that the noise PSD is expressed as a combination of the second eigenvalue of the input covariance matrix, the noise coherence, and the interaural phase difference (IPD) of the input signal. Later, to enhance the accuracy of the noise PSD estimate in time-varying noise environments, an eigenvalue compensation scheme is presented, in which two eigenvalues obtained in noise-dominant regions are combined using a weighting parameter based on the speech presence probability (SPP). Compared with the previous prediction filter-based approach, the proposed method requires neither causality delays nor explicit estimation of the prediction errors. Finally, the proposed noise PSD estimator is applied to a binaural speech enhancement system, and its performance is evaluated through computer simulations. The simulation results show that the proposed noise PSD estimator yields accurate noise PSD regardless of the direction of the target speech signal. Therefore, slightly better performance in quality and intelligibility can be obtained than that with conventional algorithms.},
	language = {en},
	number = {1},
	urldate = {2019-04-12},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Ji, Youna and Baek, Yonghyun and Park, Young-cheol},
	month = dec,
	year = {2017},
	pages = {25},
	file = {Ji et al. - 2017 - Robust noise power spectral density estimation for.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UWN4TY6V\\Ji et al. - 2017 - Robust noise power spectral density estimation for.pdf:application/pdf},
}

@article{plapous_improved_2006,
	title = {Improved {Signal}-to-{Noise} {Ratio} {Estimation} for {Speech} {Enhancement}},
	volume = {14},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/1709898/},
	doi = {10.1109/TASL.2006.872621},
	abstract = {This paper addresses the problem of single microphone speech enhancement in noisy environments. State-ofthe-art short-time noise reduction techniques are most often expressed as a spectral gain depending on the Signal-to-Noise Ratio (SNR). The well-known decision-directed (DD) approach drastically limits the level of musical noise but the estimated a priori SNR is biased since it depends on the speech spectrum estimation in the previous frame. Therefore the gain function matches the previous frame rather than the current one which degrades the noise reduction performance. The consequence of this bias is an annoying reverberation effect. We propose a method called Two-Step Noise Reduction (TSNR) technique which solves this problem while maintaining the beneﬁts of the decision-directed approach. The estimation of the a priori SNR is reﬁned by a second step to remove the bias of the DD approach, thus removing the reverberation effect.},
	language = {en},
	number = {6},
	urldate = {2019-04-12},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	author = {Plapous, C. and Marro, C. and Scalart, P.},
	month = nov,
	year = {2006},
	pages = {2098--2108},
	file = {Plapous et al. - 2006 - Improved Signal-to-Noise Ratio Estimation for Spee.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\R3PWNFVZ\\Plapous et al. - 2006 - Improved Signal-to-Noise Ratio Estimation for Spee.pdf:application/pdf},
}

@article{cohen_speech_2001,
	title = {Speech enhancement for non-stationary noise environments},
	abstract = {In this paper, we present an optimally-modiÿed log-spectral amplitude (OM-LSA) speech estimator and a minima controlled recursive averaging (MCRA) noise estimation approach for robust speech enhancement. The spectral gain function, which minimizes the mean-square error of the log-spectra, is obtained as a weighted geometric mean of the hypothetical gains associated with the speech presence uncertainty. The noise estimate is given by averaging past spectral power values, using a smoothing parameter that is adjusted by the speech presence probability in subbands. We introduce two distinct speech presence probability functions, one for estimating the speech and one for controlling the adaptation of the noise spectrum. The former is based on the time–frequency distribution of the a priori signal-to-noise ratio. The latter is determined by the ratio between the local energy of the noisy signal and its minimum within a speciÿed time window. Objective and subjective evaluation under various environmental conditions conÿrm the superiority of the OM-LSA and MCRA estimators. Excellent noise suppression is achieved, while retaining weak speech components and avoiding the musical residual noise phenomena. ? 2001 Elsevier Science B.V. All rights reserved.},
	language = {en},
	journal = {Signal Processing},
	author = {Cohen, Israel and Berdugo, Baruch},
	year = {2001},
	pages = {16},
	file = {Cohen et Berdugo - 2001 - Speech enhancement for non-stationary noise enviro.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5ELZVMWX\\Cohen et Berdugo - 2001 - Speech enhancement for non-stationary noise enviro.pdf:application/pdf},
}

@inproceedings{aroudi_cognitive-driven_2019,
	address = {Brighton, United Kingdom},
	title = {Cognitive-driven {Binaural} {LCMV} {Beamformer} {Using} {EEG}-based {Auditory} {Attention} {Decoding}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8683635/},
	doi = {10.1109/ICASSP.2019.8683635},
	abstract = {Identifying the target speaker in hearing aid applications is an essential ingredient to improve speech intelligibility. To identify the target speaker from single-trial EEG recordings in an acoustic scenario with two competing speakers, an auditory attention decoding (AAD) method was recently proposed. Aiming at enhancing the target speaker and suppressing the interfering speaker and ambient noise, in this paper we propose a cognitive-driven speech enhancement system, consisting of a direction-of-arrival (DOA) estimator, steerable beamformers and AAD. To preserve the spatial impression of the acoustic scene, which is important when intending to switch attention between speakers, the proposed system only partially suppresses the interfering speaker. The speech enhancement performance of the proposed system is evaluated in terms of the signal-to-interference-plus-noise ratio (SINR) improvement in anechoic and reverberant conditions. The experimental results show that the proposed system can obtain a considerably large SINR improvement (between 3.1 dB and 7.5 dB) in both conditions.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Aroudi, Ali and Doclo, Simon},
	month = may,
	year = {2019},
	pages = {406--410},
	file = {Aroudi et Doclo - 2019 - Cognitive-driven Binaural LCMV Beamformer Using EE.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\K6LPDKQ6\\Aroudi et Doclo - 2019 - Cognitive-driven Binaural LCMV Beamformer Using EE.pdf:application/pdf},
}

@inproceedings{sun_deep_2019,
	address = {Brighton, United Kingdom},
	title = {A {Deep} {Learning} {Based} {Binaural} {Speech} {Enhancement} {Approach} with {Spatial} {Cues} {Preservation}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8683589/},
	doi = {10.1109/ICASSP.2019.8683589},
	abstract = {The studies of binaural hearing indicated considerable beneﬁts of the spatial information of sound sources in speech understanding in noise. In this paper, we propose a binaural speech enhancement approach based on deep neural network. In this approach, the signals at the left and right channels are regarded as the real and imaginary parts of a monaural complex signal, a complex ideal ratio mask is accordingly introduced and then further estimated using the complex deep neural network, followed by applying to the monaural complex signal. Experimental results showed that the suggested binaural speech enhancement approach is able to effectively suppress multiple interfering signals and preserve the binaural cues of target signal.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Sun, Xingwei and Xia, Risheng and Li, Junfeng and Yan, Yonghong},
	month = may,
	year = {2019},
	pages = {5766--5770},
	file = {Sun et al. - 2019 - A Deep Learning Based Binaural Speech Enhancement .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YZDBAVU8\\Sun et al. - 2019 - A Deep Learning Based Binaural Speech Enhancement .pdf:application/pdf},
}

@inproceedings{koutrouvelis_novel_2019,
	address = {Brighton, United Kingdom},
	title = {A {Novel} {Binaural} {Beamforming} {Scheme} with {Low} {Complexity} {Minimizing} {Binaural}-cue {Distortions}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8682886/},
	doi = {10.1109/ICASSP.2019.8682886},
	abstract = {While the majority of binaural beamformers aim to minimize the output noise power while (approximately) preserving the binaural cues of the sources using constraints, we propose in this paper to minimize the binaural-cue distortions of the sources in the acoustic scene, such that the output noise power is below a predeﬁned threshold. This new problem formulation is a convex QCQP problem, which leads to an efﬁcient trade-off between noise reduction, binaural-cue preservation and complexity. In particular, the proposed beamformer provides a better trade-off between noise reduction and binaural-cue preservation (in terms of interaural level and phase differences) compared to the well-known binaural minimum variance distortionless response-η beamformer.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Koutrouvelis, Andreas I. and Hendriks, Richard C. and Heusdens, Richard and Jensen, Jesper and Guo, Meng},
	month = may,
	year = {2019},
	pages = {8013--8017},
	file = {Koutrouvelis et al. - 2019 - A Novel Binaural Beamforming Scheme with Low Compl.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\FMLEMZ5F\\Koutrouvelis et al. - 2019 - A Novel Binaural Beamforming Scheme with Low Compl.pdf:application/pdf},
}

@inproceedings{postma_acoustics_2016,
	address = {Buenos Aires, Argentina},
	title = {Acoustics of {Notre}-{Dame} cathedral de {Paris}},
	booktitle = {{ICA}},
	author = {Postma, B. N. J. and Katz, Brian F. G.},
	year = {2016},
	file = {ICA2016-0269.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\R6R45FLJ\\ICA2016-0269.pdf:application/pdf;ISPACS_DOA.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\B99KD2C4\\ISPACS_DOA.pdf:application/pdf},
}

@book{brown_exponential_1956,
	address = {Cambridge, Massachusetts},
	title = {Exponential {Smoothing} for {Predicting} {Demand}},
	url = {https://books.google.fr/books?id=Eo_rMgEACAAJ},
	publisher = {Arthur D. Little Inc.},
	author = {Brown, Robert G.},
	year = {1956},
	file = {Brown - 1956 - Exponential Smoothing for Predicting Demand.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YTJY75G3\\Brown - 1956 - Exponential Smoothing for Predicting Demand.pdf:application/pdf},
}

@inproceedings{llave_interaction_2019,
	title = {On the {Interaction} {Between} a {Noise} {Localization} {Cues} {Preservation} {Beamformer} and {Dynamic} {Range} {Compression} in {Hearing} {Aids}},
	abstract = {Beamforming and Dynamic Range Compression (DRC) are commonly used in the hearing aids respectively to clean the input signal and to compensate the listener hearing loss. Both processings are known to distort the localizations cues (LC) such as the localization and speech reception performances decrease in complex environment. For a decade, noise localization cues perservation beamformers are developed in order to resolve this issue. However, the best way to combine such algorithms with the DRC stage remains unclear. In this study, we focus on the multichannel Wiener ﬁlter with partial noise estimation (MWFN) and the best way to combine it with a DRC. We show that combining the MWF-N and the DRC is not straightforward without risking to distort the noise LC or making the listening level hazardous for the listener. Finally, we propose a design including a nullformer and independant DRC over the target and the noise signals avoiding the comodulation issue and ensuring the preservation of the noise LC over a large dynamic range.},
	language = {en},
	booktitle = {Interspeech {Preprint}},
	author = {Llave, Adrien and Séguier, Renaud},
	year = {2019},
	pages = {5},
	file = {Llave et Seguier - On the Interaction Between a Noise Localization Cu.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ITKUIZFV\\Llave et Seguier - On the Interaction Between a Noise Localization Cu.pdf:application/pdf},
}

@article{stone_quantifying_2007,
	title = {Quantifying the effects of fast-acting compression on the envelope of speech},
	volume = {121},
	number = {3},
	journal = {The Journal of the Acoustical Society of America},
	author = {Stone, Michael A. and Moore, Brian C. J.},
	month = mar,
	year = {2007},
	pages = {10},
	file = {stone2007.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YB3E6FFK\\stone2007.pdf:application/pdf},
}

@article{malek_block-online_2019,
	title = {Block-{Online} {Multi}-{Channel} {Speech} {Enhancement} {Using} {DNN}-{Supported} {Relative} {Transfer} {Function} {Estimates}},
	url = {http://arxiv.org/abs/1905.03632},
	abstract = {This paper addresses the problem of block-online processing for multi-channel speech enhancement. We consider several variants of a system that performs beamforming supported by DNN-based Voice Activity Detection followed by postﬁltering. The speaker is targeted through estimating relative transfer functions between microphones. Each block of the input signals is processed independently in order to make the method applicable in highly dynamic environments. The performance loss caused by the short length of the processing block is studied and compared with results achieved when recordings are processed as one block (batch processing).},
	language = {en},
	urldate = {2019-05-16},
	journal = {arXiv:1905.03632 [cs, eess]},
	author = {Malek, Jiri and Koldovsky, Zbynek and Bohac, Marek},
	month = may,
	year = {2019},
	note = {arXiv: 1905.03632},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Systems and Control},
	file = {Malek et al. - 2019 - Block-Online Multi-Channel Speech Enhancement Usin.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\I24QCTDY\\Malek et al. - 2019 - Block-Online Multi-Channel Speech Enhancement Usin.pdf:application/pdf},
}

@article{cubick_listening_2018,
	title = {Listening through hearing aids affects spatial perception and speech intelligibility in normal-hearing listeners},
	volume = {144},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.5078582},
	doi = {10.1121/1.5078582},
	language = {en},
	number = {5},
	urldate = {2019-05-16},
	journal = {The Journal of the Acoustical Society of America},
	author = {Cubick, Jens and Buchholz, Jörg M. and Best, Virginia and Lavandier, Mathieu and Dau, Torsten},
	month = nov,
	year = {2018},
	pages = {2896--2905},
	file = {Cubick et al. - 2018 - Listening through hearing aids affects spatial per.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\W9DFFCGM\\Cubick et al. - 2018 - Listening through hearing aids affects spatial per.pdf:application/pdf},
}

@inproceedings{ngamkham_analog_2010,
	address = {Paris, France},
	title = {Analog complex gammatone filter for cochlear implant channels},
	isbn = {978-1-4244-5308-5},
	url = {http://ieeexplore.ieee.org/document/5537383/},
	doi = {10.1109/ISCAS.2010.5537383},
	abstract = {According to recent physiological experiments, the envelope and phase of speech signals are required to enhance the perceptive capability of a cochlear implant processor. In this paper, the design of an analog complex gammatone filter is introduced in order to extract both envelope and phase information of the incoming speech signals as well as to emulate the basilar membrane spectral selectivity. The gammatone impulse response is first transformed into the frequency domain and the resulting 8th-order transfer function is subsequently mapped onto a state-space description of an orthonormal ladder filter. Using this approach, the real and imaginary transfer functions that share the same denominator can be extracted using two different C matrices. This results in a compact filter structure. The proposed filter is designed using Gm-C integrators and sub-threshold CMOS devices in AMIS 0.35μm technology. Simulation results using Cadence RF Spectre confirm the design principle and ultra low power operation.},
	language = {en},
	urldate = {2019-05-16},
	booktitle = {Proceedings of 2010 {IEEE} {International} {Symposium} on {Circuits} and {Systems}},
	publisher = {IEEE},
	author = {Ngamkham, Wannay and Sawigun, Chutham and Hiseni, Senad and Serdijn, Wouter A.},
	month = may,
	year = {2010},
	pages = {969--972},
	file = {Ngamkham et al. - 2010 - Analog complex gammatone filter for cochlear impla.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UYTILVWG\\Ngamkham et al. - 2010 - Analog complex gammatone filter for cochlear impla.pdf:application/pdf},
}

@inproceedings{algazi_cipic_2001,
	address = {New Platz, NY, USA},
	title = {The {CIPIC} {HRTF} database},
	isbn = {978-0-7803-7126-2},
	url = {http://ieeexplore.ieee.org/document/969552/},
	doi = {10.1109/ASPAA.2001.969552},
	language = {en},
	urldate = {2019-05-16},
	booktitle = {Proceedings of the 2001 {IEEE} {Workshop} on the {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({Cat}. {No}.{01TH8575})},
	publisher = {IEEE},
	author = {Algazi, V.R. and Duda, R.O. and Thompson, D.M. and Avendano, C.},
	year = {2001},
	pages = {99--102},
	file = {Algazi et al. - 2001 - The CIPIC HRTF database.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5MLS5XAQ\\Algazi et al. - 2001 - The CIPIC HRTF database.pdf:application/pdf},
}

@article{stone_effect_2003,
	title = {Effect of the speed of a single-channel dynamic range compressor on intelligibility in a competing speech task},
	volume = {114},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1592160},
	doi = {10.1121/1.1592160},
	language = {en},
	number = {2},
	urldate = {2019-05-17},
	journal = {The Journal of the Acoustical Society of America},
	author = {Stone, Michael A. and Moore, Brian C. J.},
	month = aug,
	year = {2003},
	pages = {1023--1034},
	file = {Stone et Moore - 2003 - Effect of the speed of a single-channel dynamic ra.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BQIS6TBL\\Stone et Moore - 2003 - Effect of the speed of a single-channel dynamic ra.pdf:application/pdf},
}

@article{luo_conv-tasnet:_2019,
	title = {Conv-{TasNet}: {Surpassing} {Ideal} {Time}-{Frequency} {Magnitude} {Masking} for {Speech} {Separation}},
	issn = {2329-9290, 2329-9304},
	shorttitle = {Conv-{TasNet}},
	url = {http://arxiv.org/abs/1809.07454},
	doi = {10.1109/TASLP.2019.2915167},
	abstract = {Robust speech processing in multitalker acoustic environments requires automatic speech separation. While singlechannel, speaker-independent speech separation methods have recently seen great progress, the accuracy, latency, and computational cost of speech separation remain insufﬁcient. The majority of the previous methods have formulated the separation problem through the time-frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of spectrogram representations for speech separation, and the long latency in calculating the spectrogram. To address these shortcomings, we propose the time-domain audio separation network (TasNet), which is a deep learning autoencoder framework for time-domain speech separation. TasNet uses a convolutional encoder to create a representation of the signal that is optimized for extracting individual speakers. Speaker extraction is achieved by applying a weighting function (mask) to the encoder output. The modiﬁed encoder representation is then inverted to the sound waveform using a linear decoder. The masks are found using a temporal convolutional network consisting of dilated convolutions, which allow the network to model the long-term dependencies of the speech signal. This end-to-end speech separation algorithm signiﬁcantly outperforms previous time-frequency methods in terms of separating speakers in mixed audio, even when compared to the separation accuracy achieved with the ideal time-frequency mask of the speakers. In addition, TasNet has a smaller model size and a shorter minimum latency, making it a suitable solution for both ofﬂine and real-time speech separation applications. This study therefore represents a major step toward actualizing speech separation for real-world speech processing technologies.},
	language = {en},
	urldate = {2019-05-29},
	journal = {arXiv},
	author = {Luo, Yi and Mesgarani, Nima},
	year = {2019},
	note = {arXiv: 1809.07454},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	pages = {1--1},
	file = {1809.07454.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QR3LBVSN\\1809.07454.pdf:application/pdf},
}

@article{pavlidi_real-time_2013,
	title = {Real-{Time} {Multiple} {Sound} {Source} {Localization} and {Counting} {Using} a {Circular} {Microphone} {Array}},
	volume = {21},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/6557035/},
	doi = {10.1109/TASL.2013.2272524},
	abstract = {In this work, a multiple sound source localization and counting method is presented, that imposes relaxed sparsity constraints on the source signals. A uniform circular microphone array is used to overcome the ambiguities of linear arrays, however the underlying concepts (sparse component analysis and matching pursuit-based operation on the histogram of estimates) are applicable to any microphone array topology. Our method is based on detecting time-frequency (TF) zones where one source is dominant over the others. Using appropriately selected TF components in these “single-source” zones, the proposed method jointly estimates the number of active sources and their corresponding directions of arrival (DOAs) by applying a matching pursuit-based approach to the histogram of DOA estimates. The method is shown to have excellent performance for DOA estimation and source counting, and to be highly suitable for real-time applications due to its low complexity. Through simulations (in various signal-to-noise ratio conditions and reverberant environments) and real environment experiments, we indicate that our method outperforms other state-of-the-art DOA and source counting methods in terms of accuracy, while being signiﬁcantly more efﬁcient in terms of computational complexity.},
	language = {en},
	number = {10},
	urldate = {2019-06-05},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Pavlidi, Despoina and Griffin, Anthony and Puigt, Matthieu and Mouchtaris, Athanasios},
	month = oct,
	year = {2013},
	pages = {2193--2206},
	file = {Pavlidi et al. - 2013 - Real-Time Multiple Sound Source Localization and C.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\8C536BYT\\Pavlidi et al. - 2013 - Real-Time Multiple Sound Source Localization and C.pdf:application/pdf},
}

@inproceedings{braun_narrowband_2015,
	address = {New Paltz, NY, USA},
	title = {Narrowband direction-of-arrival estimation for binaural hearing aids using relative transfer functions},
	isbn = {978-1-4799-7450-4},
	url = {http://ieeexplore.ieee.org/document/7336917/},
	doi = {10.1109/WASPAA.2015.7336917},
	abstract = {State-of-the-art hearing aids feature multiple microphones per hearing aid and employ spatial ﬁltering algorithms that are in general superior to single-channel algorithms. Spatial ﬁltering algorithms require usually the direction-of-arrival (DOA) of the desired sound source. Recently developed informed spatial ﬁlters are based on a parametric signal model, and require an estimate of the DOA per time and frequency. In this paper, we develop a novel DOA estimation algorithm for hearing aids that estimates the narrowband DOA by comparing estimated relative transfer functions (RTFs) with anechoic RTFs per direction from a database. Furthermore, we propose a weighting based on the coherent-to-diffuse ratio to increase the robustness against estimation errors. The algorithm is compared to two existing narrowband DOA estimation techniques. One of the existing techniques is extended to use an arbitrary number of microphones to be able to detect DOAs in the full plane around the listener. Simulations show that the accuracy of the proposed DOA estimator is higher than the existing estimators in diffuse noise ﬁelds. Finally, we demonstrate that the proposed DOA estimator makes it possible to apply an informed spatial ﬁltering technique to binaural hearing aids.},
	language = {en},
	urldate = {2019-06-06},
	booktitle = {2015 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
	publisher = {IEEE},
	author = {Braun, Sebastian and Zhou, Wei and Habets, Emanuel A. P.},
	month = oct,
	year = {2015},
	pages = {1--5},
	file = {Braun et al. - 2015 - Narrowband direction-of-arrival estimation for bin.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MTVM2HU6\\Braun et al. - 2015 - Narrowband direction-of-arrival estimation for bin.pdf:application/pdf},
}

@inproceedings{boyd_improved_2013,
	address = {Montréal},
	title = {Improved estimation of direction of arrival of sound sources for hearing aids using gyroscopic information},
	volume = {19},
	booktitle = {Proceedings of {Meetings} on {Acoustics}},
	publisher = {Acoustical Society of America},
	author = {Boyd, Alan W. and Whitmer, William M. and Brimijoin, W. Owen and Akeroyd, Michael A.},
	month = jun,
	year = {2013},
	pages = {1--9},
	file = {boyd2013.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VCVPM5ZB\\boyd2013.pdf:application/pdf},
}

@inproceedings{goetze_direction_2007,
	address = {Xiamen, China},
	title = {Direction of arrival estimation based on the dual delay line approach for binaural hearing aid microphone arrays},
	isbn = {978-1-4244-1446-8},
	url = {http://ieeexplore.ieee.org/document/4445829/},
	doi = {10.1109/ISPACS.2007.4445829},
	abstract = {Multi-channel beamformer algorithms are promising solutions for noise reduction in hearing aids as they exploit the spatial distribution of the interfering signals and therefore in general lead to less signal distortion than single channel algorithms. Beamformers need a priori information about the microphone array and the direction of arrival of the target speech source. For head-worn arrays it is usually assumed that the user physically steers the arrays’ look direction toward the desired speech source. This may become unsatisfying for the hearing aid user for high directivity beamformers with a small main lobe and when the target signal source is moving. In this contribution an automatic steering (electronic control of the look direction) is applied based on the dual delay line approach after Liu et al. [1]. This approach is modiﬁed to be applicable for head-mounted hearing-aid arrays. We show that the original free-ﬁeld approach does not work on a head-mounted array because of the inappropriate propagation model. If we apply the true HRTF or a spherical head propagation model, the estimate is reliable within ±8◦ degree mean estimation error for an input SNR of 10dB or higher. However, for lower SNR the method seems to be not robust enough.},
	language = {en},
	urldate = {2019-06-06},
	booktitle = {2007 {International} {Symposium} on {Intelligent} {Signal} {Processing} and {Communication} {Systems}},
	publisher = {IEEE},
	author = {Goetze, Stefan and Rohdenburg, Thomas and Hohmann, Volker and Kollmeier, Birger and Kammeyer, Karl-Dirk},
	year = {2007},
	pages = {84--87},
	file = {Goetze et al. - 2007 - Direction of arrival estimation based on the dual .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5RXNEKTF\\Goetze et al. - 2007 - Direction of arrival estimation based on the dual .pdf:application/pdf},
}

@inproceedings{argentieri_broadband_2007,
	address = {San Diego, CA, USA},
	title = {Broadband variations of the {MUSIC} high-resolution method for {Sound} {Source} {Localization} in {Robotics}},
	isbn = {978-1-4244-0911-2 978-1-4244-0912-9},
	url = {http://ieeexplore.ieee.org/document/4399422/},
	doi = {10.1109/IROS.2007.4399422},
	abstract = {The MUSIC algorithm (MUltiple SIgnal Classiﬁcation) is a well-known high-resolution method to sound source localization. However, as it is essentially narrowband, several extensions can be envisaged when dealing with broadband sources like human voice. This paper presents such extensions and proposes a comparative study w.r.t. speciﬁc robotics constraints. An online beamspace MUSIC method, together with a recently developed beamforming scheme, are shown to constitute a mathematically sound and potentially efﬁcient solution.},
	language = {en},
	urldate = {2019-06-07},
	booktitle = {2007 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	publisher = {IEEE},
	author = {Argentieri, Sylvain and Danes, Patrick},
	month = oct,
	year = {2007},
	pages = {2009--2014},
	file = {Argentieri et Danes - 2007 - Broadband variations of the MUSIC high-resolution .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\SFUZEPAQ\\Argentieri et Danes - 2007 - Broadband variations of the MUSIC high-resolution .pdf:application/pdf},
}

@article{knapp_generalized_1976,
	title = {The {Generalized} {Correlation} {Method} for {Estimation} of {Time} {Delay}},
	volume = {24},
	language = {en},
	number = {4},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Knapp, Charles H. and Carter, G. Clifford},
	month = aug,
	year = {1976},
	pages = {8},
	file = {Butler - SIGNAPLROCESSING, VOL. ASSP-24, NO. 4, AUGUST 1976.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BBYCLK92\\Butler - SIGNAPLROCESSING, VOL. ASSP-24, NO. 4, AUGUST 1976.pdf:application/pdf},
}

@inproceedings{marquardt_noise_2017,
	address = {New Paltz, NY},
	title = {Noise power spectral density estimation for binaural noise reduction exploiting direction of arrival estimates},
	isbn = {978-1-5386-1632-1},
	url = {http://ieeexplore.ieee.org/document/8170030/},
	doi = {10.1109/WASPAA.2017.8170030},
	abstract = {Noise reduction algorithms for head-mounted assistive listening devices are crucial to improve speech quality and intelligibility in background noise. For binaural hearing devices with one microphone per device, the noise power spectral density (PSD) is commonly estimated using various assumptions about the acoustic scenario. Since these methods lack robustness if the underlying assumptions are not satisﬁed, alternatively the noise PSD can be estimated at the output of a blocking matrix, however requiring an estimate of the relative transfer function (RTF) or direction of arrival (DOA) of the desired speech source. For constructing the blocking matrix, in this paper we exploit RTF estimates using the covariance whitening method and DOA estimates obtained from a binaural DOA estimator using anechoic prototype acoustic transfer functions (ATFs). Simulation results in a realistic cafeteria scenario show that exploiting DOA estimates for binaural noise PSD estimation leads to an improved noise reduction performance, especially in the presence of directional interfering speakers.},
	language = {en},
	urldate = {2019-06-07},
	booktitle = {2017 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
	publisher = {IEEE},
	author = {Marquardt, Daniel and Doclo, Simon},
	month = oct,
	year = {2017},
	pages = {234--238},
	file = {Marquardt et Doclo - 2017 - Noise power spectral density estimation for binaur.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CZ3LGY5P\\Marquardt et Doclo - 2017 - Noise power spectral density estimation for binaur.pdf:application/pdf},
}

@inproceedings{llave_influence_2019,
	address = {Lille, France},
	title = {Influence sur les indices de localisation du beamforming et de la compression de dynamique dans les audioprothèses},
	booktitle = {{XXVIIème} colloque {GRETSI} ({GRETSI} 2019)},
	author = {Llave, Adrien and Séguier, Renaud},
	month = aug,
	year = {2019},
}

@article{kortlang_evaluation_2017,
	title = {Evaluation of combined dynamic compression and single channel noise reduction for hearing aid applications},
	volume = {57},
	issn = {1499-2027, 1708-8186},
	url = {https://www.tandfonline.com/doi/full/10.1080/14992027.2017.1300695},
	doi = {10.1080/14992027.2017.1300695},
	abstract = {Objective: Single-channel noise reduction (SCNR) and dynamic range compression (DRC) are important elements in hearing aids. Only relatively few studies have addressed interaction effects and typically used real hearing aids with limited knowledge about the integrated algorithms. Here the potential benefit of different combinations and integration of SCNR and DRC was systematically assessed. Design: Ten different systems combining SCNR and DRC were implemented, including five serial arrangements, a parallel and two multiplicative approaches. In an instrumental evaluation, signal-to-noise ratio (SNR) improvement and spectral contrast enhancement (SCE) were assessed. Quality ratings at 0 and +6 dB SNR, and speech reception thresholds (SRTs) in noise were measured using stationary and babble noise. Study sample: Thirteen young normal-hearing (NH) listeners and 12 hearing-impaired (HI) listeners participated. Results: In line with an increased segmental SNR and spectral contrast compared to a serial concatenation, the parallel approach significantly reduced the perceived noise annoyance for both subject groups. The proposed multiplicative approaches could partly counteract increased speech distortions introduced by DRC and achieved the best overall quality for the HI listeners. Conclusions: For high SNRs well above the individual SRT, the specific combination of SCNR and DRC is perceptually relevant and the integrative approaches were preferred.},
	language = {en},
	number = {sup3},
	urldate = {2019-07-01},
	journal = {International Journal of Audiology},
	author = {Kortlang, Steffen and Chen, Zhangli and Gerkmann, Timo and Kollmeier, Birger and Hohmann, Volker and Ewert, Stephan D.},
	month = feb,
	year = {2017},
	pages = {S43--S54},
	file = {Kortlang et al. - 2018 - Evaluation of combined dynamic compression and sin.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3GWGGCSX\\Kortlang et al. - 2018 - Evaluation of combined dynamic compression and sin.pdf:application/pdf},
}

@article{ngo_incorporating_2009,
	title = {Incorporating the {Conditional} {Speech} {Presence} {Probability} in {Multi}-{Channel} {Wiener} {Filter} {Based} {Noise} {Reduction} in {Hearing} {Aids}},
	volume = {2009},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/2009/930625},
	doi = {10.1155/2009/930625},
	language = {en},
	number = {1},
	urldate = {2019-07-01},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Ngo, Kim and Spriet, Ann and Moonen, Marc and Wouters, Jan and Jensen, Søren Holdt},
	month = dec,
	year = {2009},
	pages = {11},
	file = {Ngo et al. - 2009 - Incorporating the Conditional Speech Presence Prob.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MBM3PPU7\\Ngo et al. - 2009 - Incorporating the Conditional Speech Presence Prob.pdf:application/pdf},
}

@article{liu_efficient_2019,
	title = {Efficient {Representation} of {Head}-{Related} {Transfer} {Functions} {With} {Combination} of {Spherical} {Harmonics} and {Spherical} {Wavelets}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8732317/},
	doi = {10.1109/ACCESS.2019.2921388},
	abstract = {Recently, a modeling method for head-related transfer functions (HRTFs) in the spatial domain is proposed based on spherical wavelets. Because spherical wavelets are local functions on the sphere, HRTF local features can be efﬁciently represented by using a small number of analysis functions. This sparse representation method enables to control the spatial resolutions of a desired local region on the sphere with the expansion coefﬁcients. Meanwhile, the conventional HRTF spatial variations models based on spherical harmonic decomposition, which is able to represent the HRTF coarse structure well with lower order models. In this paper, a modeling method of the HRTF spatial features is proposed by combining the spherical harmonics and spherical wavelets, which represents the coarse structure and spatial detail of the HRTFs, respectively. The numerical experiment results show that the proposed joint modeling method has smaller approximation errors within the frequency range of 7 kHz compared with 15 orders spherical harmonics modeling when the same number of parameters is used for representing the HRTF magnitude in all directions. In the comparisons with higher orders spherical harmonics modeling, the proposed joint modeling method yields smaller approximation errors within the frequency range of 20 kHz. Furthermore, the proposed joint modeling method is expected to help interpret the spatial cues in which the low orders spherical harmonic coefﬁcients corresponds to head shadow effects while the spherical wavelet coefﬁcients corresponding to the residual ﬁner details may be interpreted as pinna effects. A tool for the HRTF spatial feature extraction is provided in this paper.},
	language = {en},
	urldate = {2019-07-01},
	journal = {IEEE Access},
	author = {Liu, Huaping and Fang, Yong and Huang, Qinghua},
	year = {2019},
	pages = {78214--78222},
	file = {Liu et al. - 2019 - Efficient Representation of Head-Related Transfer .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CF27ZNZI\\Liu et al. - 2019 - Efficient Representation of Head-Related Transfer .pdf:application/pdf},
}

@article{volker_comparing_2015,
	title = {Comparing {Binaural} {Pre}-processing {Strategies} {III}: {Speech} {Intelligibility} of {Normal}-{Hearing} and {Hearing}-{Impaired} {Listeners}},
	volume = {19},
	issn = {2331-2165, 2331-2165},
	shorttitle = {Comparing {Binaural} {Pre}-processing {Strategies} {III}},
	url = {http://journals.sagepub.com/doi/10.1177/2331216515618609},
	doi = {10.1177/2331216515618609},
	abstract = {A comprehensive evaluation of eight signal pre-processing strategies, including directional microphones, coherence filters, single-channel noise reduction, binaural beamformers, and their combinations, was undertaken with normal-hearing (NH) and hearing-impaired (HI) listeners. Speech reception thresholds (SRTs) were measured in three noise scenarios (multitalker babble, cafeteria noise, and single competing talker). Predictions of three common instrumental measures were compared with the general perceptual benefit caused by the algorithms. The individual SRTs measured without pre-processing and individual benefits were objectively estimated using the binaural speech intelligibility model. Ten listeners with NH and 12 HI listeners participated. The participants varied in age and pure-tone threshold levels. Although HI listeners required a better signal-tonoise ratio to obtain 50\% intelligibility than listeners with NH, no differences in SRT benefit from the different algorithms were found between the two groups. With the exception of single-channel noise reduction, all algorithms showed an improvement in SRT of between 2.1 dB (in cafeteria noise) and 4.8 dB (in single competing talker condition). Model predictions with binaural speech intelligibility model explained 83\% of the measured variance of the individual SRTs in the no pre-processing condition. Regarding the benefit from the algorithms, the instrumental measures were not able to predict the perceptual data in all tested noise conditions. The comparable benefit observed for both groups suggests a possible application of noise reduction schemes for listeners with different hearing status. Although the model can predict the individual SRTs without pre-processing, further development is necessary to predict the benefits obtained from the algorithms at an individual level.},
	language = {en},
	urldate = {2019-07-01},
	journal = {Trends in Hearing},
	author = {Völker, Christoph and Warzybok, Anna and Ernst, Stephan M. A.},
	month = dec,
	year = {2015},
	pages = {18},
	file = {Völker et al. - 2015 - Comparing Binaural Pre-processing Strategies III .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Q39EXNVB\\Völker et al. - 2015 - Comparing Binaural Pre-processing Strategies III .pdf:application/pdf},
}

@article{ngo_combined_2012,
	title = {A combined multi-channel {Wiener} filter-based noise reduction and dynamic range compression in hearing aids},
	volume = {92},
	issn = {01651684},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165168411002635},
	doi = {10.1016/j.sigpro.2011.08.006},
	abstract = {Noise reduction (NR) and dynamic range compression (DRC) are basic components in hearing aids, but generally these components are developed and evaluated independently of each other. Hearing aids typically use a serial concatenation of NR and DRC. However, the DRC in such a concatenation negatively affects the performance of the NR stage: the residual noise after NR receives more ampliﬁcation compared to the speech, resulting in a signal-to-noise-ratio (SNR) degradation. The integration of NR and DRC has not received a lot of attention so far. In this paper, a multi-channel Wiener ﬁlter (MWF) based approach is presented for speech and noise scenarios, where an MWF based NR algorithm is combined with DRC. The proposed solution is based on modifying the MWF and the DRC to incorporate the conditional speech presence probability in order to avoid residual noise ampliﬁcation. The goal is then to analyse any undesired interaction effects by means of objective quality measures. Experimental results indeed conﬁrm that a serial concatenation of NR and DRC degrades the SNR improvement provided by the NR, whereas the combined approach proposed here shows less degradation of the SNR improvement at a low increase in distortion compared to a serial concatenation.},
	language = {en},
	number = {2},
	urldate = {2019-07-01},
	journal = {Signal Processing},
	author = {Ngo, Kim and Spriet, Ann and Moonen, Marc and Wouters, Jan and Holdt Jensen, Søren},
	month = feb,
	year = {2012},
	pages = {417--426},
	file = {Ngo et al. - 2012 - A combined multi-channel Wiener filter-based noise.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6UI7ENF2\\Ngo et al. - 2012 - A combined multi-channel Wiener filter-based noise.pdf:application/pdf},
}

@phdthesis{ngo_digital_2011,
	address = {Leuven},
	title = {Digital signal processing algorithms for noise reduction, dynamic range compression, and feedback cancellation in hearing aids},
	language = {en},
	school = {Katholieke Universiteit Leuven},
	author = {Ngo, Kim},
	year = {2011},
	file = {Ngo - Digital signal processing algorithms for noise red.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7CZ99P54\\Ngo - Digital signal processing algorithms for noise red.pdf:application/pdf},
}

@article{cohen_noise_2003,
	title = {Noise spectrum estimation in adverse environments: improved minima controlled recursive averaging},
	volume = {11},
	issn = {1063-6676},
	shorttitle = {Noise spectrum estimation in adverse environments},
	url = {http://ieeexplore.ieee.org/document/1223596/},
	doi = {10.1109/TSA.2003.811544},
	abstract = {Noise spectrum estimation is a fundamental component of speech enhancement and speech recognition systems. In this paper, we present an improved minima controlled recursive averaging (IMCRA) approach, for noise estimation in adverse environments involving nonstationary noise, weak speech components, and low input signal-to-noise ratio (SNR). The noise estimate is obtained by averaging past spectral power values, using a time-varying frequency-dependent smoothing parameter that is adjusted by the signal presence probability. The speech presence probability is controlled by the minima values of a smoothed periodogram. The proposed procedure comprises two iterations of smoothing and minimum tracking. The first iteration provides a rough voice activity detection in each frequency band. Then, smoothing in the second iteration excludes relatively strong speech components, which makes the minimum tracking during speech activity robust. We show that in nonstationary noise environments and under low SNR conditions, the IMCRA approach is very effective. In particular, compared to a competitive method, it obtains a lower estimation error, and when integrated into a speech enhancement system achieves improved speech quality and lower residual noise.},
	language = {en},
	number = {5},
	urldate = {2019-09-03},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Cohen, I.},
	month = sep,
	year = {2003},
	pages = {466--475},
	file = {Cohen - 2003 - Noise spectrum estimation in adverse environments.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VLHNXVN3\\Cohen - 2003 - Noise spectrum estimation in adverse environments.pdf:application/pdf},
}

@article{cohen_optimal_2002,
	title = {Optimal speech enhancement under signal presence uncertainty using log-spectral amplitude estimator},
	volume = {9},
	issn = {1070-9908, 1558-2361},
	url = {http://ieeexplore.ieee.org/document/1001645/},
	doi = {10.1109/97.1001645},
	abstract = {In this paper, we present an optimally modified logspectral amplitude estimator, which minimizes the mean-square error of the log-spectra for speech signals under signal presence uncertainty. We propose an estimator for the a priori signal-tonoise ratio (SNR), and introduce an efficient estimator for the a priori speech absence probability. Speech presence probability is estimated for each frequency bin and each frame by a soft-decision approach, which exploits the strong correlation of speech presence in neighboring frequency bins of consecutive frames. Objective and subjective evaluation confirm superiority in noise suppression and quality of the enhanced speech.},
	language = {en},
	number = {4},
	urldate = {2019-09-03},
	journal = {IEEE Signal Processing Letters},
	author = {Cohen, I.},
	month = apr,
	year = {2002},
	pages = {113--116},
	file = {Cohen - 2002 - Optimal speech enhancement under signal presence u.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\S3M6RWRJ\\Cohen - 2002 - Optimal speech enhancement under signal presence u.pdf:application/pdf},
}

@article{schwarz_coherent--diffuse_2015,
	title = {Coherent-to-{Diffuse} {Power} {Ratio} {Estimation} for {Dereverberation}},
	volume = {23},
	issn = {2329-9290, 2329-9304},
	doi = {10.1109/TASLP.2015.2418571},
	abstract = {The estimation of the time- and frequencydependent coherent-to-diffuse power ratio (CDR) from the measured spatial coherence between two omnidirectional microphones is investigated. Known CDR estimators are formulated in a common framework, illustrated using a geometric interpretation in the complex plane, and investigated with respect to bias and robustness towards model errors. Several novel unbiased CDR estimators are proposed, and it is shown that knowledge of either the direction of arrival (DOA) of the target source or the coherence of the noise ﬁeld is sufﬁcient for unbiased CDR estimation. The validity of the model for the application of CDR estimates to dereverberation is investigated using measured and simulated impulse responses. A CDR-based dereverberation system is presented and evaluated using signal-based quality measures as well as automatic speech recognition accuracy. The results show that the proposed unbiased estimators have a practical advantage over existing estimators, and that the proposed DOA-independent estimator can be used for effective blind dereverberation.},
	language = {en},
	number = {6},
	urldate = {2019-09-24},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Schwarz, Andreas and Kellermann, Walter},
	month = jun,
	year = {2015},
	keywords = {Computer Science - Sound},
	pages = {1006--1018},
	file = {Schwarz et Kellermann - 2015 - Coherent-to-Diffuse Power Ratio Estimation for Der.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NKBJUA3Y\\Schwarz et Kellermann - 2015 - Coherent-to-Diffuse Power Ratio Estimation for Der.pdf:application/pdf},
}

@article{zohourian_binaural_2018,
	title = {Binaural {Speaker} {Localization} {Integrated} {Into} an {Adaptive} {Beamformer} for {Hearing} {Aids}},
	volume = {26},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/8186189/},
	doi = {10.1109/TASLP.2017.2782491},
	abstract = {In this paper, we present and compare novel algorithms to localize simultaneous speakers using four microphones distributed on a pair of binaural hearing aids. The framework consists of two groups of localization algorithms, namely, beamforming-based and statistical model based localization algorithms. We ﬁrst generalize our previously proposed methods based on beamforming techniques to the binaural conﬁguration with 2 × 2 microphones. Next, we contribute two statistical model based methods for binaural localization using the maximum likelihood approach that also takes head-related transfer functions and unknown noise conditions into account. The methods enable the localization of multiple source positions for all azimuth angles and do not require prior training of binaural cues. The proposed localization algorithms are integrated into a generalized side-lobe canceller (GSC) to extract the desired speaker in the presence of competing speakers and background noise and when the head of the listener turns. The GSC components are adapted with the frequencywise target presence probability and the frame-wise broadband direction-of-arrival (DOA) estimates that track the turns of the listener’s head. We evaluate the performance of the localization algorithms individually and also in the context of the adaptive binaural beamformer in various noisy and reverberant conditions. Finally, we introduce a new adaptive beamformer, which combines the GSC with multichannel speech presence probability estimation and achieves superior source separation performance in noisy environment.},
	language = {en},
	number = {3},
	urldate = {2019-10-02},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Zohourian, Mehdi and Enzner, Gerald and Martin, Rainer},
	month = mar,
	year = {2018},
	pages = {515--528},
	file = {Zohourian et al. - 2018 - Binaural Speaker Localization Integrated Into an A.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\95TXDYPC\\Zohourian et al. - 2018 - Binaural Speaker Localization Integrated Into an A.pdf:application/pdf},
}

@article{argentieri_survey_2015,
	title = {A survey on sound source localization in robotics: {From} binaural to array processing methods},
	volume = {34},
	issn = {08852308},
	shorttitle = {A survey on sound source localization in robotics},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230815000236},
	doi = {10.1016/j.csl.2015.03.003},
	abstract = {This paper attempts to provide a state-of-the-art of sound source localization in Robotics. Noticeably, this context raises original constraints—e.g. embeddability, real time, broadband environments, noise and reverberation—which are seldom simultaneously taken into account in Acoustics or Signal Processing. A comprehensive review is proposed of recent robotics achievements, be they binaural or rooted in Array Processing techniques. The connections are highlighted with the underlying theory as well as with elements of physiology and neurology of human hearing.},
	language = {en},
	number = {1},
	urldate = {2019-10-21},
	journal = {Computer Speech \& Language},
	author = {Argentieri, Sylvain and Danès, Patrick and Souères, Philippe},
	month = nov,
	year = {2015},
	pages = {87--112},
	file = {Argentieri et al. - 2015 - A survey on sound source localization in robotics.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XIA4963F\\Argentieri et al. - 2015 - A survey on sound source localization in robotics.pdf:application/pdf},
}

@article{rascon_lightweight_2015,
	title = {Lightweight multi-{DOA} tracking of mobile speech sources},
	volume = {2015},
	issn = {1687-4722},
	url = {https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-015-0055-8},
	doi = {10.1186/s13636-015-0055-8},
	abstract = {Estimating the directions of arrival (DOAs) of multiple simultaneous mobile sound sources is an important step for various audio signal processing applications. In this contribution, we present an approach that improves upon our previous work that is now able to estimate the DOAs of multiple mobile speech sources, while being light in resources, both hardware-wise (only using three microphones) and software-wise. This approach takes advantage of the fact that simultaneous speech sources do not completely overlap each other. To evaluate the performance of this approach, a multi-DOA estimation evaluation system was developed based on a corpus collected from different acoustic scenarios named Acoustic Interactions for Robot Audition (AIRA).},
	language = {en},
	number = {1},
	urldate = {2019-10-21},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Rascon, Caleb and Fuentes, Gibran and Meza, Ivan},
	month = dec,
	year = {2015},
	pages = {11},
	file = {Rascon et al. - 2015 - Lightweight multi-DOA tracking of mobile speech so.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2JIJRX65\\Rascon et al. - 2015 - Lightweight multi-DOA tracking of mobile speech so.pdf:application/pdf},
}

@article{stone_tolerable_2003,
	title = {Tolerable {Hearing} {Aid} {Delays}. {III}. {Effects} on {Speech} {Production} and {Perception} of {Across}-{Frequency} {Variation} in {Delay}},
	volume = {24},
	issn = {0196-0202},
	shorttitle = {Tolerable {Hearing} {Aid} {Delays}. {III}. {Effects} on {Speech} {Production} and {Perception} of {Across}-{Frequency} {Variation} in {Delay}},
	url = {https://insights.ovid.com/crossref?an=00003446-200304000-00008},
	doi = {10.1097/01.AUD.0000058106.68049.9C},
	language = {en},
	number = {2},
	urldate = {2019-10-21},
	journal = {Ear and Hearing},
	author = {Stone, Michael A. and Moore, Brian C. J.},
	month = apr,
	year = {2003},
	pages = {175--183},
	file = {Stone et Moore - 2003 - Tolerable Hearing Aid Delays. III. Effects on Spee.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\8TJLWVJR\\Stone et Moore - 2003 - Tolerable Hearing Aid Delays. III. Effects on Spee.pdf:application/pdf},
}

@article{avargel_multiplicative_2007,
	title = {On {Multiplicative} {Transfer} {Function} {Approximation} in the {Short}-{Time} {Fourier} {Transform} {Domain}},
	volume = {14},
	issn = {1070-9908},
	url = {http://ieeexplore.ieee.org/document/4154719/},
	doi = {10.1109/LSP.2006.888292},
	abstract = {The multiplicative transfer function (MTF) approximation is widely used for modeling a linear time invariant system in the short-time Fourier transform (STFT) domain. It relies on the assumption of a long analysis window compared with the length of the system impulse response. In this paper, we investigate the inﬂuence of the analysis window length on the performance of a system identiﬁer that utilizes the MTF approximation. We derive analytic expressions for the minimum mean-square error (MMSE) in the STFT domain and show that the system identiﬁcation performance does not necessarily improve by increasing the length of the analysis window. The optimal window length, that achieves the MMSE, depends on the signal-to-noise ratio and the length of the input signal. The theoretical analysis is supported by simulation results.},
	language = {en},
	number = {5},
	urldate = {2019-10-22},
	journal = {IEEE Signal Processing Letters},
	author = {Avargel, Yekutiel and Cohen, Israel},
	month = may,
	year = {2007},
	pages = {337--340},
	file = {Avargel et Cohen - 2007 - On Multiplicative Transfer Function Approximation .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AQY4MWI6\\Avargel et Cohen - 2007 - On Multiplicative Transfer Function Approximation .pdf:application/pdf},
}

@article{ruksana_reverberation_2018,
	title = {Reverberation effects on source localization and beamforming for hearing aid applications},
	issn = {2226-5147},
	abstract = {Methods for Auditory Scene Analysis (CASA) aim at reproducing the capabilities of the human auditory system with machines based on sensory inputs. These methods aim at achieving human performance with respect to sound source separation, localization and recognition by using microphone recordings of the acoustic scenes. Within the context of hearing aids, it is evident that information of the auditory scene can improve the parametrization of the algorithms and lead to better performance. Commonly, localization methods rely on the computation of binaural cues such as level and time di erences and employ this information to detect the orientation angle of the sound sources with respect to the microphone positions. However, reverberation a ects the signals signi cantly and thus the binaural cues. This study investigates the relationships between direct and reverberant components of the signals with the binaural cues. Moreover, a widely used signal processing algorithm within hearing aids, such as beamforming, will be analyzed. More speci cally, the e ects of beamforming on the reverberant signals and on the binaural cues will be examined. Finally, aspects related to the in uence of the microphone positions (behind-the-ear vs. in-the-ear) on the recordings will be discussed.},
	language = {en},
	journal = {Euronoise},
	author = {Ruksana, Giurda and Eleftheria, Georganti and Norbert, Dillier},
	year = {2018},
	pages = {10},
	file = {Giurda - 2018 - Reverberation effects on source localization and b.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\I2Y87DCE\\Giurda - 2018 - Reverberation effects on source localization and b.pdf:application/pdf},
}

@article{klockgether_just_2016,
	title = {Just noticeable differences of spatial cues in echoic and anechoic acoustical environments},
	volume = {140},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4964844},
	doi = {10.1121/1.4964844},
	abstract = {The perceptual limits for detecting changes in binaural cues also deﬁne the boundaries for the perception of differences in spatial impression. This study reports just noticeable differences for interaural time delays (ITDs) and interaural level differences (ILDs) of the early part and for the interaural cross-correlation (IACC) of the early and diffuse part of binaural room impulse responses. The results show that ITDs only allow a high accuracy in localization in anechoic environments, whereas ILDs show a higher robustness against reverberation. Subjects are rather insensitive to changes in IACC, only changes that bring the IACC close to one are detectable.},
	language = {en},
	number = {4},
	urldate = {2019-11-29},
	journal = {The Journal of the Acoustical Society of America},
	author = {Klockgether, Stefan and van de Par, Steven},
	month = oct,
	year = {2016},
	pages = {EL352--EL357},
	file = {Klockgether et van de Par - 2016 - Just noticeable differences of spatial cues in ech.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IZNPWDS5\\Klockgether et van de Par - 2016 - Just noticeable differences of spatial cues in ech.pdf:application/pdf},
}

@article{middlebrooks_virtual_1999,
	title = {Virtual localization improved by scaling nonindividualized external-ear transfer functions in frequency},
	volume = {106},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.427147},
	doi = {10.1121/1.427147},
	language = {en},
	number = {3},
	urldate = {2019-11-29},
	journal = {The Journal of the Acoustical Society of America},
	author = {Middlebrooks, John C.},
	month = sep,
	year = {1999},
	pages = {1493--1510},
	file = {Middlebrooks - 1999 - Virtual localization improved by scaling nonindivi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GG5JRUW7\\Middlebrooks - 1999 - Virtual localization improved by scaling nonindivi.pdf:application/pdf},
}

@phdthesis{ospina_individualisation_2016,
	title = {Individualisation de l'écoute binaurale: création et transformation des indices spectraux et des morphologies des individus},
	language = {fr},
	author = {Ospina, Felipe Rugeles},
	year = {2016},
	file = {Ospina - Individualisation de l'écoute binaurale création .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZMDQVE5B\\Ospina - Individualisation de l'écoute binaurale création .pdf:application/pdf},
}

@article{middlebrooks_individual_1999,
	title = {Individual differences in external-ear transfer functions reduced by scaling in frequency},
	volume = {106},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.427176},
	doi = {10.1121/1.427176},
	language = {en},
	number = {3},
	urldate = {2019-11-29},
	journal = {The Journal of the Acoustical Society of America},
	author = {Middlebrooks, John C.},
	month = sep,
	year = {1999},
	pages = {1480--1492},
	file = {Middlebrooks - 1999 - Individual differences in external-ear transfer fu.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\G36Y97N2\\Middlebrooks - 1999 - Individual differences in external-ear transfer fu.pdf:application/pdf},
}

@article{denk_limitations_2019,
	title = {On the limitations of sound localization with hearing devices},
	volume = {146},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.5126521},
	doi = {10.1121/1.5126521},
	language = {en},
	number = {3},
	urldate = {2019-11-29},
	journal = {The Journal of the Acoustical Society of America},
	author = {Denk, Florian and Ewert, Stephan D. and Kollmeier, Birger},
	month = sep,
	year = {2019},
	pages = {1732--1744},
	file = {Denk et al. - 2019 - On the limitations of sound localization with hear.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\N8N6RML6\\Denk et al. - 2019 - On the limitations of sound localization with hear.pdf:application/pdf},
}

@article{bomhardt_uence_2017,
	title = {The inﬂuence of symmetrical human ears on the front-back confusion},
	abstract = {Human beings have two ears to localize sound sources. At a ﬁrst glance, the dimensions of the right and left ears are generally very similar. Nevertheless, the individual anthropometric dimensions and shape of both ears are disparate. These differences improve localization on the cone of confusion where interaural differences do not exist. To determine the inﬂuence of asymmetric ears, individual HRTF data sets are analytically and subjectively compared with their mirrored versions.},
	language = {en},
	journal = {AES},
	author = {Bomhardt, Ramona and Fels, Janina},
	year = {2017},
	pages = {9},
	file = {Bomhardt et Fels - 2017 - The inﬂuence of symmetrical human ears on the fron.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HZJHX9EF\\Bomhardt et Fels - 2017 - The inﬂuence of symmetrical human ears on the fron.pdf:application/pdf},
}

@article{romblom_blockhead:_2019,
	title = {Blockhead: {A} {Simple} {Geometric} {Head} {Model}},
	abstract = {This paper introduces a geometric head model referred to as Blockhead. The time offset to the ear is calculated using an extension of the Woodworth formula having a half-circle of radius a and a rectangle of variable height b on polar half-planes originating from the interaural axis. The anthropomorphic head width deﬁnes the Blockhead model width. Time delays associated with the pinna and with multiple propagation paths are accounted for. When optimized to ﬁt the measured high-frequency ITD, the Blockhead model has less than half the error of the spherical head model with offset ears, gives rise to a head-like shape, and offers a simple description of different head shapes ranging from narrow to nearly-spherical.},
	language = {en},
	journal = {AES},
	author = {Romblom, David and Bahu, Hélène},
	year = {2019},
	pages = {10},
	file = {Romblom et Bahu - 2019 - Blockhead A Simple Geometric Head Model.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\G3QYBP7B\\Romblom et Bahu - 2019 - Blockhead A Simple Geometric Head Model.pdf:application/pdf},
}

@phdthesis{perotin_localisation_2019,
	title = {Localisation et rehaussement de sources de parole au format {Ambisonique}},
	language = {fr},
	school = {Université de Lorraine},
	author = {Perotin, Lauréline},
	year = {2019},
	file = {Perotin - Localisation et rehaussement de sources de parole .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\8YVLVKXJ\\Perotin - Localisation et rehaussement de sources de parole .pdf:application/pdf},
}

@article{gannot_consolidated_2017,
	title = {A {Consolidated} {Perspective} on {Multimicrophone} {Speech} {Enhancement} and {Source} {Separation}},
	volume = {25},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/7805139/},
	doi = {10.1109/TASLP.2016.2647702},
	abstract = {Speech enhancement and separation are core problems in audio signal processing, with commercial applications in devices as diverse as mobile phones, conference call systems, hands-free systems, or hearing aids. In addition, they are crucial pre-processing steps for noise-robust automatic speech and speaker recognition. Many devices now have two to eight microphones. The enhancement and separation capabilities offered by these multichannel interfaces are usually greater than those of single-channel interfaces. Research in speech enhancement and separation has followed two convergent paths, starting with microphone array processing and blind source separation, respectively. These communities are now strongly interrelated and routinely borrow ideas from each other. Yet, a comprehensive overview of the common foundations and the differences between these approaches is lacking at present. In this article, we propose to ﬁll this gap by analyzing a large number of established and recent techniques according to four transverse axes: a) the acoustic impulse response model, b) the spatial ﬁlter design criterion, c) the parameter estimation algorithm, and d) optional postﬁltering. We conclude this overview paper by providing a list of software and data resources and by discussing perspectives and future trends in the ﬁeld.},
	language = {en},
	number = {4},
	urldate = {2019-12-12},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Gannot, Sharon and Vincent, Emmanuel and Markovich-Golan, Shmulik and Ozerov, Alexey},
	month = apr,
	year = {2017},
	pages = {692--730},
	file = {Gannot et al. - 2017 - A Consolidated Perspective on Multimicrophone Spee.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LC3DC5RB\\Gannot et al. - 2017 - A Consolidated Perspective on Multimicrophone Spee.pdf:application/pdf},
}

@article{cox_resolving_1973,
	title = {Resolving power and sensitivity to mismatch of optimum array processors},
	volume = {54},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1913659},
	doi = {10.1121/1.1913659},
	language = {en},
	number = {3},
	urldate = {2019-12-12},
	journal = {The Journal of the Acoustical Society of America},
	author = {Cox, Henry},
	month = sep,
	year = {1973},
	pages = {771--785},
	file = {Cox - 1973 - Resolving power and sensitivity to mismatch of opt.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4T3P4DRE\\Cox - 1973 - Resolving power and sensitivity to mismatch of opt.pdf:application/pdf},
}

@article{moore_personalized_2019,
	title = {Personalized signal-independent beamforming for binaural hearing aids},
	volume = {145},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.5102173},
	doi = {10.1121/1.5102173},
	language = {en},
	number = {5},
	urldate = {2019-12-12},
	journal = {The Journal of the Acoustical Society of America},
	author = {Moore, Alastair H. and de Haan, Jan Mark and Pedersen, Michael Syskind and Naylor, Patrick A. and Brookes, Mike and Jensen, Jesper},
	month = may,
	year = {2019},
	pages = {2971--2981},
	file = {Moore et al. - 2019 - Personalized signal-independent beamforming for bi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VJ2Y9DY6\\Moore et al. - 2019 - Personalized signal-independent beamforming for bi.pdf:application/pdf},
}

@inproceedings{rix_perceptual_2001,
	address = {Salt Lake City, UT, USA},
	title = {Perceptual evaluation of speech quality ({PESQ})-a new method for speech quality assessment of telephone networks and codecs},
	volume = {2},
	isbn = {978-0-7803-7041-8},
	url = {http://ieeexplore.ieee.org/document/941023/},
	doi = {10.1109/ICASSP.2001.941023},
	abstract = {Previous objective speech quality assessment models, such as bark spectral distortion (BSD), the perceptual speech quality measure (PSQM), and measuring normalizing blocks (MNB), have been found to be suitable for assessing only a limited range of distortions. A new model has therefore been developed for use across a wider range of network conditions, including analogue connections, codecs, packet loss and variable delay. Known as perceptual evaluation of speech quality (PESQ), it is the result of integration of the perceptual analysis measurement system (PAMS) and PSQM99, an enhanced version of PSQM. PESQ is expected to become a new ITU-T recommendation P.862, replacing P.861 which specified PSQM and MNB.},
	language = {en},
	urldate = {2019-12-18},
	booktitle = {2001 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}. {Proceedings} ({Cat}. {No}.{01CH37221})},
	publisher = {IEEE},
	author = {Rix, A.W. and Beerends, J.G. and Hollier, M.P. and Hekstra, A.P.},
	year = {2001},
	pages = {749--752},
	file = {Rix et al. - 2001 - Perceptual evaluation of speech quality (PESQ)-a n.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IBW4JNUG\\Rix et al. - 2001 - Perceptual evaluation of speech quality (PESQ)-a n.pdf:application/pdf},
}

@inproceedings{huang_deep_2014,
	address = {Florence, Italy},
	title = {Deep learning for monaural speech separation},
	isbn = {978-1-4799-2893-4},
	url = {http://ieeexplore.ieee.org/document/6853860/},
	doi = {10.1109/ICASSP.2014.6853860},
	abstract = {Monaural source separation is useful for many real-world applications though it is a challenging problem. In this paper, we study deep learning for monaural speech separation. We propose the joint optimization of the deep learning models (deep neural networks and recurrent neural networks) with an extra masking layer, which enforces a reconstruction constraint. Moreover, we explore a discriminative training criterion for the neural networks to further enhance the separation performance. We evaluate our approaches using the TIMIT speech corpus for a monaural speech separation task. Our proposed models achieve about 3.8⇠4.9 dB SIR gain compared to NMF models, while maintaining better SDRs and SARs.},
	language = {en},
	urldate = {2019-12-18},
	booktitle = {2014 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Huang, Po-Sen and Kim, Minje and Hasegawa-Johnson, Mark and Smaragdis, Paris},
	month = may,
	year = {2014},
	pages = {1562--1566},
	file = {Huang et al. - 2014 - Deep learning for monaural speech separation.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PPA8RNF8\\Huang et al. - 2014 - Deep learning for monaural speech separation.pdf:application/pdf},
}

@book{naik_blind_2014,
	address = {Berlin},
	series = {Signals and communication technology},
	title = {Blind source separation: advances in theory, algorithms and applications},
	isbn = {978-3-642-55015-7 978-3-642-55016-4},
	shorttitle = {Blind source separation},
	abstract = {Blind Source Separation intends to report the new results of the efforts on the study of Blind Source Separation (BSS). The book collects novel research ideas and some training in BSS, independent component analysis (ICA), artificial intelligence and signal processing applications. Furthermore, the research results previously scattered in many journals and conferences worldwide are methodically edited and presented in a unified form},
	language = {en},
	publisher = {Springer},
	editor = {Naik, Ganesh R. and Wang, Wenwu},
	year = {2014},
	note = {OCLC: 874067007},
	file = {Naik et Wang - 2014 - Blind source separation advances in theory, algor.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\68BRZ3H5\\Naik et Wang - 2014 - Blind source separation advances in theory, algor.pdf:application/pdf},
}

@inproceedings{maazaoui_estimation_2016,
	address = {Paris, France},
	title = {Estimation of individualized {HRTF} in unsupervized conditions},
	abstract = {Head Related Transfer Functions (HRTF) are the key features of binaural sound spatialization. Those ﬁlters are speciﬁc to each individual and generally measured in an anechoic room using a complex process. Although the use of non-individual ﬁlters can cause perceptual artefacts, the generalization of such measurements is hardly accessible for large public. Thus, many authors have proposed alternative individualization methods to prevent from measuring HRTFs. Examples of such methods are based on numerical modelling, adaptation of non-individual HRTFs or selection of non-individual HRTFs from a database. In this article, we propose an individualization method where the best matching set of HRTFs is selected from a database on the basis of an unsupervised binaural recording of the listener in real-life environment.},
	language = {en},
	author = {Maazaoui, Mounira and Warusfel, Olivier},
	month = jun,
	year = {2016},
	pages = {9},
	file = {Maazaoui et Warusfel - 2016 - Estimation of individualized HRTF in unsupervized .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CRFBN4UG\\Maazaoui et Warusfel - 2016 - Estimation of individualized HRTF in unsupervized .pdf:application/pdf},
}

@article{kreuzer_fast_2009,
	title = {Fast multipole boundary element method to calculate head-related transfer functions for a wide frequency range},
	volume = {126},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3177264},
	doi = {10.1121/1.3177264},
	abstract = {Head-related transfer functions (HRTFs) play an important role in spatial sound localization. The boundary element method (BEM) can be applied to calculate HRTFs from non-contact visual scans. Because of high computational complexity, HRTF simulations with BEM for the whole head and pinnae have only been performed for frequencies below 10 kHz. In this study, the fast multipole method (FMM) is coupled with BEM to simulate HRTFs for a wide frequency range. The basic approach of the FMM and its implementation are described. A mesh with over 70 000 elements was used to calculate HRTFs for one subject. With this mesh, the method allowed to calculate HRTFs for frequencies up to 35 kHz. Comparison to acoustically-measured HRTFs has been performed for frequencies up to 16 kHz, showing a good congruence below 7 kHz. Simulations with an additional shoulder mesh improved the congruence in the vertical direction. Reduction in the mesh size by 5\% resulted in a substantially-worse representation of spectral cues. The effects of temperature and mesh perturbation were negligible. The FMM appears to be a promising approach for HRTF simulations. Further limitations and potential advantages of the FMM-coupled BEM are discussed.},
	language = {en},
	number = {3},
	urldate = {2019-12-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kreuzer, Wolfgang and Majdak, Piotr and Chen, Zhengsheng},
	month = sep,
	year = {2009},
	pages = {1280--1290},
	file = {Kreuzer et al. - 2009 - Fast multipole boundary element method to calculat.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RFY5QP2A\\Kreuzer et al. - 2009 - Fast multipole boundary element method to calculat.pdf:application/pdf},
}

@article{andreopoulou_inter-laboratory_2015,
	title = {Inter-{Laboratory} {Round} {Robin} {HRTF} {Measurement} {Comparison}},
	volume = {9},
	doi = {10.1109/JSTSP.2015.2400417},
	number = {5},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Andreopoulou, Areti and Begault, Durand R. and Katz, Brian F. G.},
	month = aug,
	year = {2015},
	pages = {895 -- 906},
	file = {Andreopoulou et al. - 2015 - Inter-Laboratory Round Robin HRTF Measurement Comp.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\28NB9WLG\\Andreopoulou et al. - 2015 - Inter-Laboratory Round Robin HRTF Measurement Comp.pdf:application/pdf},
}

@phdthesis{fontaine_processus_2019,
	title = {Processus alpha-stables pour le traitement du signal},
	language = {fr},
	school = {Université de Lorraine},
	author = {Fontaine, Mathieu},
	month = jun,
	year = {2019},
	file = {Fontaine - Processus alpha-stables pour le traitement du sign.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\M59UJZAG\\Fontaine - Processus alpha-stables pour le traitement du sign.pdf:application/pdf},
}

@inproceedings{liutkus_generalized_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Generalized {Wiener} filtering with fractional power spectrograms},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7177973/},
	doi = {10.1109/ICASSP.2015.7177973},
	abstract = {In the recent years, many studies have focused on the singlesensor separation of independent waveforms using so-called softmasking strategies, where the short term Fourier transform of the mixture is multiplied element-wise by a ratio of spectrogram models. When the signals are wide-sense stationary, this strategy is theoretically justiﬁed as an optimal Wiener ﬁltering: the power spectrograms of the sources are supposed to add up to yield the power spectrogram of the mixture. However, experience shows that using fractional spectrograms instead, such as the amplitude, yields good performance in practice, because they experimentally better ﬁt the additivity assumption. To the best of our knowledge, no probabilistic interpretation of this ﬁltering procedure was available to date. In this paper, we show that assuming the additivity of fractional spectrograms for the purpose of building soft-masks can be understood as separating locally stationary α-stable harmonizable processes, α-harmonizable in short, thus justifying the procedure theoretically.},
	language = {en},
	urldate = {2020-01-14},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Liutkus, Antoine and Badeau, Roland},
	month = apr,
	year = {2015},
	pages = {266--270},
	file = {Liutkus et Badeau - 2015 - Generalized Wiener filtering with fractional power.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LB2SKR7N\\Liutkus et Badeau - 2015 - Generalized Wiener filtering with fractional power.pdf:application/pdf},
}

@incollection{moore_accurate_2013,
	address = {New York, NY},
	title = {Accurate {Estimation} of {Compression} in {Simultaneous} {Masking} {Enables} the {Simulation} of {Hearing} {Impairment} for {Normal}-{Hearing} {Listeners}},
	volume = {787},
	isbn = {978-1-4614-1589-3 978-1-4614-1590-9},
	url = {http://link.springer.com/10.1007/978-1-4614-1590-9_9},
	abstract = {This chapter presents a uniﬁed gammachirp framework for estimating cochlear compression and synthesizing sounds with inverse compression that cancels the compression of a normal-hearing (NH) listener to simulate the experience of a hearing-impaired (HI) listener. The compressive gammachirp (cGC) ﬁlter was ﬁtted to notched-noise masking data to derive level-dependent ﬁlter shapes and the cochlear compression function (e.g., Patterson et al., J Acoust Soc Am 114:1529–1542, 2003). The procedure is based on the analysis/synthesis technique of Irino and Patterson (IEEE Trans Audio Speech Lang Process 14:2222–2232, 2006) using a dynamic cGC ﬁlterbank (dcGC-FB). The level dependency of the dcGC-FB can be reversed to produce inverse compression and resynthesize sounds in a form that cancels the compression applied by the auditory system of the NH listener. The chapter shows that the estimation of compression in simultaneous masking is improved if the notched-noise procedure for the derivation of auditory ﬁlter shape includes noise bands with different levels. Since both the estimation and resynthesis are performed within the gammachirp framework, it is possible for a speciﬁc NH listener to experience the loss of a speciﬁc HI listener.},
	language = {en},
	urldate = {2020-01-15},
	booktitle = {Basic {Aspects} of {Hearing}},
	publisher = {Springer New York},
	author = {Irino, Toshio and Fukawatase, Tomofumi and Sakaguchi, Makoto and Nisimura, Ryuichi and Kawahara, Hideki and Patterson, Roy D.},
	editor = {Moore, Brian C. J. and Patterson, Roy D. and Winter, Ian M. and Carlyon, Robert P. and Gockel, Hedwig E},
	year = {2013},
	doi = {10.1007/978-1-4614-1590-9_9},
	pages = {73--80},
	file = {Irino et al. - 2013 - Accurate Estimation of Compression in Simultaneous.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6EKWJYQR\\Irino et al. - 2013 - Accurate Estimation of Compression in Simultaneous.pdf:application/pdf},
}

@article{grimault_real_2018,
	title = {A {Real} {Time} {Hearing} {Loss} {Simulator}},
	volume = {104},
	issn = {1610-1928},
	url = {https://www.ingentaconnect.com/content/10.3813/AAA.919252},
	doi = {10.3813/AAA.919252},
	abstract = {Several hearing loss simulators (HLS) have been developed to demonstrate the eﬀects of hearing loss on auditory perception to normal hearing (NH) listeners, and to facilitate prediction of the perception of sound products by hearing impaired customers. This paper describes a real-time HLS based on an inverse, compressive GammaChirp (GC) ﬁlterbank, and how it was used to temporarily handicap NH listeners participating in a traditional notchednoise (NN) masking experiment (e.g. [1]) with a 2-kHz signal frequency. Sets of NN thresholds were obtain with a wide range of symmetric and asymmetric notches at two noise spectrum levels while participants listened to the sounds presented both with and without the HLS. The NN data were used to derive auditory ﬁlter shapes and input/output (IO) functions, which demonstrate that the HLS can simulate the elevation of pure tone threshold and the ﬂattening of the input/output function commonly observed in sensory-neural hearing loss.},
	language = {en},
	number = {5},
	urldate = {2020-01-15},
	journal = {Acta Acustica united with Acustica},
	author = {Grimault, Nicolas and Irino, Toshio and Dimachki, Samar and Corneyllie, Alexandra and Patterson, Roy D. and Garcia, Samuel},
	month = sep,
	year = {2018},
	pages = {904--908},
	file = {Grimault et al. - 2018 - A Real Time Hearing Loss Simulator.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VBYNBY8L\\Grimault et al. - 2018 - A Real Time Hearing Loss Simulator.pdf:application/pdf},
}

@inproceedings{hu_simulation_2011,
	address = {Prague},
	title = {Simulation of hearing loss using compressive gammachirp auditory filters},
	isbn = {978-1-4577-0538-0 978-1-4577-0539-7},
	url = {https://ieeexplore.ieee.org/document/5947586/},
	doi = {10.1109/ICASSP.2011.5947586},
	abstract = {Hearing loss simulation (HLS) systems can provide normal hearing (NH) listeners with demonstrations of consequences of hearing impairment. An accurate simulation of hearing loss can be a valuable tool for developing signal processing strategies for hearing aids. This paper presents a novel HLS system which is based on a physiologically motivated compressive gammachirp auditory filter bank to simulate several aspects of hearing loss including elevated hearing threshold, loudness recruitment and reduced frequency selectivity. The model was evaluated by speech-in-noise tests. An experiment with normally hearing and hearingimpaired listeners showed that the proposed HLS model can mimic typical hearing loss. It is concluded that a physiologically-inspired hearing loss model can perform in the same way as phenomenological models, yet has more fundamental underpinning.},
	language = {en},
	urldate = {2020-01-15},
	booktitle = {2011 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Hu, Hongmei and Sang, Jinqiu and Lutman, Mark E and Bleeck, Stefan},
	month = may,
	year = {2011},
	pages = {5428--5431},
	file = {Hu et al. - 2011 - Simulation of hearing loss using compressive gamma.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VRGE7X5A\\Hu et al. - 2011 - Simulation of hearing loss using compressive gamma.pdf:application/pdf},
}

@article{desloge_speech_2010,
	title = {Speech reception by listeners with real and simulated hearing impairment: {Effects} of continuous and interrupted noise},
	volume = {128},
	issn = {0001-4966},
	shorttitle = {Speech reception by listeners with real and simulated hearing impairment},
	url = {http://asa.scitation.org/doi/10.1121/1.3436522},
	doi = {10.1121/1.3436522},
	language = {en},
	number = {1},
	urldate = {2020-01-15},
	journal = {The Journal of the Acoustical Society of America},
	author = {Desloge, Joseph G. and Reed, Charlotte M. and Braida, Louis D. and Perez, Zachary D. and Delhorne, Lorraine A.},
	month = jul,
	year = {2010},
	pages = {342--359},
	file = {Desloge et al. - 2010 - Speech reception by listeners with real and simula.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YHCXP9XN\\Desloge et al. - 2010 - Speech reception by listeners with real and simula.pdf:application/pdf},
}

@misc{lucier_im_1969,
	title = {I'm {Sitting} in a {Room}},
	author = {Lucier, Alvin},
	year = {1969},
}

@article{ban_tracking_2019,
	title = {Tracking {Multiple} {Audio} {Sources} {With} the von {Mises} {Distribution} and {Variational} {EM}},
	volume = {26},
	issn = {1070-9908, 1558-2361},
	url = {https://ieeexplore.ieee.org/document/8676315/},
	doi = {10.1109/LSP.2019.2908376},
	abstract = {In this letter, we address the problem of simultaneously tracking several moving audio sources, namely the problem of estimating source trajectories from a sequence of observed features. We propose to use the von Mises distribution to model audio-source directions of arrival with circular random variables. This leads to a Bayesian ﬁltering formulation, which is intractable because of the combinatorial explosion of associating observed variables with latent variables, over time. We propose a variational approximation of the ﬁltering distribution. We infer a variational expectationmaximization algorithm that is both computationally tractable and time efﬁcient. We propose an audio-source birth method that favors smooth source trajectories and which is used both to initialize the number of active sources and to detect new sources. We perform experiments with the recently released LOCATA dataset comprising two moving sources and a moving microphone array mounted onto a robot.},
	language = {en},
	number = {6},
	urldate = {2020-02-11},
	journal = {IEEE Signal Processing Letters},
	author = {Ban, Yutong and Alameda-Pineda, Xavier and Evers, Christine and Horaud, Radu},
	month = jun,
	year = {2019},
	pages = {798--802},
	file = {Ban et al. - 2019 - Tracking Multiple Audio Sources With the von Mises.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KQKZFCYV\\Ban et al. - 2019 - Tracking Multiple Audio Sources With the von Mises.pdf:application/pdf},
}

@book{borisagar_speech_2019,
	address = {Cham},
	title = {Speech {Enhancement} {Techniques} for {Digital} {Hearing} {Aids}},
	isbn = {978-3-319-96820-9 978-3-319-96821-6},
	url = {http://link.springer.com/10.1007/978-3-319-96821-6},
	language = {en},
	urldate = {2020-02-11},
	publisher = {Springer International Publishing},
	author = {Borisagar, Komal R. and Thanki, Rohit M. and Sedani, Bhavin S.},
	year = {2019},
	doi = {10.1007/978-3-319-96821-6},
	file = {Borisagar et al. - 2019 - Speech Enhancement Techniques for Digital Hearing .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CQWBUW23\\Borisagar et al. - 2019 - Speech Enhancement Techniques for Digital Hearing .pdf:application/pdf},
}

@inproceedings{braun_directional_2019,
	address = {Brighton, United Kingdom},
	title = {Directional {Interference} {Suppression} {Using} a {Spatial} {Relative} {Transfer} {Function} {Feature}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8682442/},
	doi = {10.1109/ICASSP.2019.8682442},
	abstract = {Many speech enhancement systems consist of a beamformer and a spectral suppression postﬁlter. While it is well understood how to design beamformers to suppress either non-directional or directional interference, their suppression ability is limited by the number, type and positions of microphones. However, spectral postﬁlters that can further increase the suppression, are usually only designed to suppress non-directional noise. In this work, we propose a spatially selective spectral suppressor addressing directional and nondirectional interference. The proposed suppressor is based on the relative transfer function of the target source location. While existing directional suppression techniques are limited to farﬁeld scenarios or certain microphone geometries, we propose a general approach without restrictions on the microphone array and without farﬁeld assumption. We show that the proposed spatial suppressor is able to suppress noise and directional interfering speakers, which substantially improves the performance of speech recognizer, and reduces undesired recognition of interfering talkers.},
	language = {en},
	urldate = {2020-02-11},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Braun, Sebastian and Tashev, Ivan},
	month = may,
	year = {2019},
	pages = {661--665},
	file = {Braun et Tashev - 2019 - Directional Interference Suppression Using a Spati.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\N29QMBV3\\Braun et Tashev - 2019 - Directional Interference Suppression Using a Spati.pdf:application/pdf},
}

@inproceedings{braun_acoustic_2019,
	address = {New Paltz, NY, USA},
	title = {Acoustic {Localization} {Using} {Spatial} {Probability} in {Noisy} and {Reverberant} {Environments}},
	isbn = {978-1-72811-123-0},
	url = {https://ieeexplore.ieee.org/document/8937163/},
	doi = {10.1109/WASPAA.2019.8937163},
	abstract = {In realistic acoustic sound source localization scenarios, we often encounter not only the presence of multiple simultaneous sound sources, but also reverberation and noise. We propose a novel multisource localization method based on the spatial sound presence probability (SSPP). The SSPP can be computed using prior knowledge of the anechoic relative transfer functions (RTFs), which incorporate magnitude and phase information, and makes the approach general for any device and geometry. From the SSPP we can not only obtain multiple simultaneous sound source direction estimates, but also their spatial presence probability. The SSPP can be used for a probabilistic update of the estimated directions, and can further be used to determine the dominant sound source. We demonstrate the robustness of our method in challenging non-stationary scenarios for single- and multi-speaker localization in noisy and reverberant conditions. The proposed method still localizes a sound source at 8 m with an average error below 7◦.},
	language = {en},
	urldate = {2020-02-11},
	booktitle = {2019 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
	publisher = {IEEE},
	author = {Braun, Sebastian and Tashev, Ivan},
	month = oct,
	year = {2019},
	pages = {353--357},
	file = {Braun et Tashev - 2019 - Acoustic Localization Using Spatial Probability in.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9BL4NHA8\\Braun et Tashev - 2019 - Acoustic Localization Using Spatial Probability in.pdf:application/pdf},
}

@article{mosgaard_circular_2018,
	title = {Circular {Statistics}-based low complexity {DOA} estimation for hearing aid application},
	url = {http://arxiv.org/abs/1812.06697},
	abstract = {The proposed Circular statistics-based Inter-Microphone Phase difference estimation Localizer (CIMPL) method is tailored toward binaural hearing aid systems with microphone arrays in each unit. The method utilizes the circular statistics (circular mean and circular variance) of inter-microphone phase difference (IPD) across different microphone pairs. These IPDs are ﬁrstly mapped to time delays through a variance-weighted linear ﬁt, then mapped to azimuth direction-of-arrival (DoA) and lastly information of different microphone pairs is combined. The variance is carried through the different transformations and acts as a reliability index of the estimated angle. Both the resulting angle and variance are fed into a wrapped Kalman ﬁlter, which provides a smoothed estimate of the DoA. The proposed method improves the accuracy of the tracked angle of a single moving source compared with the benchmark method provided by the LOCATA challenge, and it runs approximately 75 times faster.},
	language = {en},
	urldate = {2020-02-11},
	journal = {arXiv:1812.06697 [cs, eess]},
	author = {Mosgaard, Lars D. and Pelegrin-Garcia, David and Elmedyb, Thomas B. and Pihl, Michael J. and Mowlaee, Pejman},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.06697},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Mosgaard et al. - 2018 - Circular Statistics-based low complexity DOA estim.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KAJ38IJ7\\Mosgaard et al. - 2018 - Circular Statistics-based low complexity DOA estim.pdf:application/pdf},
}

@misc{ansi_ansi_2003,
	title = {{ANSI} {S3}.22: {Specification} of {Hearing} {Aid} {Characteristics}},
	language = {en},
	publisher = {American National Standards Institute, New York},
	author = {ANSI},
	year = {2003},
	file = {ANSI S3.22 Specification of Hearing Aid Character.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\C8ZMIKAV\\ANSI S3.22 Specification of Hearing Aid Character.pdf:application/pdf},
}

@article{salvati_localization_2018,
	title = {Localization and {Tracking} of an {Acoustic} {Source} using a {Diagonal} {Unloading} {Beamforming} and a {Kalman} {Filter}},
	url = {http://arxiv.org/abs/1812.01521},
	abstract = {We present the signal processing framework and some results for the IEEE AASP challenge on acoustic source localization and tracking (LOCATA). The system is designed for the direction of arrival (DOA) estimation in single-source scenarios. The proposed framework consists of four main building blocks: pre-processing, voice activity detection (VAD), localization, tracking. The signal pre-processing pipeline includes the short-time Fourier transform (STFT) of the multichannel input captured by the array and the cross power spectral density (CPSD) matrices estimation. The VAD is calculated with a trace-based threshold of the CPSD matrices. The localization is then computed using our recently proposed diagonal unloading (DU) beamforming, which has low-complexity and high resolution. The DOA estimation is ﬁnally smoothed with a Kalman ﬁler (KF). Experimental results on the LOCATA development dataset are reported in terms of the root mean square error (RMSE) for a 7-microphone linear array, the 12-microphone pseudo-spherical array integrated in a prototype head for a humanoid robot, and the 32-microphone spherical array.},
	language = {en},
	urldate = {2020-02-17},
	journal = {arXiv:1812.01521 [cs, eess]},
	author = {Salvati, Daniele and Drioli, Carlo and Foresti, Gian Luca},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.01521},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Salvati et al. - 2018 - Localization and Tracking of an Acoustic Source us.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4QZT3AG5\\Salvati et al. - 2018 - Localization and Tracking of an Acoustic Source us.pdf:application/pdf},
}

@article{traa_wrapped_2013,
	title = {A {Wrapped} {Kalman} {Filter} for {Azimuthal} {Speaker} {Tracking}},
	volume = {20},
	issn = {1070-9908, 1558-2361},
	url = {http://ieeexplore.ieee.org/document/6646231/},
	doi = {10.1109/LSP.2013.2287125},
	abstract = {We present the wrapped Kalman ﬁlter (WKF) for tracking the azimuth of a speaker with a compact, 3-channel microphone array. Traditional extended and unscented ﬁlters assume that the observation is a rotating vector in R2. However, the azimuth inhabits a 1D subspace: the unit circle. We model the state variable with a wrapped Gaussian distribution and show that this achieves a lower mean squared error than 2D methods. We demonstrate the superior tracking performance of the WKF in simulated and real reverberant environments.},
	language = {en},
	number = {12},
	urldate = {2020-02-17},
	journal = {IEEE Signal Processing Letters},
	author = {Traa, Johannes and Smaragdis, Paris},
	month = dec,
	year = {2013},
	pages = {1257--1260},
	file = {Traa et Smaragdis - 2013 - A Wrapped Kalman Filter for Azimuthal Speaker Trac.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4UUKSNH5\\Traa et Smaragdis - 2013 - A Wrapped Kalman Filter for Azimuthal Speaker Trac.pdf:application/pdf},
}

@inproceedings{yamaoka_cnn-based_2019,
	title = {{CNN}-based virtual microphone signal estimation for {MPDR} beamforming in underdetermined situations},
	booktitle = {27th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {Yamaoka, Kouei and Li, Li and Ono, Nobutaka and Makino, Shoji and Yamada, Takeshi},
	year = {2019},
	file = {yamaoka19eusipco1-5.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BCUUEYYU\\yamaoka19eusipco1-5.pdf:application/pdf},
}

@article{katahira_nonlinear_2016,
	title = {Nonlinear speech enhancement by virtual increase of channels and maximum {SNR} beamformer},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Katahira, Hiroki and Ono, Nobutaka and Miyabe, Shigeki and Yamada, Takeshi and Makino, Shoji},
	month = nov,
	year = {2016},
	file = {Katahira2016_Article_NonlinearSpeechEnhancementByVi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2R5LC688\\Katahira2016_Article_NonlinearSpeechEnhancementByVi.pdf:application/pdf},
}

@inproceedings{yamaoka_performance_2017,
	title = {Performance {Evaluation} of {Nonlinear} {Speech} {Enhancement} {Based} on {Virtual} {Increase} of {Channels} in {Reverberant} {Environments}},
	booktitle = {25th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {Yamaoka, Kouei and Makino, Shoji and Ono, Nobutaka and Yamada, Takeshi},
	year = {2017},
}

@article{kim_control_2001,
	title = {Control of auditory distance perception based on the auditory parallax model},
	volume = {62},
	issn = {0003682X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0003682X00000232},
	doi = {10.1016/S0003-682X(00)00023-2},
	language = {en},
	number = {3},
	urldate = {2020-03-02},
	journal = {Applied Acoustics},
	author = {Kim, Hae-Young and Suzuki, Yôiti and Takane, Shouichi and Sone, Toshio},
	month = mar,
	year = {2001},
	pages = {245--270},
	file = {Kim et al. - 2001 - Control of auditory distance perception based on t.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IMIEWEQP\\Kim et al. - 2001 - Control of auditory distance perception based on t.pdf:application/pdf},
}

@article{baumgartner_modeling_2014,
	title = {Modeling sound-source localization in sagittal planes for human listeners},
	volume = {136},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4887447},
	doi = {10.1121/1.4887447},
	language = {en},
	number = {2},
	urldate = {2020-03-03},
	journal = {The Journal of the Acoustical Society of America},
	author = {Baumgartner, Robert and Majdak, Piotr and Laback, Bernhard},
	month = aug,
	year = {2014},
	pages = {791--802},
	file = {Baumgartner et al. - 2014 - Modeling sound-source localization in sagittal pla.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HTXUIDEV\\Baumgartner et al. - 2014 - Modeling sound-source localization in sagittal pla.pdf:application/pdf},
}

@article{hendrickx_influence_2017,
	title = {Influence of head tracking on the externalization of speech stimuli for non-individualized binaural synthesis},
	volume = {141},
	issn = {0001-4966},
	url = {http://aip.scitation.org/doi/10.1121/1.4978612},
	doi = {10.1121/1.4978612},
	language = {en},
	number = {3},
	urldate = {2020-03-03},
	journal = {The Journal of the Acoustical Society of America},
	author = {Hendrickx, Etienne and Stitt, Peter and Messonnier, Jean-Christophe and Lyzwa, Jean-Marc and Katz, Brian FG and de Boishéraud, Catherine},
	month = mar,
	year = {2017},
	pages = {2011--2023},
	file = {Hendrickx et al. - 2017 - Influence of head tracking on the externalization .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LG7J6GNP\\Hendrickx et al. - 2017 - Influence of head tracking on the externalization .pdf:application/pdf},
}

@inproceedings{fontana_binaural_2007,
	address = {Montréal, Canada},
	title = {Binaural for {Popular} {Music}: a {Case} of {Study}},
	abstract = {The goal of this study is to retrieve useful information about the reactions of listeners to different recording techniques, namely binaural and stereo. This has been done comparing different mixes of the same song. Each mix is obtained from stereophonic and binaural recordings, or processing proximity recordings with stereo panning, binaural synthesis or a hybrid approach. The comparison is made through listening tests with headphones and an analysis of subjects’ reactions and meaningful subjective parameters ratings.},
	language = {en},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Auditory} {Display}},
	author = {Fontana, Simone and Farina, Angelo and Grenier, Yves},
	month = jun,
	year = {2007},
	pages = {6},
	file = {2007 - ICAD 2007 Proceedings.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\24X9ARFV\\2007 - ICAD 2007 Proceedings.pdf:application/pdf},
}

@inproceedings{sunder_eect_2014,
	address = {Los Angeles},
	title = {Eﬀect of {Headphone} {Equalization} on {Auditory} {Distance} {Perception}},
	abstract = {Headphones are not acoustically transparent and thus it aﬀects both the timbral as well as the spatial quality of the input sound source. The eﬀect of the headphones has to be compensated by calculating the inverse of the headphones transfer function and convolving it with the binaurally synthesized audio. Headphone transfer function (HPTF) also depends on the headphone-ear coupling and thus displays high spectral variation between individuals. It has been found that the type of equalization (individual or non-individual) aﬀects the directional perception of the virtual audio reproduced using headphones. However, little investigation has been carried out on the eﬀect of headphone equalization on auditory distance perception. In this paper, we study in detail the perceptual eﬀects of equalization on the auditory distance perception in the proximal region in anechoic conditions. It was found that the equalization of the headphone is critical for good distance perception. The type of equalization (individual or non-individual) did not have a signiﬁcant eﬀect on the auditory distance perception indicating that the distance perception does not depend on the idiosyncratic features. The eﬀect of repositioning of the headphone on auditory depth perception is also studied in this work.},
	language = {en},
	booktitle = {Audio {Engineering} {Society} {Convention} 137},
	author = {Sunder, Kaushik and Tan, Ee-Leng and Gan, Woon-Seng},
	year = {2014},
	pages = {11},
	file = {Sunder et al. - 2014 - Eﬀect of Headphone Equalization on Auditory Distan.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PQ5IIY7L\\Sunder et al. - 2014 - Eﬀect of Headphone Equalization on Auditory Distan.pdf:application/pdf},
}

@article{evers_locata_2020,
	title = {The {LOCATA} {Challenge}: {Acoustic} {Source} {Localization} and {Tracking}},
	shorttitle = {The {LOCATA} {Challenge}},
	url = {http://arxiv.org/abs/1909.01008},
	abstract = {The ability to localize and track acoustic events is a fundamental prerequisite for equipping machines with the ability to be aware of and engage with humans in their surrounding environment. However, in realistic scenarios, audio signals are adversely affected by reverberation, noise, interference, and periods of speech inactivity. In dynamic scenarios, where the sources and microphone platforms may be moving, the signals are additionally affected by variations in the source-sensor geometries. In practice, approaches to sound source localization and tracking are often impeded by missing estimates of active sources, estimation errors, as well as false estimates. The aim of the LOCAlization and TrAcking (LOCATA) Challenge is an openaccess framework for the objective evaluation and benchmarking of broad classes of algorithms for sound source localization and tracking. This paper provides a review of relevant localization and tracking algorithms and, within the context of the existing literature, a detailed evaluation and dissemination of the LOCATA submissions. The evaluation highlights achievements in the ﬁeld, open challenges, and identiﬁes potential future directions.},
	language = {en},
	urldate = {2020-03-13},
	journal = {arXiv:1909.01008 [cs, eess]},
	author = {Evers, Christine and Loellmann, Heinrich and Mellmann, Heinrich and Schmidt, Alexander and Barfuss, Hendrik and Naylor, Patrick and Kellermann, Walter},
	month = jan,
	year = {2020},
	note = {arXiv: 1909.01008},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
	file = {Evers et al. - 2020 - The LOCATA Challenge Acoustic Source Localization.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ECLXR828\\Evers et al. - 2020 - The LOCATA Challenge Acoustic Source Localization.pdf:application/pdf},
}

@inproceedings{rickard_approximate_2002,
	address = {Orlando, USA},
	title = {On the {Approximate} {W}-{Disjoint} {Orthogonality} of {Speech}},
	language = {en},
	booktitle = {{ICASSP} 2002 - 2002 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Rickard, Scott and Yilmaz, Özgür},
	month = may,
	year = {2002},
	pages = {4},
	file = {Rickard - ON THE APPROXIMATE W-DISJOINT ORTHOGONALITY OF SPE.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\53NZ2TI2\\Rickard - ON THE APPROXIMATE W-DISJOINT ORTHOGONALITY OF SPE.pdf:application/pdf},
}

@article{itturriet_perceptually_2019,
	title = {Perceptually {Relevant} {Preservation} of {Interaural} {Time} {Differences} in {Binaural} {Hearing} {Aids}},
	volume = {27},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/8629305/},
	doi = {10.1109/TASLP.2019.2895973},
	abstract = {This paper presents a noise reduction method with perceptually relevant preservation of the interaural time difference (ITD) of the residual noise in binaural hearing aids. The interaural coherence (IC) concept, previously applied to the multichannel Wiener ﬁlter (MWF) for preservation of the spatial subjective sensation of diffuse noise ﬁelds, is proposed here to both preserve and emphasize the ITD binaural cues of a directional acoustic noise source. It is demonstrated that the previously developed MWF-ITD technique may decrease the original IC magnitude of the processed noise, consequently increasing the variance of the interaural phase difference (IPD) of the output signals. It is shown that the MWFIC technique simultaneously minimizes a nonlinear function of the difference between input and output IPD, which is strictly related to ITD, and preserves the natural coherence of directional noise captured by the reference microphones. Objective measures and psychoacoustic experiments corroborate the theoretical ﬁndings, showing that the MWF-IC technique provides relevant noise reduction, while preserving the original ITD subjective perception and original lateralization for a directional noise source. These results are especially relevant for hearing aid designers, since they indicate MWF-IC as a noise reduction technique that provides residual noise spatial preservation for both diffuse and directional noise sources in frequencies below 1.5 kHz.},
	language = {en},
	number = {4},
	urldate = {2020-03-23},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Itturriet, Fabio Pires and Costa, Marcio Holsbach},
	month = apr,
	year = {2019},
	pages = {753--764},
	file = {Itturriet et Costa - 2019 - Perceptually Relevant Preservation of Interaural T.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZWIZSHHI\\Itturriet et Costa - 2019 - Perceptually Relevant Preservation of Interaural T.pdf:application/pdf},
}

@inproceedings{habets_online_2010,
	address = {Dallas, TX},
	title = {An online quasi-{Newton} algorithm for blind {SIMO} identification},
	isbn = {978-1-4244-4295-9},
	url = {http://ieeexplore.ieee.org/document/5496248/},
	doi = {10.1109/ICASSP.2010.5496248},
	abstract = {In the last decade various time- and frequency-domain algorithms were derived to blindly identify acoustic systems. One of these algorithms is the multichannel Newton (MCN) algorithm, which is also the basis of the well known normalized multichannel frequency-domain least-mean-square (NMCFLMS) algorithm. A major drawback of the MCN is that it requires the computation and inversion of a Hessian matrix, which involves extensive computation making it unsuitable for online applications. In this paper, we therefore derive and investigate an efﬁcient online multichannel quasiNewton (MCQN) algorithm that updates the inverse of the Hessian by analyzing successive gradient vectors. The new MCQN is shown to exhibit similar performance to MCN but with much reduced complexity.},
	language = {en},
	urldate = {2020-03-23},
	booktitle = {2010 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Habets, Emanuel A.P. and Naylor, Patrick A.},
	month = mar,
	year = {2010},
	pages = {2662--2665},
	file = {Habets et Naylor - 2010 - An online quasi-Newton algorithm for blind SIMO id.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JDX5V9MW\\Habets et Naylor - 2010 - An online quasi-Newton algorithm for blind SIMO id.pdf:application/pdf},
}

@article{ye_maximum_1995,
	title = {Maximum likelihood {DOA} estimation and asymptotic {Cramer}-{Rao} bounds for additive unknown colored noise},
	volume = {43},
	issn = {1053587X},
	url = {http://ieeexplore.ieee.org/document/376846/},
	doi = {10.1109/78.376846},
	language = {en},
	number = {4},
	urldate = {2020-03-23},
	journal = {IEEE Transactions on Signal Processing},
	author = {Ye, Hao and DeGroat, D.},
	month = apr,
	year = {1995},
	pages = {938--949},
	file = {Hao Ye et DeGroat - 1995 - Maximum likelihood DOA estimation and asymptotic C.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5VHGF3VD\\Hao Ye et DeGroat - 1995 - Maximum likelihood DOA estimation and asymptotic C.pdf:application/pdf},
}

@inproceedings{hadad_binaural_2012,
	address = {Aachen},
	title = {Binaural {Linearly} {Constrained} {Minimum} {Variance} {Beamformer} for {Hearing} {Aid} {Applications}},
	abstract = {In many cases hearing impaired persons suffer from hearing loss in both ears, necessitating two hearing apparatuses. In such cases, the applied speech enhancement algorithms should be capable of preserving the, so called, binaural cues. In this paper, a binaural extension of the linearly constrained minimum variance (LCMV) beamformer is proposed. The proposed algorithm, denoted binaural linearly constrained minimum variance (BLCMV) beamformer, is capable of extracting desired speakers while suppressing interference speakers. The BLCMV maintains the binaural cues of both the desired and the interference sources in the constrained space. The ability of preserving the binaural cues makes the BLCMV beamformer particularly suitable for hearing aid applications. It is further proposed to obtain a reduced complexity implementation by sharing common blocks in both sides of the hearing aid device. The performance of the proposed method, in terms of imposed distortion, interference cancellation and cue preservation, is veriﬁed by an extensive experimental study using signals recorded by a dummy head in an actual room.},
	language = {en},
	booktitle = {International {Workshop} on {Acoustic} {Signal} {Enhancemoment} ({IWAENC})},
	author = {Hadad, Elior and Gannot, Sharon and Doclo, Simon},
	month = sep,
	year = {2012},
	pages = {4},
	file = {Hadad et al. - Binaural Linearly Constrained Minimum Variance Bea.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Z84QWVWI\\Hadad et al. - Binaural Linearly Constrained Minimum Variance Bea.pdf:application/pdf},
}

@article{marquardt_interaural_2018,
	title = {Interaural {Coherence} {Preservation} for {Binaural} {Noise} {Reduction} {Using} {Partial} {Noise} {Estimation} and {Spectral} {Postfiltering}},
	volume = {26},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/8331109/},
	doi = {10.1109/TASLP.2018.2823081},
	abstract = {The objective of binaural speech enhancement algorithms is to reduce the undesired noise component, while preserving the desired speech source and the binaural cues of all sound sources. For the scenario of a single desired speech source in a diffuse noise ﬁeld, an extension of the binaural multi-channel Wiener ﬁlter (MWF), namely the MWF-IC, has been recently proposed, which aims to preserve the interaural coherence (IC) of the noise component. However, due to the large complexity of the MWF-IC, in this paper we propose several alternative algorithms at a lower computational complexity. First, we consider a quasidistortionless version of the MWF-IC, denoted as MVDR-IC. Secondly, we propose to preserve the IC of the noise component using the binaural MWF with partial noise estimation (MWFN) and the binaural minimum-variance-distortionless response beamformer with partial noise estimation (MVDR-N), for which closed-form expressions exist. In addition, we show that for the MVDR-N a closed-form expression can be derived for the tradeoff parameter yielding a desired magnitude squared coherence (MSC) for the output noise component. Since contrary to the MWF-IC and the MWF-N the MVDR-IC and the MVDR-N do not take into account the spectro-temporal properties of the speech and the noise components, we propose to apply a spectral postﬁlter to the ﬁlter outputs, improving the noise reduction performance. The performance of all algorithms is compared in several diffuse noise scenarios. The simulation results show that both the MVDR-IC and the MVDR-N are able to preserve the MSC of the noise component, while generally the MVDRIC shows a slightly better noise reduction performance at a larger complexity. Further simulation results show that applying a spectral postﬁlter leads to a very similar performance for all considered algorithms in terms of noise reduction and speech distortion.},
	language = {en},
	number = {7},
	urldate = {2020-03-25},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Marquardt, Daniel and Doclo, Simon},
	month = jul,
	year = {2018},
	pages = {1261--1274},
	file = {Marquardt et Doclo - 2018 - Interaural Coherence Preservation for Binaural Noi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IMDK3CEK\\Marquardt et Doclo - 2018 - Interaural Coherence Preservation for Binaural Noi.pdf:application/pdf},
}

@article{eggermont_electrocochleography_1977,
	title = {Electrocochleography and {Recruitment}},
	volume = {86},
	doi = {10.1177/000348947708600202},
	abstract = {Electrocochleography was performed in about 80 recruiting ears, including Meniere ears and ears showing hearing loss due to asphyxia, noise trauma, or the use of ototoxic drugs. Criteria for recruitment detection on the basis of electrocochleography were investigated by means of the description of action potential waveforms, slopes of action potential inputoutput curves, and latency-intensity functions. The validity of two models for the mechanism of loudness recruitment is discussed in relation to data obtained from narrow-band response contributions to the compound action potential in response to tonebursts. A relationship was found between the shape of the AP input-output curve for toneburst stimulation and the shape of the response area, which favors the recruitment model recently proposed by Evans. This justifies use of the slope of the input-output curve as an indicator for the presence of loudness recruitment, and additional support is provided by the latency at near-threshold values. The AP waveform seems to have pure illustrative interest.},
	language = {en},
	number = {2},
	journal = {Annals of Otology, Rhinology \& Laryngology},
	author = {Eggermont, J J},
	year = {1977},
	pages = {12},
	file = {Eccehmont - ELECTROCOCHLEOGRAPHY AND RECRUITMENT.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3K8EVMXU\\Eccehmont - ELECTROCOCHLEOGRAPHY AND RECRUITMENT.pdf:application/pdf},
}

@inproceedings{marquardt_coherence_2013,
	address = {Vancouver, BC, Canada},
	title = {Coherence preservation in multi-channel {Wiener} filtering based noise reduction for binaural hearing aids},
	isbn = {978-1-4799-0356-6},
	url = {http://ieeexplore.ieee.org/document/6639354/},
	doi = {10.1109/ICASSP.2013.6639354},
	abstract = {Besides noise reduction an important objective of binaural speech enhancement algorithms is the preservation of the binaural cues, i.e. the Interaural Level Difference and the Interaural Time Difference of all sound sources. Recently, extensions of the binaural Multichannel Wiener ﬁlter (MWF) have been presented, which aim to preserve the binaural cues of the residual noise component. However, since these algorithms aim to preserve the Interaural Transfer Function (ITF), they are well-suited only for directional noise sources but not for, e.g. spatially isotropic noise, which can not be fully described by the ITF. In this paper, we present an extension of the binaural MWF, aiming to preserve the Interaural Coherence of the residual noise component. Experimental results using spatially isotropic noise show that the proposed algorithm yields a good preservation of the Interaural Coherence without signiﬁcantly degrading the output SNR compared to the binaural MWF and the binaural MWF with ITF preservation.},
	language = {en},
	urldate = {2020-03-27},
	booktitle = {2013 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Marquardt, Daniel and Hohmann, Volker and Doclo, Simon},
	month = may,
	year = {2013},
	pages = {8648--8652},
	file = {Marquardt et al. - 2013 - Coherence preservation in multi-channel Wiener fil.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UKY3MIET\\Marquardt et al. - 2013 - Coherence preservation in multi-channel Wiener fil.pdf:application/pdf},
}

@article{chen_jingdong_new_2006,
	title = {New insights into the noise reduction {Wiener} filter},
	volume = {14},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/1643650/},
	doi = {10.1109/TSA.2005.860851},
	abstract = {The problem of noise reduction has attracted a considerable amount of research attention over the past several decades. Among the numerous techniques that were developed, the optimal Wiener ﬁlter can be considered as one of the most fundamental noise reduction approaches, which has been delineated in different forms and adopted in various applications. Although it is not a secret that the Wiener ﬁlter may cause some detrimental effects to the speech signal (appreciable or even signiﬁcant degradation in quality or intelligibility), few efforts have been reported to show the inherent relationship between noise reduction and speech distortion. By deﬁning a speech-distortion index to measure the degree to which the speech signal is deformed and two noise-reduction factors to quantify the amount of noise being attenuated, this paper studies the quantitative performance behavior of the Wiener ﬁlter in the context of noise reduction. We show that in the single-channel case the a posteriori signal-to-noise ratio (SNR) (deﬁned after the Wiener ﬁlter) is greater than or equal to the a priori SNR (deﬁned before the Wiener ﬁlter), indicating that the Wiener ﬁlter is always able to achieve noise reduction. However, the amount of noise reduction is in general proportional to the amount of speech degradation. This may seem discouraging as we always expect an algorithm to have maximal noise reduction without much speech distortion. Fortunately, we show that speech distortion can be better managed in three different ways. If we have some a priori knowledge (such as the linear prediction coefﬁcients) of the clean speech signal, this a priori knowledge can be exploited to achieve noise reduction while maintaining a low level of speech distortion. When no a priori knowledge is available, we can still achieve a better control of noise reduction and speech distortion by properly manipulating the Wiener ﬁlter, resulting in a suboptimal Wiener ﬁlter. In case that we have multiple microphone sensors, the multiple observations of the speech signal can be used to reduce noise with less or even no speech distortion.},
	language = {en},
	number = {4},
	urldate = {2020-03-29},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	author = {{Chen, Jingdong} and Benesty, J. and {Huang, Yiteng} and Doclo, S.},
	month = jul,
	year = {2006},
	pages = {1218--1234},
	file = {Jingdong Chen et al. - 2006 - New insights into the noise reduction Wiener filte.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\K6RR4CYI\\Jingdong Chen et al. - 2006 - New insights into the noise reduction Wiener filte.pdf:application/pdf},
}

@article{hazrati_blind_2013,
	title = {Blind binary masking for reverberation suppression in cochlear implants},
	volume = {133},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4789891},
	doi = {10.1121/1.4789891},
	language = {en},
	number = {3},
	urldate = {2020-04-20},
	journal = {The Journal of the Acoustical Society of America},
	author = {Hazrati, Oldooz and Lee, Jaewook and Loizou, Philipos C.},
	month = mar,
	year = {2013},
	pages = {1607--1614},
	file = {Hazrati et al. - 2013 - Blind binary masking for reverberation suppression.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4S59DQWI\\Hazrati et al. - 2013 - Blind binary masking for reverberation suppression.pdf:application/pdf},
}

@inproceedings{romblom_near-field_2008,
	address = {San Francisco, USA},
	title = {Near-{Field} {Compensation} for {HRTF} {Processing}},
	abstract = {It is difficult to present near-field virtual audio displays using available HRTF filters, as most existing databases are measured at a single distance in the far-field of the listener’s head. Measuring near-field data is possible, but would quickly become tiresome due to the large number of distances required to simulate sources moving close to the head. For applications requiring a convincing near-field virtual audio display, one could compensate the far-field HRTF filters with a scheme based on 1/r spreading roll off. However, this would not account for spectral differences that occur in the near-field. Using difference filters based on a spherical head model [1][2][3], as well as a geometrically accurate HRTF lookup scheme [4], one can compensate existing measured data and present a convincing virtual audio display for near-field distances.},
	language = {en},
	booktitle = {125th {Convention} of the {Audio} {Engineering} {Society}},
	author = {Romblom, David and Cook, Bryan},
	month = oct,
	year = {2008},
	pages = {6},
	file = {Romblom et Cook - 2008 - Near-Field Compensation for HRTF Processing.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\B46V9FYN\\Romblom et Cook - 2008 - Near-Field Compensation for HRTF Processing.pdf:application/pdf},
}

@inproceedings{carpentier_twenty_2015,
	address = {Denton, TX, United States},
	title = {Twenty {Years} of {Ircam} {Spat}: {Looking} {Back}, {Looking} {Forward}},
	abstract = {Ircam Spat is a software suite dedicated to sound spatialization and artiﬁcial reverberation. This paper traces the evolution of the software during the last few years and presents its current state with a focus on the newly available features.},
	language = {en},
	booktitle = {41st {International} {Computer} {Music} {Conference} ({ICMC})},
	author = {Carpentier, Thibaut and Noisternig, Markus and Warusfel, Olivier},
	month = sep,
	year = {2015},
	pages = {9},
	file = {Carpentier et al. - Twenty Years of Ircam Spat Looking Back, Looking .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GSPDPZI7\\Carpentier et al. - Twenty Years of Ircam Spat Looking Back, Looking .pdf:application/pdf},
}

@article{algazi_approximating_2002,
	title = {Approximating the head-related transfer function using simple geometric models of the head and torso},
	volume = {112},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1508780},
	doi = {10.1121/1.1508780},
	language = {en},
	number = {5},
	urldate = {2020-04-22},
	journal = {The Journal of the Acoustical Society of America},
	author = {Algazi, V. Ralph and Duda, Richard O. and Duraiswami, Ramani and Gumerov, Nail A. and Tang, Zhihui},
	month = nov,
	year = {2002},
	pages = {2053--2064},
	file = {Algazi et al. - 2002 - Approximating the head-related transfer function u.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3PWXW775\\Algazi et al. - 2002 - Approximating the head-related transfer function u.pdf:application/pdf},
}

@article{algazi_estimation_2001,
	title = {Estimation of a {Spherical}-{Head} {Model} from {Anthropometry}},
	volume = {48},
	number = {6},
	journal = {Journal of Audio Engineering Society},
	author = {Algazi, V. Ralph and Avendano, Carlos and Duda, Richard O.},
	year = {2001},
	pages = {472--479},
	file = {113e01fca6973c6073a05d2ebd20d9f5acbb.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\8FTLJZF3\\113e01fca6973c6073a05d2ebd20d9f5acbb.pdf:application/pdf},
}

@inproceedings{zohourian_new_2017,
	address = {Kos, Greece},
	title = {New insights into the role of the head radius in model-based binaural speaker localization},
	isbn = {978-0-9928626-7-1},
	url = {http://ieeexplore.ieee.org/document/8081201/},
	doi = {10.23919/EUSIPCO.2017.8081201},
	abstract = {In this work we evaluate the effects of the head radius on binaural localization algorithms. We employ a spherical head model and the null-steering beamforming localization method. The model characterizes the binaural cues in the form of HRTFs. One of the main parameters in this model is the head radius. We propose to optimize jointly for both the source location and the head radius. In contrast to the free-ﬁeld conﬁguration where it is difﬁcult to estimate the source location and microphone distance simultaneously, the binaural algorithm yields a unique solution to the head radius. Moreover, for real recordings we show that the commonly-assumed size of the head achieves a fairly reliable performance. For applications with non-typical size of the head, e.g., hearing-impaired children the adaptation of the head radius using the proposed algorithm would improve the accuracy of the binaural localization algorithm.},
	language = {en},
	urldate = {2020-04-23},
	booktitle = {2017 25th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	publisher = {IEEE},
	author = {Zohourian, Mehdi and Martin, Rainer and Madhu, Nilesh},
	month = aug,
	year = {2017},
	pages = {221--225},
	file = {Zohourian et al. - 2017 - New insights into the role of the head radius in m.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JI8PQ9N4\\Zohourian et al. - 2017 - New insights into the role of the head radius in m.pdf:application/pdf},
}

@phdthesis{corey_microphone_2019,
	address = {Urbana-Champaign},
	title = {Microphone {Array} {Processing} for {Augmented} {Listening}},
	language = {en},
	school = {University of Illinois},
	author = {Corey, Ryan Michael},
	year = {2019},
	file = {Corey - Microphone Array Processing for Augmented Listenin.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IM7MJ7QM\\Corey - Microphone Array Processing for Augmented Listenin.pdf:application/pdf},
}

@inproceedings{schwartz_joint_2016,
	address = {Budapest, Hungary},
	title = {Joint estimation of late reverberant and speech power spectral densities in noisy environments using frobenius norm},
	isbn = {978-0-9928626-5-7},
	url = {http://ieeexplore.ieee.org/document/7760423/},
	doi = {10.1109/EUSIPCO.2016.7760423},
	abstract = {Various dereverberation and noise reduction algorithms require power spectral density estimates of the anechoic speech, reverberation, and noise. In this work, we derive a novel multichannel estimator for the power spectral densities (PSDs) of the reverberation and the speech suitable also for noisy environments. The speech and reverberation PSDs are estimated from all the entries of the received signals power spectral density (PSD) matrix. The Frobenius norm of a general error matrix is minimized to ﬁnd the best ﬁtting PSDs. Experimental results show that the proposed estimator provides accurate estimates of the PSDs, and is outperforming competing estimators. Moreover, when used in a multi-microphone noise reduction and dereverberation algorithm, the estimated reverberation and speech PSDs are shown to provide improved performance measures as compared with the competing estimators.},
	language = {en},
	urldate = {2020-04-23},
	booktitle = {2016 24th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	publisher = {IEEE},
	author = {Schwartz, Ofer and Gannot, Sharon and Habets, Emanuel A.P.},
	month = aug,
	year = {2016},
	pages = {1123--1127},
	file = {Schwartz et al. - 2016 - Joint estimation of late reverberant and speech po.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YZLYHJCF\\Schwartz et al. - 2016 - Joint estimation of late reverberant and speech po.pdf:application/pdf},
}

@article{ayllon_design_2013,
	title = {Design of microphone arrays for hearing aids optimized to unknown subjects},
	volume = {93},
	issn = {01651684},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165168413001564},
	doi = {10.1016/j.sigpro.2013.04.013},
	abstract = {The improvement of speech intelligibility in hearing aids is still an unsolved problem. Modern devices, either monaural or binaural, may include microphone arrays to provide directivity by means of spatial filtering, but the signals that arrive at the array are distorted by the so-called head shadow effect, which must be considered in the design. The fact that these effects are highly dependent on a person causes the design of an array customized for a subject to need a correct measurement of these effects, which is not practical in real scenarios. The lack of information about the head causes directivity reduction and distortions. In this work we propose three different methods to compute optimized filter coefficients with the purpose of maximizing the array gain and minimizing distortions, and with the objective that these filter coefficients are valid for all users. Furthermore, measurements of the amplitude and phase distortions caused by a lack of head information in the design are introduced. In addition, the methods proposed here have been evaluated in 13 different array configurations. Results show that these approaches reduce significantly the gain reduction and distortions, obtaining a similar intelligibility in comparison to the case where head information is available.},
	language = {en},
	number = {11},
	urldate = {2020-04-23},
	journal = {Signal Processing},
	author = {Ayllón, David and Gil-Pita, Roberto and Rosa-Zurera, Manuel},
	month = nov,
	year = {2013},
	pages = {3239--3250},
	file = {Ayllón et al. - 2013 - Design of microphone arrays for hearing aids optim.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5ELCWQYM\\Ayllón et al. - 2013 - Design of microphone arrays for hearing aids optim.pdf:application/pdf},
}

@article{ag_benefit_nodate,
	title = {Benefit from different beamforming schemes in bilateral hearing aid users: {Do} binaural hearing abilities matter?},
	language = {en},
	author = {Ag, Sonova and Oldenburg, Hörzentrum},
	pages = {8},
	file = {Ag et Oldenburg - Benefit from different beamforming schemes in bila.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Z4FUMEJ2\\Ag et Oldenburg - Benefit from different beamforming schemes in bila.pdf:application/pdf},
}

@phdthesis{anderson_robust_2015,
	title = {Robust {Multichannel} {Microphone} {Beamforming}},
	abstract = {In this thesis, a method for the design and implementation of a spatially robust multichannel microphone beamforming system is presented. A set of spatial correlation functions are derived for 2D and 3D farﬁeld/near-ﬁeld scenarios based on von Mises(-Fisher), Gaussian, and uniform source location distributions. These correlation functions are used to design spatially robust beamformers and blocking beamformers (nullformers) designed to enhance or suppress a known source, where the target source location is not perfectly known — due to either an incorrect location estimate or movement of the target while the beamformers are active.},
	language = {en},
	school = {Victoria University of Wellington},
	author = {Anderson, Craig},
	year = {2015},
	file = {Anderson - Robust Multichannel Microphone Beamforming.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HSLB2E39\\Anderson - Robust Multichannel Microphone Beamforming.pdf:application/pdf},
}

@article{schwartz_multispeaker_2017,
	title = {Multispeaker {LCMV} {Beamformer} and {Postfilter} for {Source} {Separation} and {Noise} {Reduction}},
	volume = {25},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/7842601/},
	doi = {10.1109/TASLP.2017.2655258},
	abstract = {The problem of source separation and noise reduction using multiple microphones is addressed. The minimum mean square error (MMSE) estimator for the multi-speaker case is derived and a novel decomposition of this estimator is presented. The MMSE estimator is decomposed into two stages: i) a multi-speaker linearly constrained minimum variance (LCMV) beamformer (BF), and ii) a subsequent multi-speaker Wiener postﬁlter. The ﬁrst stage separates and enhances the signals of the individual speakers by utilizing the spatial characteristics of the speakers (as manifested by the respective acoustic transfer functions (ATFs)) and the noise spatial correlation matrix, while the second stage exploits the speakers’ power spectral density matrix to reduce the residual noise at the output of the ﬁrst stage. The output vector of the multi-speaker LCMV BF is proven to be the sufﬁcient statistic for estimating the marginal speech signals in both the classic sense and the Bayesian sense. The log spectral amplitude estimator for the multi-speaker case is also derived given the multi-speaker LCMV BF outputs. The performance evaluation was conducted using measured ATFs and directional noise with various signal-to-noise ratio levels. It is empirically veriﬁed that the multi-speaker postﬁlters are beneﬁcial in terms of signal-to-interference plus noise ratio improvement when compared with the single-speaker postﬁlter.},
	language = {en},
	number = {5},
	urldate = {2020-04-23},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Schwartz, Ofer and Gannot, Sharon and Habets, Emanuel A. P.},
	month = may,
	year = {2017},
	pages = {940--951},
	file = {Schwartz et al. - 2017 - Multispeaker LCMV Beamformer and Postfilter for So.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KS9XAWKK\\Schwartz et al. - 2017 - Multispeaker LCMV Beamformer and Postfilter for So.pdf:application/pdf},
}

@inproceedings{weisberg_online_2019,
	address = {Brighton, United Kingdom},
	title = {An {Online} {Multiple}-speaker {DOA} {Tracking} {Using} the {CappÉ}-{Moulines} {Recursive} {Expectation}-maximization {Algorithm}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8682659/},
	doi = {10.1109/ICASSP.2019.8682659},
	abstract = {In this paper, we present a multiple-speaker direction of arrival (DOA) tracking algorithm with a microphone array that utilizes the recursive EM (REM) algorithm proposed by Cappe´ and Moulines. In our model, all sources can be located in one of a predeﬁned set of candidate DOAs. Accordingly, the received signals from all microphones are modeled as Mixture of Gaussians (MoG) vectors in which each speaker is associated with a corresponding Gaussian. The localization task is then formulated as a maximum likelihood (ML) problem, where the MoG weights and the power spectral density (PSD) of the speakers are the unknown parameters. The REM algorithm is then utilized to estimate the ML parameters in an online manner, facilitating multiple source tracking. By using Fisher-Neyman factorization, the outputs of the minimum variance distortionless response (MVDR)-beamformer (BF) are shown to be sufﬁcient statistics for estimating the parameters of the problem at hand. With that, the terms for the E-step are signiﬁcantly simpliﬁed to a scalar form. An experimental study demonstrates the beneﬁts of the using proposed algorithm in both a simulated dataset and real recordings from the acoustic source localization and tracking (LOCATA) data-set.},
	language = {en},
	urldate = {2020-04-23},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Weisberg, Koby and Gannot, Sharon and Schwartz, Ofer},
	month = may,
	year = {2019},
	pages = {656--660},
	file = {Weisberg et al. - 2019 - An Online Multiple-speaker DOA Tracking Using the .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\J9BPAV4G\\Weisberg et al. - 2019 - An Online Multiple-speaker DOA Tracking Using the .pdf:application/pdf},
}

@inproceedings{boeddeker_jointly_2020,
	address = {Barcelona, Spain},
	title = {Jointly optimal dereverberation and beamforming},
	doi = {10.1109/ICASSP40776.2020.9054393},
	abstract = {We previously proposed an optimal (in the maximum likelihood sense) convolutional beamformer that can perform simultaneous denoising and dereverberation, and showed its superiority over the widely used cascade of a Weighted Prediction Error (WPE) dereverberation ﬁlter and a conventional Minimum-Power Distortionless Response (MPDR) beamformer. However, it has not been fully investigated which components in the convolutional beamformer yield such superiority. To this end, this paper presents a new derivation of the convolutional beamformer that allows us to factorize it into a WPE dereverberation ﬁlter, and a special type of a (nonconvolutional) beamformer, referred to as a weighted MPDR (wMPDR) beamformer, without loss of optimality. With experiments, we show that the superiority of the convolutional beamformer in fact comes from its wMPDR part.},
	language = {en},
	urldate = {2020-04-23},
	booktitle = {International {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Boeddeker, Christoph and Nakatani, Tomohiro and Kinoshita, Keisuke and Haeb-Umbach, Reinhold},
	month = may,
	year = {2020},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language},
	pages = {216--220},
	file = {Boeddeker et al. - 2019 - Jointly optimal dereverberation and beamforming.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\U62825Y3\\Boeddeker et al. - 2019 - Jointly optimal dereverberation and beamforming.pdf:application/pdf},
}

@article{nakatani_maximum_2019,
	title = {Maximum likelihood convolutional beamformer for simultaneous denoising and dereverberation},
	url = {http://arxiv.org/abs/1908.02710},
	abstract = {This article describes a probabilistic formulation of a Weighted Power minimization Distortionless response convolutional beamformer (WPD). The WPD uniﬁes a weighted prediction error based dereverberation method (WPE) and a minimum power distortionless response beamformer (MPDR) into a single convolutional beamformer, and achieves simultaneous dereverberation and denoising in an optimal way. However, the optimization criterion is obtained simply by combining existing criteria without any clear theoretical justiﬁcation. This article presents a generative model and a probabilistic formulation of a WPD, and derives an optimization algorithm based on a maximum likelihood estimation. We also describe a method for estimating the steering vector of the desired signal by utilizing WPE within the WPD framework to provide an effective and efﬁcient beamformer for denoising and dereverberation.},
	language = {en},
	urldate = {2020-04-23},
	journal = {arXiv:1908.02710 [cs, eess]},
	author = {Nakatani, Tomohiro and Kinoshita, Keisuke},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.02710},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Nakatani et Kinoshita - 2019 - Maximum likelihood convolutional beamformer for si.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CINK2YB5\\Nakatani et Kinoshita - 2019 - Maximum likelihood convolutional beamformer for si.pdf:application/pdf},
}

@article{kavalekalam_model-based_2018,
	title = {Model-based {Speech} {Enhancement} for {Intelligibility} {Improvement} in {Binaural} {Hearing} {Aids}},
	url = {http://arxiv.org/abs/1806.04885},
	abstract = {Speech intelligibility is often severely degraded among hearing impaired individuals in situations such as the cocktail party scenario. The performance of the current hearing aid technology has been observed to be limited in these scenarios. In this paper, we propose a binaural speech enhancement framework that takes into consideration the speech production model. The enhancement framework proposed here is based on the Kalman ﬁlter that allows us to take the speech production dynamics into account during the enhancement process. The usage of a Kalman ﬁlter requires the estimation of clean speech and noise short term predictor (STP) parameters, and the clean speech pitch parameters. In this work, a binaural codebookbased method is proposed for estimating the STP parameters, and a directional pitch estimator based on the harmonic model and maximum likelihood principle is used to estimate the pitch parameters. The proposed method for estimating the STP and pitch parameters jointly uses the information from left and right ears, leading to a more robust estimation of the ﬁlter parameters. Objective measures such as PESQ and STOI have been used to evaluate the enhancement framework in different acoustic scenarios representative of the cocktail party scenario. We have also conducted subjective listening tests on a set of nine normal hearing subjects, to evaluate the performance in terms of intelligibility and quality improvement. The listening tests show that the proposed algorithm, even with access to only a single channel noisy observation, signiﬁcantly improves the overall speech quality, and the speech intelligibility by up to 15\%.},
	language = {en},
	urldate = {2020-04-23},
	journal = {arXiv:1806.04885 [cs, eess]},
	author = {Kavalekalam, Mathew Shaji and Nielsen, Jesper K. and Boldt, Jesper B. and Christensen, Mads G.},
	month = oct,
	year = {2018},
	note = {arXiv: 1806.04885},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Kavalekalam et al. - 2018 - Model-based Speech Enhancement for Intelligibility.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\FACIMCU9\\Kavalekalam et al. - 2018 - Model-based Speech Enhancement for Intelligibility.pdf:application/pdf},
}

@phdthesis{chabot-leclerc_computational_2016,
	title = {Computational modeling of speech intelligibility in adverse conditions},
	language = {en},
	school = {DTU},
	author = {Chabot-Leclerc, Alexandre},
	year = {2016},
	file = {Chabot-Leclerc - Computational modeling of speech intelligibility i.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MKWNVPV3\\Chabot-Leclerc - Computational modeling of speech intelligibility i.pdf:application/pdf},
}

@article{ngo_modied_2010,
	title = {A {Modiﬁed} {Multi}-{Channel} {Wiener} {Filter} based {Noise} {Reduction} in {Hearing} {Aids}},
	abstract = {In this paper, a multi-channel noise reduction algorithm is presented based on a Speech Distortion Weighted Multi-channel Wiener Filter (SDW-MWF) approach that incorporates a ﬂexible weighting factor, which is an extension of the SDW-MWF incorporating the conditional Speech Presence Probability (SPP). A typical SDW-MWF uses a ﬁxed weighting factor to trade-off between noise reduction and speech distortion without taking speech presence or speech absence into account. Consequently, the improvement in noise reduction comes at the cost of a higher speech distortion since the speech dominant segments and the noise dominant segments are weighted equally. Based on a two-state speech model with a noiseonly and a speech+noise state, a solution is introduced that allows for a more ﬂexible trade-off between noise reduction and speech distortion. Experimental results with hearing aid scenarios demonstrate that the proposed SDW-MWF incorporating the ﬂexible weighting factor improves the signal-to-noise-ratio with lower speech distortion compared to a typical SDW-MWF and the SDW-MWF incorporating the conditional SPP.},
	language = {en},
	author = {Ngo, Kim and Jensen, Søren Holdt},
	year = {2010},
	pages = {17},
	file = {Ngo et Jensen - 2010 - A Modiﬁed Multi-Channel Wiener Filter based Noise .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\8YPK8Y4H\\Ngo et Jensen - 2010 - A Modiﬁed Multi-Channel Wiener Filter based Noise .pdf:application/pdf},
}

@inproceedings{bagheri_exploiting_2019,
	title = {Exploiting {Multi}-{Channel} {Speech} {Presence} {Probability} in {Parametric} {Multi}-{Channel} {Wiener} {Filter}},
	url = {http://www.isca-speech.org/archive/Interspeech_2019/abstracts/2665.html},
	doi = {10.21437/Interspeech.2019-2665},
	abstract = {In this paper, we present a practical implementation of the parametric multi-channel Wiener ﬁlter (PMWF) noise reduction algorithm. In particular, we extend on methods that incorporate the multi-channel speech presence probability (MC-SPP) in the PMWF derivation and its output. The use of the MC-SPP brings several advantages. Firstly, the MC-SPP allows for better estimates of noise and speech statistics, for which we derive a direct update of the inverse of the noise power spectral density (PSD). Secondly, the MC-SPP is used to control the trade-off parameter in PMWF which, with proper tuning, outperforms the traditional approach with a ﬁxed trade-off parameter. Thirdly, the MC-SPP for each frequency-band is used to obtain the MMSE estimate of the desired speech signal at the output, where we control the maximum amount of noise reduction based on our application. Experimental results on a large number of simulated scenarios show signiﬁcant beneﬁts of employing MC-SPP in terms of SNR improvements and speech distortion.},
	language = {en},
	urldate = {2020-04-23},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Bagheri, Saeed and Giacobello, Daniele},
	month = sep,
	year = {2019},
	pages = {101--105},
	file = {Bagheri et Giacobello - 2019 - Exploiting Multi-Channel Speech Presence Probabili.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2EZIXEVB\\Bagheri et Giacobello - 2019 - Exploiting Multi-Channel Speech Presence Probabili.pdf:application/pdf},
}

@techreport{das_eeg-informed_2020,
	type = {preprint},
	title = {{EEG}-informed speaker extraction from noisy recordings in neuro-steered hearing aids: linear versus deep learning methods},
	shorttitle = {{EEG}-informed speaker extraction from noisy recordings in neuro-steered hearing aids},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.01.22.915181},
	abstract = {Objective: A hearing aid’s noise reduction algorithm cannot infer to which speaker the user intends to listen to. Auditory attention decoding (AAD) algorithms allow to infer this information from neural signals, which leads to the concept of neuro-steered hearing aids. We aim to evaluate and demonstrate the feasibility of AAD-supported speech enhancement pipelines in challenging noisy conditions without access to clean speech signals. Methods: We evaluated a linear versus a deep neural network (DNN) based speaker separation pipeline, with samegender speaker mixtures for 3 different speaker positions and 3 different noise conditions. Results: AAD results based on the linear approach were found to be at least on par and sometimes even better than pure DNN-based approaches in terms of AAD accuracy in all tested conditions. However, when extending the DNN with a linear data-driven beamformer, a performance improvement over the purely linear approach was obtained in the most challenging scenarios. The use of multiple microphones was also found to improve speaker separation and AAD performance over single-microphone systems. Conclusion: Our study shows that neuro-steered speech enhancement, combining the best of both worlds (linear and DNN), results in robust performance. Signiﬁcance: Recent proof-of-concept studies in this context each focus on a different method in a different experimental setting, which makes it hard to compare them. Furthermore, their idealized experimental conditions only give a rather premature evidence on the viability of the AAD paradigm in a hearing aid context. This work provides a systematic proof-of-concept of neuro-steered speech enhancement in challenging conditions.},
	language = {en},
	urldate = {2020-04-23},
	institution = {Bioengineering},
	author = {Das, Neetha and Zegers, Jeroen and Van hamme, Hugo and Francart, Tom and Bertrand, Alexander},
	month = jan,
	year = {2020},
	doi = {10.1101/2020.01.22.915181},
	file = {Das et al. - 2020 - EEG-informed speaker extraction from noisy recordi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EHLI2865\\Das et al. - 2020 - EEG-informed speaker extraction from noisy recordi.pdf:application/pdf},
}

@article{liu_intensity_2018,
	title = {Intensity {Particle} {Flow} {SMC}-{PHD} {Filter} {For} {Audio} {Speaker} {Tracking}},
	url = {http://arxiv.org/abs/1812.01570},
	abstract = {Non-zero diffusion particle ﬂow Sequential Monte Carlo probability hypothesis density (NPF-SMC-PHD) ﬁltering has been recently introduced for multi-speaker tracking. However, the NPF does not consider the missing detection which plays a key role in estimation of the number of speakers with their states. To address this limitation, we propose to use intensity particle ﬂow (IPF) in NPFSMC-PHD ﬁlter. The proposed method, IPF-SMC-PHD, considers the clutter intensity and detection probability while no data association algorithms are used for the calculation of particle ﬂow. Experiments on the LOCATA (acoustic source Localization and Tracking) dataset with the sequences of task 4 show that our proposed IPF-SMC-PHD ﬁlter improves the tracking performance in terms of estimation accuracy as compared to its baseline counterparts.},
	language = {en},
	urldate = {2020-04-23},
	journal = {arXiv:1812.01570 [cs, eess]},
	author = {Liu, Yang and Wang, Wenwu and Kilic, Volkan},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.01570},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Liu et al. - 2018 - Intensity Particle Flow SMC-PHD Filter For Audio S.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GMZ4YMQ3\\Liu et al. - 2018 - Intensity Particle Flow SMC-PHD Filter For Audio S.pdf:application/pdf},
}

@phdthesis{tsakalides_array_1995,
	title = {{ARRAY} {SIGNAL} {PROCESSING} {WITH} {ALPHA}-{STABLE} {DISTRIBUTIONS}},
	language = {en},
	school = {UNIVERSITY OF SOUTHERN CALIFORNIA},
	author = {Tsakalides, Panagiotis},
	year = {1995},
	file = {Tsakalides - ARRAY SIGNAL PROCESSING WITH ALPHA-STABLE DISTRIBU.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MACZA9ZI\\Tsakalides - ARRAY SIGNAL PROCESSING WITH ALPHA-STABLE DISTRIBU.pdf:application/pdf},
}

@phdthesis{jeanvoine_interet_2012,
	address = {Lyon},
	title = {Intérêt des algorithmes de réduction de bruit dans l'implant cochléaire: {Application} à la binauralité},
	language = {fr},
	school = {Université claude bernard lyon 1},
	author = {Jeanvoine, Arnaud},
	month = dec,
	year = {2012},
	file = {Jeanvoine - Intérêt des algorithmes de réduction de bruit dans.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WVQ89HFZ\\Jeanvoine - Intérêt des algorithmes de réduction de bruit dans.pdf:application/pdf},
}

@article{zolfaghari_large_2014,
	title = {Large {Deformation} {Diffeomorphic} {Metric} {Mapping} {And} {Fast}-{Multipole} {Boundary} {Element} {Method} {Provide} {New} {Insights} {For} {Binaural} {Acoustics}},
	url = {http://arxiv.org/abs/1401.7100},
	doi = {10.1109/ICASSP.2014.6854123},
	abstract = {This paper describes how Large Deformation Diffeomorphic Metric Mapping (LDDMM) can be coupled with a Fast Multipole (FM) Boundary Element Method (BEM) to investigate the relationship between morphological changes in the head, torso, and outer ears and their acoustic ﬁltering (described by Head Related Transfer Functions, HRTFs). The LDDMM technique provides the ability to study and implement morphological changes in ear, head and torso shapes. The FMBEM technique provides numerical simulations of the acoustic properties of an individual’s head, torso, and outer ears. This paper describes the ﬁrst application of LDDMM to the study of the relationship between a listener’s morphology and a listener’s HRTFs. To demonstrate some of the new capabilities provided by the coupling of these powerful tools, we examine the classical question of what it means to “listen through another individual’s outer ears.” This work utilizes the data provided by the Sydney York Morphological and Acoustic Recordings of Ears (SYMARE) database.},
	language = {en},
	urldate = {2020-04-23},
	journal = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	author = {Zolfaghari, Reza and Epain, Nicolas and Jin, Craig T. and Glaunès, Joan and Tew, Anthony},
	month = may,
	year = {2014},
	note = {arXiv: 1401.7100},
	keywords = {Computer Science - Computational Geometry},
	pages = {2863--2867},
	file = {Zolfaghari et al. - 2014 - Large Deformation Diffeomorphic Metric Mapping And.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7QX9KSB4\\Zolfaghari et al. - 2014 - Large Deformation Diffeomorphic Metric Mapping And.pdf:application/pdf},
}

@inproceedings{galdo_generating_2011,
	title = {Generating {Virtual} {Microphone} {Signals} {Using} {Geometrical} {Information} {Gathered} by {Distributed} {Arrays}},
	abstract = {Conventional recording techniques for spatial audio are limited to the fact that the spatial image obtained is always relative to the position in which the microphones have been physically placed. In many applications, however, it is desired to place the microphones outside the sound scene and yet be able to capture the sound from an arbitrary perspective. This contribution proposes a method to place a virtual microphone at an arbitrary point in space, by computing a signal perceptually similar to the one which would have been picked up if the microphone had been physically placed in the sound scene. The method relies on a parametric model of the sound ﬁeld based on point-like isotropic sound sources. The required geometrical information is gathered by two or more distributed microphone arrays. Measurement results demonstrate the applicability of the proposed method and reveal its limitations.},
	language = {en},
	booktitle = {Joint {Workshop} on {Hands}-free {Speech} {Communication} and {Microphone} {Arrays}},
	author = {Galdo, Giovanni Del and Thiergart, Oliver and Weller, Tobias and Habets, Emanuel A P},
	month = may,
	year = {2011},
	pages = {6},
	file = {Galdo et al. - GENERATING VIRTUAL MICROPHONE SIGNALS USING GEOMET.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AKQESG73\\Galdo et al. - GENERATING VIRTUAL MICROPHONE SIGNALS USING GEOMET.pdf:application/pdf},
}

@article{souden_gaussian_2010,
	title = {Gaussian {Model}-{Based} {Multichannel} {Speech} {Presence} {Probability}},
	volume = {18},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/5299039/},
	doi = {10.1109/TASL.2009.2035150},
	abstract = {The knowledge of the target speech presence probability in a mixture of signals captured by a speech communication system is of paramount importance in several applications including reliable noise reduction algorithms. In this correspondence, we establish a new expression for speech presence probability when an array of microphones with an arbitrary geometry is used. Our study is based on the assumption of the Gaussian statistical model for all signals and involves the noise and noisy data statistics only. In comparison with the single-channel case, the new proposed multichannel approach can signiﬁcantly increase the detection accuracy. In particular, when the additive noise is spatially coherent, perfect speech presence detection is theoretically possible, while when the noise is spatially white, a coherent summation of speech components is performed to allow for enhanced speech presence probability estimation.},
	language = {en},
	number = {5},
	urldate = {2020-05-04},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Souden, Mehrez and {Jingdong Chen} and Benesty, Jacob and Affes, Sofiène},
	month = jul,
	year = {2010},
	pages = {1072--1077},
	file = {Souden et al. - 2010 - Gaussian Model-Based Multichannel Speech Presence .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RGPL3DSH\\Souden et al. - 2010 - Gaussian Model-Based Multichannel Speech Presence .pdf:application/pdf},
}

@phdthesis{gomez_consolidating_2018,
	title = {Consolidating natural spatial perception and improved {SNR} in hearing aids: {Jackrabbit}, a new method},
	language = {en},
	school = {Technische Universität München},
	author = {Gomez, Gabriel},
	year = {2018},
	file = {Gomez - Consolidating natural spatial perception and impro.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\37BCYA5K\\Gomez - Consolidating natural spatial perception and impro.pdf:application/pdf},
}

@inproceedings{corey_nonstationary_2016,
	address = {Pacific Grove, CA, USA},
	title = {Nonstationary source separation for underdetermined speech mixtures},
	isbn = {978-1-5386-3954-2},
	url = {http://ieeexplore.ieee.org/document/7869186/},
	doi = {10.1109/ACSSC.2016.7869186},
	abstract = {We propose a multichannel source separation method for underdetermined mixtures of nonstationary signals, such as speech. Like other underdetermined algorithms, our method relies on the time-frequency sparsity of speech. However, our interference model allows more than one source to be active at the same time and frequency, providing better separation performance for mixtures of many sources. The system consists of several beamformers designed for different combinations of interference sources. A decision rule selects the beamformer that best suppresses the active interferers at each time-frequency point. Experiments on both simulated and real mixtures show improved interference suppression compared to conventional beamformers.},
	language = {en},
	urldate = {2020-05-04},
	booktitle = {2016 50th {Asilomar} {Conference} on {Signals}, {Systems} and {Computers}},
	publisher = {IEEE},
	author = {Corey, Ryan M. and Singer, Andrew C.},
	month = nov,
	year = {2016},
	pages = {934--938},
	file = {Corey et Singer - 2016 - Nonstationary source separation for underdetermine.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BIRTKV5G\\Corey et Singer - 2016 - Nonstationary source separation for underdetermine.pdf:application/pdf},
}

@article{brooks_equivalence_1972,
	title = {Equivalence of the {Likelihood} {Ratio} {Processor}, the {Maximum} {Signal}-to-{Noise} {Ratio} {Filter}, and the {Wiener} {Filter}},
	volume = {AES-8},
	issn = {0018-9251},
	url = {http://ieeexplore.ieee.org/document/4103030/},
	doi = {10.1109/TAES.1972.309588},
	abstract = {It is shown that for the problem of detecting a nonfluctuating target in Gaussian noise, three common optimality criteria lead to identical multichannel filter designs.},
	language = {en},
	number = {5},
	urldate = {2020-05-07},
	journal = {IEEE Transactions on Aerospace and Electronic Systems},
	author = {Brooks, Lowell and Reed, Irving},
	month = sep,
	year = {1972},
	pages = {690--692},
	file = {Brooks et Reed - 1972 - Equivalence of the Likelihood Ratio Processor, the.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\99BAHBS9\\Brooks et Reed - 1972 - Equivalence of the Likelihood Ratio Processor, the.pdf:application/pdf},
}

@inproceedings{hiipakka_estimating_2009,
	title = {{ESTIMATING} {PRESSURE} {AT} {EARDRUM} {WITH} {PRESSURE}-{VELOCITY} {MEASUREMENT} {FROM} {EAR} {CANAL} {ENTRANCE}},
	abstract = {It is important to know the sound pressure signal at the eardrum in headphone reproduction and in audiological applications. Unfortunately, it is difﬁcult to conduct direct measurement safely. Estimating pressure signals at the eardrum based on measurements done elsewhere in the ear canal is sensitive to positioning errors. This is particularly the case at the canal entrance. In addition, not knowing the acoustic properties of the ear canal makes estimation difﬁcult. This study shows that when both sound pressure and velocity are measured at the canal entrance using a pressure-velocity probe, the pressure signal at the eardrum can be estimated with much higher accuracy than from the pressure-only measurement. The method is demonstrated and validated by using physical simulators and computational modeling.},
	language = {en},
	booktitle = {{IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics}},
	author = {Hiipakka, Marko and Karjalainen, Matti and Pulkki, Ville},
	year = {2009},
	pages = {4},
	file = {Hiipakka et al. - 2009 - ESTIMATING PRESSURE AT EARDRUM WITH PRESSURE-VELOC.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\FUCW5L2L\\Hiipakka et al. - 2009 - ESTIMATING PRESSURE AT EARDRUM WITH PRESSURE-VELOC.pdf:application/pdf},
}

@inproceedings{larcher_equalization_1998,
	address = {San Francisco, USA},
	title = {Equalization methods in binaural technology},
	abstract = {The diffuse-field Head-Related Transfer Function can be used to equalize headphones, binaural (dummy head) recordings and binaural synthesis filters in 3-D audio systems, in order to eliminate the effects of thc transducers involved in t.he measurement or recording process. The advantages of diffuse-field equalization (compared to free-field equalizat.ion) in these different applications are reviewed and illustrated.},
	language = {en},
	booktitle = {105th convention of the {Audio} {Engineering} {Society}},
	author = {Larcher, Vdronique},
	month = sep,
	year = {1998},
	pages = {29},
	file = {Larcher - Equalization methods in binaural technology.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3P2QN8ZS\\Larcher - Equalization methods in binaural technology.pdf:application/pdf},
}

@inproceedings{moore_improving_2019,
	title = {Improving robustness of adaptive beamforming for hearing devices},
	volume = {7},
	language = {en},
	booktitle = {International {Symposium} on {Auditory} and {Audiological} {Research}},
	author = {Moore, Alastair H and Naylor, Patrick A and Brookes, Mike},
	month = aug,
	year = {2019},
	pages = {12},
	file = {Moore et al. - Improving robustness of adaptive beamforming for h.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\8YQHCBAE\\Moore et al. - Improving robustness of adaptive beamforming for h.pdf:application/pdf},
}

@inproceedings{ehrenberg_sensitivity_2010,
	address = {Eilat, Israel},
	title = {Sensitivity analysis of {MVDR} and {MPDR} beamformers},
	isbn = {978-1-4244-8681-6},
	url = {http://ieeexplore.ieee.org/document/5662190/},
	doi = {10.1109/EEEI.2010.5662190},
	abstract = {A sensitivity analysis of two distortionless beamformers is presented in this paper1. Speciﬁcally, two well-known variants, namely the minimum power distortionless response (MPDR) and minimum variance distortionless response (MVDR) beamformers, are considered. In our scenario, which is typical to many modern communications systems, waves emitted by multiple point sources are received by an antenna array. An analytical expression for the signal to interference and noise ratio (SINR) improvement obtained by both beamformers under steering errors is derived. These expression are experimentally evaluated and compared with the robust Capon beamformer (RCB), a robust variant of the MPDR beamformer. We show that the MVDR beamformer, which uses the noise correlation matrix in its minimization criterion, is more robust to steering errors than its counterparts, that use the received signal correlation matrix. Furthermore, even if the noise correlation matrix is erroneously estimated due to steering errors in the interference direction, the MVDR advantage is still maintained for reasonable range of steering errors. These conclusions conform with Cox [1] ﬁndings. Only line of sight propagation regime is considered in the current contribution. Ongoing research extends this work to fading channels.},
	language = {en},
	urldate = {2020-05-07},
	booktitle = {2010 {IEEE} 26-th {Convention} of {Electrical} and {Electronics} {Engineers} in {Israel}},
	publisher = {IEEE},
	author = {Ehrenberg, Livnat and Gannot, Sharon and Leshem, Amir and Zehavi, Ephraim},
	month = nov,
	year = {2010},
	pages = {416--420},
	file = {Ehrenberg et al. - 2010 - Sensitivity analysis of MVDR and MPDR beamformers.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VUYKAS4P\\Ehrenberg et al. - 2010 - Sensitivity analysis of MVDR and MPDR beamformers.pdf:application/pdf},
}

@article{corey_binaural_2020,
	title = {Binaural {Audio} {Source} {Remixing} with {Microphone} {Array} {Listening} {Devices}},
	url = {http://arxiv.org/abs/2004.11956},
	abstract = {Augmented listening devices, such as hearing aids and augmented reality headsets, enhance human perception by changing the sounds that we hear. Microphone arrays can improve the performance of listening systems in noisy environments, but most array-based listening systems are designed to isolate a single sound source from a mixture. This work considers a source-remixing ﬁlter that alters the relative level of each source independently. Remixing rather than separating sounds can help to improve perceptual transparency: it causes less distortion to the signal spectrum and especially to the interaural cues that humans use to localize sounds in space.},
	language = {en},
	urldate = {2020-05-07},
	journal = {arXiv:2004.11956 [cs, eess]},
	author = {Corey, Ryan M. and Singer, Andrew C.},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.11956},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Corey et Singer - 2020 - Binaural Audio Source Remixing with Microphone Arr.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Z7I4AZIR\\Corey et Singer - 2020 - Binaural Audio Source Remixing with Microphone Arr.pdf:application/pdf},
}

@article{scheibler_fast_2019,
	title = {Fast {Independent} {Vector} {Extraction} by {Iterative} {SINR} {Maximization}},
	url = {http://arxiv.org/abs/1910.10654},
	abstract = {We propose fast independent vector extraction (FIVE), a new algorithm that blindly extracts a single non-Gaussian source from a Gaussian background. The algorithm iteratively computes beamforming weights maximizing the signal-to-interference-and-noise ratio for an approximate noise covariance matrix. We demonstrate that this procedure minimizes the negative log-likelihood of the input data according to a well-deﬁned probabilistic model. The minimization is carried out via the auxiliary function technique whereas, unlike related methods, the auxiliary function is globally minimized at every iteration. Numerical experiments are carried out to assess the performance of FIVE. We ﬁnd that it is vastly superior to competing methods in terms of convergence speed, and has high potential for real-time applications.},
	language = {en},
	urldate = {2020-05-07},
	journal = {arXiv:1910.10654 [cs, eess]},
	author = {Scheibler, Robin and Ono, Nobutaka},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.10654},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
	file = {Scheibler et Ono - 2019 - Fast Independent Vector Extraction by Iterative SI.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YG2DRAAE\\Scheibler et Ono - 2019 - Fast Independent Vector Extraction by Iterative SI.pdf:application/pdf},
}

@article{ephraim_speech_1984,
	title = {Speech enhancement using a minimum-mean square error short-time spectral amplitude estimator},
	volume = {32},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1164453/},
	doi = {10.1109/TASSP.1984.1164453},
	language = {en},
	number = {6},
	urldate = {2020-05-07},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Ephraim, Y. and Malah, D.},
	month = dec,
	year = {1984},
	pages = {1109--1121},
	file = {Ephraim et Malah - 1984 - Speech enhancement using a minimum-mean square err.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NMECEDGH\\Ephraim et Malah - 1984 - Speech enhancement using a minimum-mean square err.pdf:application/pdf},
}

@incollection{vincent_time-frequency_2018,
	address = {Chichester, UK},
	title = {Time-{Frequency} {Processing}: {Spectral} {Properties}},
	isbn = {978-1-119-27986-0 978-1-119-27989-1},
	shorttitle = {Time-{Frequency} {Processing}},
	url = {http://doi.wiley.com/10.1002/9781119279860.ch2},
	language = {en},
	urldate = {2020-05-07},
	booktitle = {Audio {Source} {Separation} and {Speech} {Enhancement}},
	publisher = {John Wiley \& Sons Ltd},
	author = {Virtanen, Tuomas and Vincent, Emmanuel and Gannot, Sharon},
	editor = {Vincent, Emmanuel and Virtanen, Tuomas and Gannot, Sharon},
	month = aug,
	year = {2018},
	doi = {10.1002/9781119279860.ch2},
	pages = {15--29},
	file = {Virtanen et al. - 2018 - Time-Frequency Processing Spectral Properties.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JDSL3GRA\\Virtanen et al. - 2018 - Time-Frequency Processing Spectral Properties.pdf:application/pdf},
}

@article{crochiere_weighted_1980,
	title = {A weighted overlap-add method of short-time {Fourier} analysis/{Synthesis}},
	volume = {28},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1163353/},
	doi = {10.1109/TASSP.1980.1163353},
	language = {en},
	number = {1},
	urldate = {2020-05-19},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Crochiere, R.},
	month = feb,
	year = {1980},
	pages = {99--102},
	file = {Crochiere - 1980 - A weighted overlap-add method of short-time Fourie.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3L29KJEV\\Crochiere - 1980 - A weighted overlap-add method of short-time Fourie.pdf:application/pdf},
}

@inproceedings{hendriks_mmse_2010,
	address = {Dallas, TX, USA},
	title = {{MMSE} based noise {PSD} tracking with low complexity},
	isbn = {978-1-4244-4295-9},
	url = {http://ieeexplore.ieee.org/document/5495680/},
	doi = {10.1109/ICASSP.2010.5495680},
	abstract = {Most speech enhancement algorithms heavily depend on the noise power spectral density (PSD). Because this quantity is unknown in practice, estimation from the noisy data is necessary.},
	language = {en},
	urldate = {2020-05-19},
	booktitle = {2010 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Hendriks, Richard C. and Heusdens, Richard and Jensen, Jesper},
	year = {2010},
	pages = {4266--4269},
	file = {Hendriks et al. - 2010 - MMSE based noise PSD tracking with low complexity.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PTX9XR8C\\Hendriks et al. - 2010 - MMSE based noise PSD tracking with low complexity.pdf:application/pdf},
}

@article{erkelens_minimum_2007,
	title = {Minimum {Mean}-{Square} {Error} {Estimation} of {Discrete} {Fourier} {Coefficients} {With} {Generalized} {Gamma} {Priors}},
	volume = {15},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/4276753/},
	doi = {10.1109/TASL.2007.899233},
	abstract = {This paper considers techniques for single-channel speech enhancement based on the discrete Fourier transform (DFT). Speciﬁcally, we derive minimum mean-square error (MMSE) estimators of speech DFT coefﬁcient magnitudes as well as of complex-valued DFT coefﬁcients based on two classes of generalized gamma distributions, under an additive Gaussian noise assumption. The resulting generalized DFT magnitude estimator has as a special case the existing scheme based on a Rayleigh speech prior, while the complex DFT estimators generalize existing schemes based on Gaussian, Laplacian, and Gamma speech priors. Extensive simulation experiments with speech signals degraded by various additive noise sources verify that signiﬁcant improvements are possible with the more recent estimators based on super-Gaussian priors. The increase in perceptual evaluation of speech quality (PESQ) over the noisy signals is about 0.5 points for street noise and about 1 point for white noise, nearly independent of input signal-to-noise ratio (SNR). The assumptions made for deriving the complex DFT estimators are less accurate than those for the magnitude estimators, leading to a higher maximum achievable speech quality with the magnitude estimators.},
	language = {en},
	number = {6},
	urldate = {2020-05-19},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	author = {Erkelens, Jan S. and Hendriks, Richard C. and Heusdens, Richard and Jensen, Jesper},
	month = aug,
	year = {2007},
	pages = {1741--1752},
	file = {Erkelens et al. - 2007 - Minimum Mean-Square Error Estimation of Discrete F.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QK9SZVM7\\Erkelens et al. - 2007 - Minimum Mean-Square Error Estimation of Discrete F.pdf:application/pdf},
}

@article{alexander_acoustic_2017,
	title = {Acoustic and perceptual effects of amplitude and frequency compression on high-frequency speech},
	volume = {142},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4997938},
	doi = {10.1121/1.4997938},
	language = {en},
	number = {2},
	urldate = {2020-06-15},
	journal = {The Journal of the Acoustical Society of America},
	author = {Alexander, Joshua M. and Rallapalli, Varsha},
	month = aug,
	year = {2017},
	pages = {908--923},
	file = {Alexander et Rallapalli - 2017 - Acoustic and perceptual effects of amplitude and f.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\C87LUA35\\Alexander et Rallapalli - 2017 - Acoustic and perceptual effects of amplitude and f.pdf:application/pdf},
}

@article{kurozumi_relationship_1983,
	title = {The relationship between the cross‐correlation coefficient of two‐channel acoustic signals and sound image quality},
	volume = {74},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.390281},
	doi = {10.1121/1.390281},
	language = {en},
	number = {6},
	urldate = {2020-06-15},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kurozumi, Kohichi and Ohgushi, Kengo},
	month = dec,
	year = {1983},
	pages = {1726--1733},
	file = {Kurozumi et Ohgushi - 1983 - The relationship between the cross‐correlation coe.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\F4C3RW69\\Kurozumi et Ohgushi - 1983 - The relationship between the cross‐correlation coe.pdf:application/pdf},
}

@article{kowalewski_perceptual_2020,
	title = {Perceptual {Evaluation} of {Signal}-to-{Noise}-{Ratio}-{Aware} {Dynamic} {Range} {Compression} in {Hearing} {Aids}},
	volume = {24},
	issn = {2331-2165, 2331-2165},
	url = {http://journals.sagepub.com/doi/10.1177/2331216520930531},
	doi = {10.1177/2331216520930531},
	abstract = {Dynamic range compression is a compensation strategy commonly used in modern hearing aids. Fast-acting systems respond relatively quickly to the fluctuations in the input level. This allows for more effective compression of the dynamic range of speech and hence enhanced the audibility of its low-intensity components. However, such processing also amplifies the background noise, distorts the modulation spectra of both the speech and the background, and can reduce the output signalto-noise ratio (SNR). Recently, May et al. proposed a novel SNR-aware compression strategy, in which the compression speed is adapted depending on whether speech is present or absent. Fast-acting compression is applied to speech-dominated time–frequency (T-F) units, while noise-dominated T-F units are processed using slow-acting compression. It has been shown that this strategy provides a similar effective compression of the speech dynamic range as conventional fast-acting compression, while introducing fewer distortions of the modulation spectrum of the background and providing an improved output SNR. In this study, this SNR-aware compression strategy was compared with conventional fast- and slow-acting compression in terms of speech intelligibility and subjective preference in a group of 17 hearing-impaired listeners with varying degree of hearing loss. The results show a speech intelligibility benefit of the SNR-aware compression strategy over the conventional slow-acting system. Furthermore, the SNR-aware approach demonstrates an increased subjective preference compared with both conventional fast- and slow-acting systems.},
	language = {en},
	urldate = {2020-06-29},
	journal = {Trends in Hearing},
	author = {Kowalewski, Borys and Dau, Torsten and May, Tobias},
	month = jan,
	year = {2020},
	pages = {14},
	file = {Kowalewski et al. - 2020 - Perceptual Evaluation of Signal-to-Noise-Ratio-Awa.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VHPG85TS\\Kowalewski et al. - 2020 - Perceptual Evaluation of Signal-to-Noise-Ratio-Awa.pdf:application/pdf},
}

@article{jian_li_robust_2003,
	title = {On robust capon beamforming and diagonal loading},
	volume = {51},
	issn = {1053-587X},
	url = {http://ieeexplore.ieee.org/document/1206680/},
	doi = {10.1109/TSP.2003.812831},
	abstract = {The Capon beamformer has better resolution and much better interference rejection capability than the standard (data-independent) beamformer, provided that the array steering vector corresponding to the signal of interest (SOI) is accurately known. However, whenever the knowledge of the SOI steering vector is imprecise (as is often the case in practice), the performance of the Capon beamformer may become worse than that of the standard beamformer. Diagonal loading (including its extended versions) has been a popular approach to improve the robustness of the Capon beamformer. In this paper, we show that a natural extension of the Capon beamformer to the case of uncertain steering vectors also belongs to the class of diagonal loading approaches, but the amount of diagonal loading can be precisely calculated based on the uncertainty set of the steering vector. The proposed robust Capon beamformer can be efficiently computed at a comparable cost with that of the standard Capon beamformer. Its excellent performance for SOI power estimation is demonstrated via a number of numerical examples.},
	language = {en},
	number = {7},
	urldate = {2020-08-25},
	journal = {IEEE Transactions on Signal Processing},
	author = {{Jian Li} and Stoica, P. and {Zhisong Wang}},
	month = jul,
	year = {2003},
	pages = {1702--1715},
	file = {Jian Li et al. - 2003 - On robust capon beamforming and diagonal loading.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UUTR9PSK\\Jian Li et al. - 2003 - On robust capon beamforming and diagonal loading.pdf:application/pdf},
}

@article{brungart_auditory_1999,
	title = {Auditory localization of nearby sources. {Head}-related transfer functions},
	volume = {106},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.427180},
	doi = {10.1121/1.427180},
	language = {en},
	number = {3},
	urldate = {2020-08-31},
	journal = {The Journal of the Acoustical Society of America},
	author = {Brungart, Douglas S. and Rabinowitz, William M.},
	month = sep,
	year = {1999},
	pages = {1465--1479},
	file = {Brungart et Rabinowitz - 1999 - Auditory localization of nearby sources. Head-rela.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\M76HX65N\\Brungart et Rabinowitz - 1999 - Auditory localization of nearby sources. Head-rela.pdf:application/pdf},
}

@book{gay_acoustic_2000,
	address = {Boston, MA},
	title = {Acoustic {Signal} {Processing} for {Telecommunication}},
	isbn = {978-1-4613-4656-2 978-1-4419-8644-3},
	url = {http://link.springer.com/10.1007/978-1-4419-8644-3},
	language = {en},
	urldate = {2020-09-01},
	publisher = {Springer US},
	editor = {Gay, Steven L. and Benesty, Jacob},
	year = {2000},
	doi = {10.1007/978-1-4419-8644-3},
	file = {Gay et Benesty - 2000 - Acoustic Signal Processing for Telecommunication.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZTLXYTGZ\\Gay et Benesty - 2000 - Acoustic Signal Processing for Telecommunication.pdf:application/pdf},
}

@inproceedings{huwel_hearing_2020,
	address = {Barcelona, Spain},
	title = {Hearing aid {Research} {Data} {Set} for {Acoustic} {Environment} {Recognition}},
	isbn = {978-1-5090-6631-5},
	url = {https://ieeexplore.ieee.org/document/9053611/},
	doi = {10.1109/ICASSP40776.2020.9053611},
	language = {en},
	urldate = {2020-09-07},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Huwel, Andreas and Adiloglu, Kamil and Bach, Jorg-Hendrik},
	month = may,
	year = {2020},
	pages = {706--710},
	file = {Huwel et al. - 2020 - Hearing aid Research Data Set for Acoustic Environ.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HB8B342E\\Huwel et al. - 2020 - Hearing aid Research Data Set for Acoustic Environ.pdf:application/pdf},
}

@inproceedings{markovich-golan_performance_2015,
	address = {Brisbane, QLD, Australia},
	title = {Performance {Analysis} of the {Covariance} {Substraction} {Method} for {Relative} {Transfer} {Function} {Estimation} and {Comparison} to the {Covariance} {Whitening} {Method}},
	abstract = {Microphone array processing utilize spatial separation between the desired speaker and interference signal for speech enhancement. The transfer functions (TFs) relating the speaker component at a reference microphone with all other microphones, denoted as the relative TFs (RTFs), play an important role in beamforming design criteria such as minimum variance distortionless response (MVDR) and speech distortion weighted multichannel Wiener ﬁlter (SDW-MWF). Two common methods for estimating the RTF are surveyed here, namely, the covariance subtraction (CS) and the covariance whitening (CW) methods. We analyze the performance of the CS method theoretically and empirically validate the results of the analysis through extensive simulations. Furthermore, empirically comparing the methods performances in various scenarios evidently shows thats the CW method outperforms the CS method.},
	language = {en},
	booktitle = {{ICASSP} 2015 - 2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Markovich-Golan, Shmulik and Gannot, Sharon},
	year = {2015},
	pages = {5},
	file = {Markovich-Golan et Gannot - PERFORMANCE ANALYSIS OF THE COVARIANCE SUBTRACTION.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XA2AKKTB\\Markovich-Golan et Gannot - PERFORMANCE ANALYSIS OF THE COVARIANCE SUBTRACTION.pdf:application/pdf},
}

@inproceedings{heitkaemper_demystifying_2020,
	address = {Barcelona, Spain},
	title = {Demystifying {TasNet}: {A} {Dissecting} {Approach}},
	isbn = {978-1-5090-6631-5},
	shorttitle = {Demystifying {TasNet}},
	url = {https://ieeexplore.ieee.org/document/9052981/},
	doi = {10.1109/ICASSP40776.2020.9052981},
	abstract = {In recent years time domain speech separation has excelled over frequency domain separation in single channel scenarios and noise-free environments. In this paper we dissect the gains of the time-domain audio separation network (TasNet) approach by gradually replacing components of an utterance-level permutation invariant training (u-PIT) based separation system in the frequency domain until the TasNet system is reached, thus blending components of frequency domain approaches with those of time domain approaches. Some of the intermediate variants achieve comparable signal-to-distortion ratio (SDR) gains to TasNet, but retain the advantage of frequency domain processing: compatibility with classic signal processing tools such as frequency-domain beamforming and the human interpretability of the masks. Furthermore, we show that the scale invariant signalto-distortion ratio (si-SDR) criterion used as loss function in TasNet is related to a logarithmic mean square error criterion and that it is this criterion which contributes most reliable to the performance advantage of TasNet. Finally, we critically assess which gains in a noise-free single channel environment generalize to more realistic reverberant conditions.},
	language = {en},
	urldate = {2020-10-12},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Heitkaemper, Jens and Jakobeit, Darius and Boeddeker, Christoph and Drude, Lukas and Haeb-Umbach, Reinhold},
	month = may,
	year = {2020},
	pages = {6359--6363},
	file = {Heitkaemper et al. - 2020 - Demystifying TasNet A Dissecting Approach.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6WS55GSD\\Heitkaemper et al. - 2020 - Demystifying TasNet A Dissecting Approach.pdf:application/pdf},
}

@inproceedings{schonstein_hrtf_2010,
	address = {Sydney, Australia},
	title = {{HRTF} selection for binaural synthesis from a database using morphological parameters},
	abstract = {Auditory virtual environments are becoming increasingly relevant for applications such as teleconferencing, hearing aids, video games, and general immersive listening. To enable high fidelity renderings of the sound scene in such environments, the audio content must be treated with the actual listener's acoustical filters, the so-called head-related transfer functions (HRTFs). The current challenge for general public applications, given the difficulty of measuring HRTFs for a given listener, is to be able to individually generate HRTFs or perform a selection from a database of pre-existing HRTFs, so as to provide the listener an HRTF that enables a listening experience that is as realistic as possible, using for example only data taken from a photo of the listener's ear. A process is described in which a database of 46 measured HRTFs was analysed using various data reduction techniques such as principal component analysis and frequency scaling. A selection of the subjects' most significant morphological parameters was performed using data mining techniques such as support vector machines. This subset of morphological parameters for subjects associated to the HRTF database were then used to perform multiple linear regressions against the reduced dataset of HRTFs in order to predict what might be the listener's preferred HRTFs. The prediction performance was then compared to the results of a perceptual evaluation of the HRTFs from the database using a listening test. The results show that the proposed process was able to predict preferred HRTFs for a listener significantly better than if the HRTFs were chosen at random. The results from the listening test were also used to explore a perceptually relevant frequency range of the HRTF.},
	language = {en},
	booktitle = {Proceedings of 20th {International} {Congress} on {Acoustics}, {ICA} 2010},
	author = {Schönstein, David and Katz, Brian F G},
	month = aug,
	year = {2010},
	pages = {6},
	file = {Schönstein et Katz - 2010 - HRTF selection for binaural synthesis from a datab.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BPDRZYQG\\Schönstein et Katz - 2010 - HRTF selection for binaural synthesis from a datab.pdf:application/pdf},
}

@inproceedings{pu_penalized_2017,
	address = {New Paltz, NY},
	title = {A penalized inequality-constrained minimum variance beamformer with applications in hearing aids},
	isbn = {978-1-5386-1632-1},
	url = {http://ieeexplore.ieee.org/document/8170018/},
	doi = {10.1109/WASPAA.2017.8170018},
	abstract = {A well known challenge in linearly-constrained beamforming is how to suppress multiple interferences when the degrees of freedom (DoF) provided by the array are fewer than the number of sources in the environment. In this paper, we propose a beamformer design to address this challenge. Speciﬁcally, we propose a min-max beamformer design that penalizes the spatial responses on all interferences’ directions. The new design can efﬁciently mitigate the total interference power regardless of whether the number of interfering sources is less than the array DoF or not. We formulate this min-max beamformer design as a convex second-order cone programming (SOCP) and proposed a low complexity iterative algorithm based on alternating direction method of multipliers (ADMM) method to solve it. In the simulation, we compare the proposed beamfomer with the linearly constrained minimum variance (LCMV) beamformer and a recently proposed inequality-constrained minimum variance (ICMV) beamformer [1]. The ability of the proposed beamformer to handle more interferences is demonstrated.},
	language = {en},
	urldate = {2020-10-12},
	booktitle = {2017 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
	publisher = {IEEE},
	author = {Pu, Wenqiang and Xiao, Jinjun and Zhang, Tao and Luo, Zhi-Quan},
	month = oct,
	year = {2017},
	pages = {175--179},
	file = {Pu et al. - 2017 - A penalized inequality-constrained minimum varianc.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6VF4QSUQ\\Pu et al. - 2017 - A penalized inequality-constrained minimum varianc.pdf:application/pdf},
}

@inproceedings{ito_probabilistic_2017,
	address = {New Orleans, LA},
	title = {Probabilistic spatial dictionary based online adaptive beamforming for meeting recognition in noisy and reverberant environments},
	isbn = {978-1-5090-4117-6},
	url = {http://ieeexplore.ieee.org/document/7952242/},
	doi = {10.1109/ICASSP.2017.7952242},
	abstract = {Here we propose online adaptive beamforming for automatic speech recognition (ASR) in meetings in noisy, reverberant environments. The proposed method is based on recently developed mask-based beamforming, in which accurate mask estimation and diarization are paramount. Real-world experiments have shown that mask-based beamforming enables accurate ASR in meetings in small noise and reverberation with a signal-to-noise ratio (SNR) of 15-25 dB and a reverberation time (RT) of 120-350 ms. In this paper, we deal with a more adverse condition: meetings in large noise and reverberation with an SNR of 3-15 dB and an RT of 500 ms. To this end, we exploit a probabilistic spalial dictionary, a dictionary that consists of a pre-trained probability distribution of source location features for each potential speaker location. This dictionary enables us to perform mask estimation and diarization for beamforming accurately, even in the above adverse condition. The proposed method reduced the word error rate (WER) on real meeting data by 54.8 \% relative to our previous beamforming method.},
	language = {en},
	urldate = {2020-10-12},
	booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Ito, Nobutaka and Araki, Shoko and Delcroix, Marc and Nakatani, Tomohiro},
	month = mar,
	year = {2017},
	pages = {681--685},
	file = {Ito et al. - 2017 - Probabilistic spatial dictionary based online adap.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\J68CXG22\\Ito et al. - 2017 - Probabilistic spatial dictionary based online adap.pdf:application/pdf},
}

@article{strubell_energy_2019,
	title = {Energy and {Policy} {Considerations} for {Deep} {Learning} in {NLP}},
	url = {http://arxiv.org/abs/1906.02243},
	abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both ﬁnancially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate ﬁnancial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these ﬁndings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
	language = {en},
	urldate = {2020-10-12},
	journal = {arXiv:1906.02243 [cs]},
	author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.02243},
	keywords = {Computer Science - Computation and Language},
	file = {Strubell et al. - 2019 - Energy and Policy Considerations for Deep Learning.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QKYN6BE8\\Strubell et al. - 2019 - Energy and Policy Considerations for Deep Learning.pdf:application/pdf},
}

@article{lee_54_2008,
	title = {(54) {METHOD} {FORESTIMATING} {PRIORI} {SAP}},
	abstract = {A priori speech absence probability refers to a probability that a speech is not present with respect to a frame and a frequency bin resulting from an input signal. The priori speech absence probability has been regarded as a constant (generally, 0.5) because it is difficult to estimate. However, attempts to estimate the priori speech absence probability have been made since 2002. A novel method for estimating a priori speech absence probability using a statistical model is proposed. The method for estimating a priori speech absence probability obtains a priori speech absence prob ability of input speech data using a local parameter, a global parameter and an average parameter. The local parameter and the global parameter are obtained by determining a smaller value than a first threshold value as 0, determining a greater value than a second threshold value as 1, and applying a raised cosine function to values between the first threshold value and the second threshold value. The average parameter is obtained by a frame average of a posteriori signal-to-noise ratio in log scale.},
	language = {en},
	author = {Lee, Sung Joo and Cl, U S},
	year = {2008},
	pages = {9},
	file = {Lee et Cl - 2008 - (54) METHOD FORESTIMATING PRIORI SAP.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EH2IUK5P\\Lee et Cl - 2008 - (54) METHOD FORESTIMATING PRIORI SAP.pdf:application/pdf},
}

@article{marquardt_interaural_2015,
	title = {Interaural {Coherence} {Preservation} in {Multi}-{Channel} {Wiener} {Filtering}-{Based} {Noise} {Reduction} for {Binaural} {Hearing} {Aids}},
	volume = {23},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/7214220/},
	doi = {10.1109/TASLP.2015.2471096},
	abstract = {Besides noise reduction an important objective of binaural speech enhancement algorithms is the preservation of the binaural cues of all sound sources. To this end, an extension of the binaural multi-channel Wiener ﬁlter (MWF), namely the MWF-ITF, has been proposed, which aims to preserve the Interaural Transfer Function (ITF) of the noise sources. However, the MWF-ITF is well-suited only for directional noise sources but not for, e.g., spatially isotropic noise, whose spatial characteristics cannot be properly described by the ITF but rather by the Interaural Coherence (IC). Hence, another extension of the binaural MWF, namely the MWF-IC, has been recently proposed, which aims to preserve the IC of the noise component. Since for the MWF-IC a substantial tradeoff between noise reduction and IC preservation exists, in this paper we propose a perceptually constrained version of the MWF-IC, where the amount of IC preservation is controlled based on the IC discrimination ability of the human auditory system. In addition, a theoretical analysis of the binaural cue preservation capabilities of the binaural MWF and the MWF-ITF for spatially isotropic noise ﬁelds is provided. Several simulations in diffuse noise scenarios show that the perceptually constrained MWF-IC yields a controllable preservation of the IC without signiﬁcantly degrading the output SNR compared to the binaural MWF and the MWF-ITF. Furthermore, contrary to the binaural MWF and MWF-ITF, the proposed algorithm retains the spatial separation between the output speech and noise components while the binaural cues of the speech component are only slightly distorted, such that the binaural hearing advantage for speech intelligibility can still be exploited.},
	language = {en},
	number = {12},
	urldate = {2020-10-12},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Marquardt, Daniel and Hohmann, Volker and Doclo, Simon},
	month = dec,
	year = {2015},
	pages = {2162--2176},
	file = {Marquardt et al. - 2015 - Interaural Coherence Preservation in Multi-Channel.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5VBSQMSN\\Marquardt et al. - 2015 - Interaural Coherence Preservation in Multi-Channel.pdf:application/pdf},
}

@article{yu_audio_2008,
	title = {Audio {Denoising} by {Time}-{Frequency} {Block} {Thresholding}},
	volume = {56},
	issn = {1053-587X},
	url = {http://ieeexplore.ieee.org/document/4490115/},
	doi = {10.1109/TSP.2007.912893},
	abstract = {Removing noise from audio signals requires a nondiagonal processing of time-frequency coefﬁcients to avoid producing “musical noise.” State of the art algorithms perform a parameterized ﬁltering of spectrogram coefﬁcients with empirically ﬁxed parameters. A block thresholding estimation procedure is introduced, which adjusts all parameters adaptively to signal property by minimizing a Stein estimation of the risk. Numerical experiments demonstrate the performance and robustness of this procedure through objective and subjective evaluations.},
	language = {en},
	number = {5},
	urldate = {2020-10-12},
	journal = {IEEE Transactions on Signal Processing},
	author = {Yu, Guoshen and Mallat, Stéphane and Bacry, Emmanuel},
	month = may,
	year = {2008},
	keywords = {wiener},
	pages = {1830--1839},
	file = {Yu et al. - 2008 - Audio Denoising by Time-Frequency Block Thresholdi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\8D4KLKAU\\Yu et al. - 2008 - Audio Denoising by Time-Frequency Block Thresholdi.pdf:application/pdf},
}

@article{petersen__nodate,
	title = {[ http://matrixcookbook.com ]},
	language = {en},
	author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
	pages = {72},
	file = {Petersen et Pedersen - [ httpmatrixcookbook.com ].pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Q22AC4A5\\Petersen et Pedersen - [ httpmatrixcookbook.com ].pdf:application/pdf},
}

@inproceedings{thiergart_combining_2019,
	address = {Brighton, United Kingdom},
	title = {Combining {Linear} {Spatial} {Filtering} and {Non}-linear {Parametric} {Processing} for {High}-quality {Spatial} {Sound} {Capturing}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8683515/},
	doi = {10.1109/ICASSP.2019.8683515},
	abstract = {Flexible spatial sound capturing and reproduction can be achieved with multiple microphones by using linear spatial ﬁltering or nonlinear parametric processing. The non-linear approaches usually provide a superior spatial resolution compared to the linear approaches but can result in artifacts due to violations of the sound ﬁeld model. In this paper, we combine both approaches to achieve a high robustness against model violations and a high spatial resolution. We assume linear spatial ﬁlters that approximate the spatial responses of the desired output format and compensate remaining deviations with an optimal post ﬁlter. The post ﬁlter is computed such that the proposed approach behaves like a linear system when the spatial ﬁlters achieve the desired spatial response, and scales towards a non-linear system otherwise. Experimental results show that the proposed approach can signiﬁcantly reduce distortions of existing parametric processing schemes especially when a sufﬁciently high number of microphones is available.},
	language = {en},
	urldate = {2020-10-15},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Thiergart, Oliver and Milano, Guendalina and Habets, Emanuel A. P.},
	month = may,
	year = {2019},
	pages = {571--575},
	file = {Thiergart et al. - 2019 - Combining Linear Spatial Filtering and Non-linear .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GK877SY5\\Thiergart et al. - 2019 - Combining Linear Spatial Filtering and Non-linear .pdf:application/pdf},
}

@article{thiergart_informed_2014,
	title = {An {Informed} {Parametric} {Spatial} {Filter} {Based} on {Instantaneous} {Direction}-of-{Arrival} {Estimates}},
	volume = {22},
	abstract = {Extracting desired source signals in noisy and reverberant environments is required in many hands-free communication systems. In practical situations, where the position and number of active sources may be unknown and time-varying, conventional implementations of spatial ﬁlters do not provide sufﬁciently good performance. Recently, informed spatial ﬁlters have been introduced that incorporate almost instantaneous parametric information on the sound ﬁeld, thereby enabling adaptation to new acoustic conditions and moving sources. In this contribution, we propose a spatial ﬁlter which generalizes the recently proposed informed linearly constrained minimum variance ﬁlter and informed minimum mean square error ﬁlter. The proposed ﬁlter uses multiple direction-of-arrival estimates and second-order statistics of the noise and diffuse sound. To determine those statistics, an optimal diffuse power estimator is proposed that outperforms state-of-the-art estimators. Extensive performance evaluation demonstrates the effectiveness of the proposed ﬁlter in dynamic acoustic conditions. For this purpose, we have considered a challenging scenario which consists of quickly moving sound sources during double-talk. The performance of the proposed spatial ﬁlter was evaluated in terms of objective measures including segmental signal-to-reverberation ratio and log spectral distance, and by means of a listening test conﬁrming the objective results.},
	language = {en},
	number = {12},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Thiergart, Oliver and Habets, Emanuël A P},
	year = {2014},
	pages = {15},
	file = {Thiergart et Habets - 2014 - An Informed Parametric Spatial Filter Based on Ins.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NXYKR7JV\\Thiergart et Habets - 2014 - An Informed Parametric Spatial Filter Based on Ins.pdf:application/pdf},
}

@inproceedings{thiergart_low_2016,
	address = {Shanghai},
	title = {A low complexity weighted least squares narrowband {DOA} estimator for arbitrary array geometries},
	isbn = {978-1-4799-9988-0},
	url = {http://ieeexplore.ieee.org/document/7471693/},
	doi = {10.1109/ICASSP.2016.7471693},
	abstract = {An increasing number of spatial ﬁltering approaches requires narrowband direction-of-arrival (DOA) estimates. State-of-the-art (SOA) estimators such as root-MUSIC and ESPRIT are computationally complex and can be used only with speciﬁc array geometries. In this work, a low complexity DOA estimator is proposed that can be applied to arbitrary array geometries. The DOA is estimated by minimizing the weighted error between the observed and expected inter-microphone phase differences. The complexity of the proposed DOA estimator is signiﬁcantly lower compared to that of the SOA estimators while providing a similar performance.},
	language = {en},
	urldate = {2020-10-15},
	booktitle = {2016 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Thiergart, Oliver and Huang, Weilong and Habets, Emanuel A. P.},
	month = mar,
	year = {2016},
	pages = {340--344},
	file = {Thiergart et al. - 2016 - A low complexity weighted least squares narrowband.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\I5EWGXJ7\\Thiergart et al. - 2016 - A low complexity weighted least squares narrowband.pdf:application/pdf},
}

@book{press_numerical_2007,
	edition = {Third},
	title = {Numerical {Recipes}},
	isbn = {978-0-511-33555-6},
	publisher = {Cambridge University Press},
	author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, rian P.},
	year = {2007},
	file = {numerical_recipes.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\I4S3G5IJ\\numerical_recipes.pdf:application/pdf},
}

@article{vincent_performance_2006,
	title = {Performance measurement in blind audio source separation},
	abstract = {In this article, we discuss the evaluation of Blind Audio Source Separation (BASS) algorithms. Depending on the exact application, different distortions can be allowed between an estimated source and the wanted true source. We consider four different sets of such allowed distortions, from time-invariant gains to time-varying ﬁlters. In each case we decompose the estimated source into a true source part plus error terms corresponding to interferences, additive noise and algorithmic artifacts. Then we derive a global performance measure using an energy ratio, plus a separate performance measure for each error term. These measures are computed and discussed on the results of several BASS problems with various difﬁculty levels.},
	language = {en},
	journal = {IEEE Transactions on Audio, Speech and Language Processing, Institute of Electrical and Electronics Engineers},
	author = {Vincent, Emmanuel and Gribonval, Rémi and Févotte, Cédric},
	year = {2006},
	pages = {10},
	file = {Vincent et al. - Performance measurement in blind audio source sepa.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GLAT96UZ\\Vincent et al. - Performance measurement in blind audio source sepa.pdf:application/pdf},
}

@article{guo_low-complexity_2014,
	title = {Low-{Complexity} {Iterative} {Adaptive} {Linearly} {Constrained} {Minimum} {Variance} {Beamformer}},
	volume = {33},
	issn = {0278-081X, 1531-5878},
	url = {http://link.springer.com/10.1007/s00034-013-9668-2},
	doi = {10.1007/s00034-013-9668-2},
	abstract = {The closed-form solution of linearly constrained minimum variance (CFLCMV) suffers heavy computational burden from two-matrix inversion when computing the optimal vector. CF-LCMV is not an adaptive beamformer and performs poorly with low signal-to-interference-plus-noise-ratio (SINR) and small number of snapshots. In this study, we derive a low-complexity iterative adaptive LCMV (IALCMV) algorithm based on conjugate gradient (CG) technique with threefold advantages: ﬁrst, IA-LCMV can remarkably alleviate the complexity of CF-LCMV; second, IA-LCMV can adjust output adaptively with comparable convergence speed.},
	language = {en},
	number = {3},
	urldate = {2020-10-21},
	journal = {Circuits, Systems, and Signal Processing},
	author = {Guo, Xiansheng and Xu, Baogen and Rao, Zhongchu and Wan, Qun and Feng, Zhengming and Shen, Yijiang},
	month = mar,
	year = {2014},
	pages = {987--997},
	file = {Guo et al. - 2014 - Low-Complexity Iterative Adaptive Linearly Constra.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\U9N43UW5\\Guo et al. - 2014 - Low-Complexity Iterative Adaptive Linearly Constra.pdf:application/pdf},
}

@article{markovich-golan_low-complexity_2012,
	title = {Low-{Complexity} {Addition} or {Removal} of {Sensors}/{Constraints} in {LCMV} {Beamformers}},
	volume = {60},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/6094233/},
	doi = {10.1109/TSP.2011.2177829},
	abstract = {We address the application of the linearly constrained minimum variance (LCMV) beamformer in sensor networks. In signal processing applications, it is common to have a redundancy in the number of nodes, fully covering the area of interest. Here we consider suboptimal LCMV beamformers utilizing only a subset of the available sensors for signal enhancement applications. Multiple desired and interfering sources scenarios in multipath environments are considered. We assume that an oracle entity determines the group of sensors participating in the spatial ﬁltering, denoted as the active sensors. The oracle is also responsible for updating the constraints set according to either sensors or sources activity or dynamics. Any update of the active sensors or of the constraints set necessitates recalculation of the beamformer and increases the power consumption. As power consumption is a most valuable resource in sensor networks, it is important to derive efﬁcient update schemes. In this paper, we derive procedures for adding or removing either an active sensor or a constraint from an existing LCMV beamformer. Closed-form, as well as generalized sidelobe canceller (GSC)-form implementations, are derived. These procedures use the previous beamformer to save calculations in the updating process. We analyze the computational burden of the proposed procedures and show that it is much lower than the computational burden of the straightforward calculation of their corresponding beamformers.},
	language = {en},
	number = {3},
	urldate = {2020-10-21},
	journal = {IEEE Transactions on Signal Processing},
	author = {Markovich-Golan, Shmulik and Gannot, Sharon and Cohen, Israel},
	month = mar,
	year = {2012},
	pages = {1205--1214},
	file = {Markovich-Golan et al. - 2012 - Low-Complexity Addition or Removal of SensorsCons.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\I742APXZ\\Markovich-Golan et al. - 2012 - Low-Complexity Addition or Removal of SensorsCons.pdf:application/pdf},
}

@inproceedings{liao_effective_2015,
	address = {New Paltz, NY, USA},
	title = {An effective low complexity binaural beamforming algorithm for hearing aids},
	isbn = {978-1-4799-7450-4},
	url = {http://ieeexplore.ieee.org/document/7336916/},
	doi = {10.1109/WASPAA.2015.7336916},
	abstract = {In this paper, we propose a low complexity beamforming algorithm for binaural hearing aids that achieves a high degree of noise suppression without causing signiﬁcant speech distortion. Speciﬁcally, we incorporate a priori approximate spatial information in the formulation and develop several effective techniques to reduce the communication overhead. For the resulting quadratically constrained beamforming problem, we propose a low complexity algorithm based on the Alternating Direction Method of Multipliers (ADMM) method. This new approach is shown to be computationally much more efﬁcient than the previously proposed binaural beamforming algorithms. Through experiments on the real-world recording, the effectiveness of the binaural hearing aid design and the corresponding low complexity implementation are validated.},
	language = {en},
	urldate = {2020-10-21},
	booktitle = {2015 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
	publisher = {IEEE},
	author = {Liao, Wei-Cheng and Luo, Zhi-Quan and Merks, Ivo and Zhang, Tao},
	month = oct,
	year = {2015},
	pages = {1--5},
	file = {Liao et al. - 2015 - An effective low complexity binaural beamforming a.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XYFNWJTM\\Liao et al. - 2015 - An effective low complexity binaural beamforming a.pdf:application/pdf},
}

@inproceedings{llave_localization_2020,
	address = {Auckland, New Zealand},
	title = {Localization {Cues} {Preservation} in {Hearing} {Aids} by {Combining} {Noise} {Reduction} and {Dynamic} {Range} {Compression}},
	isbn = {978-1-72818-130-1},
	abstract = {Dynamic range compression (DRC) and noise reduction algorithms are commonly used in hearing aids. They are known to have opposite objectives concerning the Signal-to-Noise Ratio (SNR) and to affect negatively the localization performance. Yet, the study of their interaction received few attention. In this work, we improve an existing combined approach of DRC and noise reduction to bridge the gap between the algorithms proposed independently in their respective communities. The proposed solution is then compared to state-of-the-art algorithms thanks to objective criteria assessing the spatial ﬁdelity preservation, the SNR improvement and the output dynamic range reduction. Experimental results show that the standard serial concatenation of noise reduction and DRC stages is unable to improve the SNR and preserve the noise component acoustic characteristics. They suggest that the proposed design restores the noise localization cues and manages to improve the output SNR.},
	language = {en},
	booktitle = {12th {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	publisher = {IEEE},
	author = {Llave, Adrien and Leglaive, Simon and Seguier, Renaud},
	month = dec,
	year = {2020},
	pages = {9},
	file = {Llave et al. - Localization cues preservation in hearing aids by .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4D8YNTAH\\Llave et al. - Localization cues preservation in hearing aids by .pdf:application/pdf},
}

@article{rhebergen_dynamic_2009,
	title = {The dynamic range of speech, compression, and its effect on the speech reception threshold in stationary and interrupted noise},
	volume = {126},
	doi = {10.1121/1.3257225},
	language = {en},
	number = {6},
	journal = {The Journal of Acoustical Society of America},
	author = {Rhebergen, Koenraad S and Versfeld, Niek J and Dreschler, Wouter A},
	year = {2009},
	pages = {10},
	file = {Rhebergen et al. - 2009 - The dynamic range of speech, compression, and its .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LAIUKFRR\\Rhebergen et al. - 2009 - The dynamic range of speech, compression, and its .pdf:application/pdf},
}

@phdthesis{carbajal_apprentissage_2020,
	title = {Apprentissage profond bout-en-bout pour le rehaussement de la parole},
	school = {Université de Lorraine},
	author = {Carbajal, Guillaume},
	month = apr,
	year = {2020},
	file = {DDOC_T_2020_0017_CARBAJAL.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3TAKVP9G\\DDOC_T_2020_0017_CARBAJAL.pdf:application/pdf},
}

@article{zohourian_binaural_2019,
	title = {Binaural {Direct}-to-{Reverberant} {Energy} {Ratio} and {Speaker} {Distance} {Estimation}},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	author = {Zohourian, Mehdi and Martin, Rainer},
	year = {2019},
	file = {zohourian2019.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\97DHTAVD\\zohourian2019.pdf:application/pdf},
}

@article{rhebergen_characterizing_2017,
	title = {Characterizing {Speech} {Intelligibility} in {Noise} {After} {Wide} {Dynamic} {Range} {Compression}:},
	volume = {38},
	issn = {0196-0202},
	shorttitle = {Characterizing {Speech} {Intelligibility} in {Noise} {After} {Wide} {Dynamic} {Range} {Compression}},
	url = {http://journals.lww.com/00003446-201703000-00008},
	doi = {10.1097/AUD.0000000000000369},
	abstract = {Objectives: The effects of nonlinear signal processing on speech intelligibility in noise are difficult to evaluate. Often, the effects are examined by comparing speech intelligibility scores with and without processing measured at fixed signal to noise ratios (SNRs) or by comparing the adaptive measured speech reception thresholds corresponding to 50\% intelligibility (SRT50) with and without processing. These outcome measures might not be optimal. Measuring at fixed SNRs can be affected by ceiling or floor effects, because the range of relevant SNRs is not know in advance. The SRT50 is less time consuming, has a fixed performance level (i.e., 50\% correct), but the SRT50 could give a limited view, because we hypothesize that the effect of most nonlinear signal processing algorithms at the SRT50 cannot be generalized to other points of the psychometric function. Design: In this article, we tested the value of estimating the entire psychometric function. We studied the effect of wide dynamic range compression (WDRC) on speech intelligibility in stationary, and interrupted speech-shaped noise in normal-hearing subjects, using a fast methodbased local linear fitting approach and by two adaptive procedures.
Results: The measured performance differences for conditions with and without WDRC for the psychometric functions in stationary noise and interrupted speech-shaped noise show that the effects of WDRC on speech intelligibility are SNR dependent.
Conclusions: We conclude that favorable and unfavorable effects of WDRC on speech intelligibility can be missed if the results are presented in terms of SRT50 values only.},
	language = {en},
	number = {2},
	urldate = {2020-12-17},
	journal = {Ear and Hearing},
	author = {Rhebergen, Koenraad S. and Maalderink, Thijs H. and Dreschler, Wouter A.},
	year = {2017},
	pages = {194--204},
	file = {Rhebergen et al. - 2017 - Characterizing Speech Intelligibility in Noise Aft.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HD9LZLY2\\Rhebergen et al. - 2017 - Characterizing Speech Intelligibility in Noise Aft.pdf:application/pdf},
}

@article{cox_distribution_1988,
	title = {Distribution of short‐term rms levels in conversational speech},
	volume = {84},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.396697},
	doi = {10.1121/1.396697},
	language = {en},
	number = {3},
	urldate = {2021-02-15},
	journal = {The Journal of the Acoustical Society of America},
	author = {Cox, Robyn M. and Matesich, Joseph S. and Moore, Jeffrey N.},
	month = sep,
	year = {1988},
	pages = {1100--1104},
	file = {Cox et al. - 1988 - Distribution of short‐term rms levels in conversat.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\I4Y7LD6F\\Cox et al. - 1988 - Distribution of short‐term rms levels in conversat.pdf:application/pdf},
}

@article{fullgrabe_age-group_2015,
	title = {Age-group differences in speech identification despite matched audiometrically normal hearing: contributions from auditory temporal processing and cognition},
	volume = {6},
	abstract = {Hearing loss with increasing age adversely affects the ability to understand speech, an effect that results partly from reduced audibility. The aims of this study were to establish whether aging reduces speech intelligibility for listeners with normal audiograms, and, if so, to assess the relative contributions of auditory temporal and cognitive processing. Twenty-one older normal-hearing (ONH; 60–79 years) participants with bilateral audiometric thresholds ≤ 20 dB HL at 0.125–6 kHz were matched to nine young (YNH; 18–27 years) participants in terms of mean audiograms, years of education, and performance IQ. Measures included: (1) identiﬁcation of consonants in quiet and in noise that was unmodulated or modulated at 5 or 80 Hz; (2) identiﬁcation of sentences in quiet and in co-located or spatially separated two-talker babble; (3) detection of modulation of the temporal envelope (TE) at frequencies 5–180 Hz; (4) monaural and binaural sensitivity to temporal ﬁne structure (TFS); (5) various cognitive tests. Speech identiﬁcation was worse for ONH than YNH participants in all types of background. This deﬁcit was not reﬂected in self-ratings of hearing ability. Modulation masking release (the improvement in speech identiﬁcation obtained by amplitude modulating a noise background) and spatial masking release (the beneﬁt obtained from spatially separating masker and target speech) were not affected by age. Sensitivity to TE and TFS was lower for ONH than YNH participants, and was correlated positively with speech-in-noise (SiN) identiﬁcation. Many cognitive abilities were lower for ONH than YNH participants, and generally were correlated positively with SiN identiﬁcation scores. The best predictors of the intelligibility of SiN were composite measures of cognition and TFS sensitivity. These results suggest that declines in speech perception in older persons are partly caused by cognitive and perceptual changes separate from age-related changes in audiometric sensitivity.},
	language = {en},
	journal = {Frontiers in Aging Neuroscience},
	author = {Füllgrabe, Christian},
	month = jan,
	year = {2015},
	pages = {25},
	file = {Füllgrabe - Age-group differences in speech identification des.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WU54X7VV\\Füllgrabe - Age-group differences in speech identification des.pdf:application/pdf},
}

@article{stelmachowicz_effect_2001,
	title = {Effect of stimulus bandwidth on the perception of /s/ in normal- and hearing-impaired children and adults},
	volume = {110},
	language = {en},
	number = {4},
	journal = {J. Acoust. Soc. Am.},
	author = {Stelmachowicz, Patricia G and Pittman, Andrea L and Hoover, Brenda M and Lewis, Dawna E},
	year = {2001},
	pages = {8},
	file = {Stelmachowicz et al. - 2001 - Effect of stimulus bandwidth on the perception of .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3NP3PAWG\\Stelmachowicz et al. - 2001 - Effect of stimulus bandwidth on the perception of .pdf:application/pdf},
}

@techreport{noauthor_itu-t_1996,
	address = {Geneva, Switzerland},
	title = {{ITU}-{T}: {Recommendation}: {P}.800. {Methods} for subjective determination of transmission quality},
	institution = {International Telecommunications Union},
	year = {1996},
	file = {T-REC-P.800-199608-I!!PDF-F.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\M7W2TXCM\\T-REC-P.800-199608-I!!PDF-F.pdf:application/pdf},
}

@article{brons_effects_2014,
	title = {Effects of {Noise} {Reduction} on {Speech} {Intelligibility}, {Perceived} {Listening} {Effort}, and {Personal} {Preference} in {Hearing}-{Impaired} {Listeners}},
	volume = {18},
	issn = {2331-2165, 2331-2165},
	url = {http://journals.sagepub.com/doi/10.1177/2331216514553924},
	doi = {10.1177/2331216514553924},
	abstract = {This study evaluates the perceptual effects of single-microphone noise reduction in hearing aids. Twenty subjects with moderate sensorineural hearing loss listened to speech in babble noise processed via noise reduction from three different linearly fitted hearing aids. Subjects performed (a) speech-intelligibility tests, (b) listening-effort ratings, and (c) pairedcomparison ratings on noise annoyance, speech naturalness, and overall preference. The perceptual effects of noise reduction differ between hearing aids. The results agree well with those of normal-hearing listeners in a previous study. None of the noise-reduction algorithms improved speech intelligibility, but all reduced the annoyance of noise. The noise reduction that scored best with respect to noise annoyance and preference had the worst intelligibility scores. The trade-off between intelligibility and listening comfort shows that preference measurements might be useful in addition to intelligibility measurements in the selection of noise reduction. Additionally, this trade-off should be taken into consideration to create realistic expectations in hearing-aid users.},
	language = {en},
	urldate = {2021-01-26},
	journal = {Trends in Hearing},
	author = {Brons, Inge and Houben, Rolph and Dreschler, Wouter A.},
	month = oct,
	year = {2014},
	pages = {10},
	file = {Brons et al. - 2014 - Effects of Noise Reduction on Speech Intelligibili.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IUFBPWM9\\Brons et al. - 2014 - Effects of Noise Reduction on Speech Intelligibili.pdf:application/pdf},
}

@article{zwicker_subdivision_1961,
	title = {Subdivision of the {Audible} {Frequency} {Range} into {Critical} {Bands} ({Frequenzgruppen})},
	volume = {33},
	language = {en},
	number = {2},
	journal = {The Journal of Acoustical Society of America},
	author = {Zwicker, E},
	month = feb,
	year = {1961},
	pages = {1},
	file = {Zwicker - Subdivision of the Audible Frequency Range into Cr.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CZEX3UY5\\Zwicker - Subdivision of the Audible Frequency Range into Cr.pdf:application/pdf},
}

@article{schoeffler_webmushra_2018,
	title = {{webMUSHRA} — {A} {Comprehensive} {Framework} for {Web}-based {Listening} {Tests}},
	volume = {6},
	issn = {2049-9647},
	url = {http://openresearchsoftware.metajnl.com/articles/10.5334/jors.187/},
	doi = {10.5334/jors.187},
	language = {en},
	urldate = {2021-01-20},
	journal = {Journal of Open Research Software},
	author = {Schoeffler, Michael and Bartoschek, Sarah and Stöter, Fabian-Robert and Roess, Marlene and Westphal, Susanne and Edler, Bernd and Herre, Jürgen},
	month = feb,
	year = {2018},
	pages = {8},
	file = {Schoeffler et al. - 2018 - webMUSHRA — A Comprehensive Framework for Web-base.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\FZ4CQNPM\\Schoeffler et al. - 2018 - webMUSHRA — A Comprehensive Framework for Web-base.pdf:application/pdf},
}

@article{adali_complex-valued_2011,
	title = {Complex-{Valued} {Signal} {Processing}: {The} {Proper} {Way} to {Deal} {With} {Impropriety}},
	volume = {59},
	issn = {1053-587X, 1941-0476},
	shorttitle = {Complex-{Valued} {Signal} {Processing}},
	url = {http://ieeexplore.ieee.org/document/5961645/},
	doi = {10.1109/TSP.2011.2162954},
	abstract = {Complex-valued signals occur in many areas of science and engineering and are thus of fundamental interest. In the past, it has often been assumed, usually implicitly, that complex random signals are proper or circular. A proper complex random variable is uncorrelated with its complex conjugate, and a circular complex random variable has a probability distribution that is invariant under rotation in the complex plane. While these assumptions are convenient because they simplify computations, there are many cases where proper and circular random signals are very poor models of the underlying physics. When taking impropriety and noncircularity into account, the right type of processing can provide signiﬁcant performance gains. There are two key ingredients in the statistical signal processing of complex-valued data: 1) utilizing the complete statistical characterization of complex-valued random signals; and 2) the optimization of real-valued cost functions with respect to complex parameters. In this overview article, we review the necessary tools, among which are widely linear transformations, augmented statistical descriptions, and Wirtinger calculus. We also present some selected recent developments in the ﬁeld of complex-valued signal processing, addressing the topics of model selection, ﬁltering, and source separation.},
	language = {en},
	number = {11},
	urldate = {2021-01-18},
	journal = {IEEE Transactions on Signal Processing},
	author = {Adali, T. and Schreier, P. J. and Scharf, L. L.},
	month = nov,
	year = {2011},
	pages = {5101--5125},
	file = {Adali et al. - 2011 - Complex-Valued Signal Processing The Proper Way t.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PU2KRWXM\\Adali et al. - 2011 - Complex-Valued Signal Processing The Proper Way t.pdf:application/pdf},
}

@article{souza_multichannel_1998,
	title = {Multichannel {Compression}, {Temporal} {Cues}, and {Audibility}},
	volume = {41},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jslhr.4102.315},
	doi = {10.1044/jslhr.4102.315},
	abstract = {Department of Speech Pathology and Audiology University of Iowa, Iowa City Although multichannel compression systems are quickly becoming integral components of programmable hearing aids, research results have not consistently demonstrated their benefit over conventional amplification. The present study examined two confounding factors that may have contributed to this inconsistency in results: alteration of temporal information and audibility of speech cues. Recognition of linearly amplified and multichannel-compressed speech was measured for listeners with mild-to-severe sensorineural hearing loss and for a control group of listeners with normal hearing. In addition to the standard speech signal, which provided both temporal and spectral information, the listener’s ability to use temporal information in a multichannel compressed signal was directly tested using a signal-correlated noise (SCN) stimulus. This stimulus consisted of a time-varying speech envelope modulating a two-channel noise carrier. It preserved temporal cues but provided minimal spectral information. For each stimulus condition, short-term level measurements were used to determine the range of audible speech. Multichannel compression improved speech recognition under conditions where superior audibility was provided by the twochannel compression system over linear amplification. When audibility of both linearly amplified and multichannel-compressed speech was maximized, the multichannel compression had no significant effect on speech recognition score for speech containing both temporal and spectral cues. However, results for the SCN stimuli show that more extreme amounts of multichannel compression can reduce use of temporal information.},
	language = {en},
	number = {2},
	urldate = {2021-02-15},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Souza, Pamela E. and Turner, Christopher W.},
	month = apr,
	year = {1998},
	pages = {315--326},
	file = {Souza et Turner - 1998 - Multichannel Compression, Temporal Cues, and Audib.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7KW7LU2Q\\Souza et Turner - 1998 - Multichannel Compression, Temporal Cues, and Audib.pdf:application/pdf},
}

@article{jerlvall_influence_1978,
	title = {The influence of attack time and release time on speech intelligibility. {A} study of the effects of {AGC} on normal hearing and hearing impaired subjects.},
	volume = {6},
	journal = {Scandinavian audiology. Supplementum},
	author = {Jerlvall, LB and Lindblad, AC},
	year = {1978},
	pages = {341--353},
}

@article{sun_spherical_2019,
	title = {Spherical {Reverse} {Beamforming} for {Sound} {Source} {Localization} {Based} on the {Inverse} {Method}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/11/2618},
	doi = {10.3390/s19112618},
	abstract = {A spherical array is not limited to providing an acoustic map in all directions by the azimuth of the array. In this paper, spherical reverse beamforming for sound source localization based on spherical harmonic beamforming and the principle of sound ﬁeld reconstruction is proposed in order to output a sharper scanning beam. It is assumed that there is an imaginary sound source at each scan point, and the acoustic map of a spherical array to the actual sound source is regarded as the combination of all of the imaginary sound sources. Sound source localization can be realized by calculating the contribution of each imaginary sound source to the sound ﬁeld. Also in this work, the non-convex constrained optimization problem is established using p-norm. Combined with the norm method, the sparse solution of the imaginary sources is obtained through iterative weighted techniques, and the resolution of sound source localization is improved signiﬁcantly. The performance of this method is investigated in comparison to conventional spherical beamforming. The numerical results show that the proposed method can achieve higher resolution for the localization of sound sources without being limited by the frequency and array aperture, and has a stronger ability to suppress ﬂuctuations in background noise.},
	language = {en},
	number = {11},
	urldate = {2021-02-18},
	journal = {Sensors},
	author = {Sun, Chao and Liu, Yuechan},
	month = jun,
	year = {2019},
	pages = {2618},
	file = {Sun et Liu - 2019 - Spherical Reverse Beamforming for Sound Source Loc.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5K5DLRQG\\Sun et Liu - 2019 - Spherical Reverse Beamforming for Sound Source Loc.pdf:application/pdf},
}

@article{mills_minimum_1958,
	title = {On the {Minimum} {Audible} {Angle}},
	volume = {30},
	number = {237},
	journal = {The Journal of the Acoustical Society of America},
	author = {Mills, A. W.},
	year = {1958},
}

@article{keidser_effect_2009,
	title = {The effect of frequency-dependent microphone directionality on horizontal localization performance in hearing-aid users},
	volume = {48},
	doi = {10.3109/14992020903036357},
	number = {11},
	journal = {International Journal of Audiology},
	author = {Keidser, Gitte and O'Brien, Anna and Hain, Jens-Uwe and McLelland, Margot and Yeend, Ingrid},
	year = {2009},
	pages = {789--803},
}

@article{vaillancourt_evaluation_2011,
	title = {Evaluation of {Auditory} {Functions} for {Royal} {Canadian} {Mounted} {Police} {Officers}},
	volume = {22},
	issn = {1050-0545, 2157-3107},
	url = {http://www.thieme-connect.de/DOI/DOI?10.3766/jaaa.22.6.2},
	doi = {10.3766/jaaa.22.6.2},
	abstract = {Background: Auditory ﬁtness for duty (AFFD) testing is an important element in an assessment of workers’ ability to perform job tasks safely and effectively. Functional hearing is particularly critical to job performance in law enforcement. Most often, assessment is based on pure-tone detection thresholds; however, its validity can be questioned and challenged in court. In an attempt to move beyond the pure-tone audiogram, some organizations like the Royal Canadian Mounted Police (RCMP) are incorporating additional testing to supplement audiometric data in their AFFD protocols, such as measurements of speech recognition in quiet and/or in noise, and sound localization.
Purpose: This article reports on the assessment of RCMP ofﬁcers wearing hearing aids in speech recognition and sound localization tasks. The purpose was to quantify individual performance in different domains of hearing identiﬁed as necessary components of ﬁtness for duty, and to document the type of hearing aids prescribed in the ﬁeld and their beneﬁt for functional hearing. The data are to help RCMP in making more informed decisions regarding AFFD in ofﬁcers wearing hearing aids. Research Design: The proposed new AFFD protocol included unaided and aided measures of speech recognition in quiet and in noise using the Hearing in Noise Test (HINT) and sound localization in the left/ right (L/R) and front/back (F/B) horizontal planes. Sixty-four ofﬁcers were identiﬁed and selected by the RCMP to take part in this study on the basis of hearing thresholds exceeding current audiometrically based criteria. This article reports the results of 57 ofﬁcers wearing hearing aids.
Results: Based on individual results, 49\% of ofﬁcers were reclassiﬁed from nonoperational status to operational with limitations on ﬁne hearing duties, given their unaided and/or aided performance. Group data revealed that hearing aids (1) improved speech recognition thresholds on the HINT, the effects being most prominent in Quiet and in conditions of spatial separation between target and noise (Noise Right and Noise Left) and least considerable in Noise Front; (2) neither signiﬁcantly improved nor impeded L/R localization; and (3) substantially increased F/B errors in localization in a number of cases. Additional analyses also pointed to the poor ability of threshold data to predict functional abilities for speech in noise (r250.26 to 0.33) and sound localization (r2 50.03 to 0.28). Only speech in quiet (r25 0.68 to 0.85) is predicted adequately from threshold data.
Conclusions: Combined with previous ﬁndings, results indicate that the use of hearing aids can considerably affect F/B localization abilities in a number of individuals. Moreover, speech understanding in noise and sound localization abilities were poorly predicted from pure-tone thresholds, demonstrating the need to speciﬁcally test these abilities, both unaided and aided, when assessing AFFD. Finally, further work is needed to develop empirically based hearing criteria for the RCMP and identify best practices in hearing aid ﬁttings for optimal functional hearing abilities.},
	language = {en},
	number = {06},
	urldate = {2021-02-24},
	journal = {Journal of the American Academy of Audiology},
	author = {Vaillancourt, Véronique and Laroche, Chantal and Giguère, Christian and Beaulieu, Marc-André and Legault, Jean-Pierre},
	month = jun,
	year = {2011},
	pages = {313--331},
	file = {Vaillancourt et al. - 2011 - Evaluation of Auditory Functions for Royal Canadia.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TIXCJBQB\\Vaillancourt et al. - 2011 - Evaluation of Auditory Functions for Royal Canadia.pdf:application/pdf},
}

@article{drennan_localization_2005,
	title = {Localization and {Speech}-{Identification} {Ability} of {Hearing}-{Impaired} {Listeners} {Using} {Phase}-{Preserving} {Amplification}},
	volume = {26},
	doi = {10.1097/01.aud.0000179690.30137.21},
	number = {5},
	journal = {Ear and Hearing},
	author = {Drennan, Ward R. and Gatehouse, Stuart and Howell, Patrick and Tasell, Dianne Van and Lund, Steven},
	month = oct,
	year = {2005},
	pages = {461--472},
}

@article{blauert_sound_1969,
	title = {Sound {Localization} in the {Median} {Plane}},
	volume = {22},
	abstract = {Psychoacoustic measurements with observers who were stimulated at both ears with identical narrow band signals yielded the following results: The sound sensations of the observers were localized in the median plane. The direction of the sound sensation is a function of frequency only and does not depend on the angle of incidence.},
	language = {en},
	journal = {Acta Acustica United with Acustica},
	author = {Blauert, J},
	year = {1969},
	pages = {205--213},
	file = {Blauert - Sound Localization in the Median Plane.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\DTCQYL4W\\Blauert - Sound Localization in the Median Plane.pdf:application/pdf},
}

@article{zakarauskas_computational_1993,
	title = {A computational theory of spectral cue localization},
	volume = {94},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.408160},
	doi = {10.1121/1.408160},
	language = {en},
	number = {3},
	urldate = {2021-02-25},
	journal = {The Journal of the Acoustical Society of America},
	author = {Zakarauskas, Pierre and Cynader, Max S.},
	month = sep,
	year = {1993},
	pages = {1323--1331},
	file = {Zakarauskas et Cynader - 1993 - A computational theory of spectral cue localizatio.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IYD4LCKD\\Zakarauskas et Cynader - 1993 - A computational theory of spectral cue localizatio.pdf:application/pdf},
}

@inproceedings{yost_history_2017,
	address = {Boston, Massachusetts},
	title = {History of sound source localization: 1850-1950},
	shorttitle = {History of sound source localization},
	url = {http://asa.scitation.org/doi/abs/10.1121/2.0000529},
	doi = {10.1121/2.0000529},
	language = {en},
	urldate = {2021-02-25},
	author = {Yost, William A.},
	year = {2017},
	pages = {050002},
	file = {Yost - 2017 - History of sound source localization 1850-1950.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\DEMUHXP9\\Yost - 2017 - History of sound source localization 1850-1950.pdf:application/pdf},
}

@article{batteau_role_1967,
	title = {The {Role} of the {Pinna} in {Human} {Localization}},
	volume = {168},
	language = {en},
	journal = {Proc. R. Soc. London. Series B, Biological Sciences},
	author = {Batteau, Dwight W},
	month = aug,
	year = {1967},
	pages = {158--180},
	file = {Batteau - 7 THE ROLE OF THE PINNA IN HUMAN LOCALIZATION.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HPXS983S\\Batteau - 7 THE ROLE OF THE PINNA IN HUMAN LOCALIZATION.pdf:application/pdf},
}

@article{zahorik_auditory_2005,
	title = {Auditory {Distance} {Perception} in {Humans}: {A} {Summary} of {Past} and {Present} {Research}},
	volume = {91},
	abstract = {Although auditory distance perception is a critical component of spatial hearing, it has received substantially less scientiﬁc attention than the directional aspects of auditory localization. Here we summarize current knowledge on auditory distance perception, with special emphasis on recent research results. The summary will be structured around three central questions. 1. How accurately can humans estimate the distances of stationary sound sources? We show that this psychophysical relationship is well approximated by a compressive power function, which suggests that listeners systematically underestimate distances to faraway sound sources. 2. What determines perceived sound source distance? We examine the various acoustical and non-acoustical factors thought to contribute to source distance percepts, and summarize the psychophysical literature relevant to each factor. 3. What are the neural correlates to perceived sound source distance? Recent evidence points to the role of areas within right temporal cortex in auditory distance perception, as well as in other spatial tasks in different sensory modalities.},
	language = {en},
	journal = {Acta Acustica united with Acustica},
	author = {Zahorik, Pavel},
	year = {2005},
	pages = {12},
	file = {Zahorik - 2005 - Auditory Distance Perception in Humans A Summary .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\524XFS2I\\Zahorik - 2005 - Auditory Distance Perception in Humans A Summary .pdf:application/pdf},
}

@article{zahorik_direct--reverberant_2002,
	title = {Direct-to-reverberant energy ratio sensitivity},
	volume = {112},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1506692},
	doi = {10.1121/1.1506692},
	language = {en},
	number = {5},
	urldate = {2021-02-25},
	journal = {The Journal of the Acoustical Society of America},
	author = {Zahorik, Pavel},
	month = nov,
	year = {2002},
	pages = {2110--2117},
	file = {Zahorik - 2002 - Direct-to-reverberant energy ratio sensitivity.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VFDR5ZDS\\Zahorik - 2002 - Direct-to-reverberant energy ratio sensitivity.pdf:application/pdf},
}

@article{brand_adaptive_2002,
	title = {An adaptive procedure for categorical loudness scaling},
	volume = {112},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1502902},
	doi = {10.1121/1.1502902},
	language = {en},
	number = {4},
	urldate = {2021-02-26},
	journal = {The Journal of the Acoustical Society of America},
	author = {Brand, Thomas and Hohmann, Volker},
	month = oct,
	year = {2002},
	pages = {1597--1604},
	file = {Brand et Hohmann - 2002 - An adaptive procedure for categorical loudness sca.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QP755W8R\\Brand et Hohmann - 2002 - An adaptive procedure for categorical loudness sca.pdf:application/pdf},
}

@article{steinberg_dependence_1937,
	title = {The {Dependence} of {Hearing} {Impairment} on {Sound} {Intensity}},
	volume = {9},
	language = {en},
	number = {11},
	journal = {The Journal of the Acoustical Society of America},
	author = {Steinberg, John C and Gardner, Mark B},
	year = {1937},
	pages = {11--23},
	file = {Steinberg et Gardner - The Dependence of Hearing Impairment on Sound Inte.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3TZY47BT\\Steinberg et Gardner - The Dependence of Hearing Impairment on Sound Inte.pdf:application/pdf},
}

@article{elberling_loudness_1999,
	title = {Loudness {Scaling} {Revisited}},
	volume = {10},
	abstract = {The present work was undertaken in an attempt to evaluate whether it is reasonable to expect that categorical loudness scaling can provide useful information for nonlinear hearing aid fitting . Normative data from seven scaling procedures show that the individual procedures relate the perceptual categories differently to sound level and with a substantial betweensubject variance . Hearing-impaired data from four studies demonstrate that the inverse slope of the loudness function varies linearly with hearing loss and with a constant variance . In relation to hearing aid fitting, the slope can, in most cases, be predicted from the hearing loss with an accuracy within the range of a normal finetuning . For the fitting of nonlinear hearing aids, the statistical properties of both normal and impaired loudness functions are equally important. The present analysis strongly suggests that categorical loudness scaling cannot, in general, provide significant information for the fitting process.},
	language = {en},
	number = {5},
	journal = {Journal of the American Academy of Audiology},
	author = {Elberling, Claus},
	year = {1999},
	pages = {13},
	file = {Elberling - 1999 - Loudness Scaling Revisited.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\X6JMLE9W\\Elberling - 1999 - Loudness Scaling Revisited.pdf:application/pdf},
}

@article{studebaker_rationalized_1985,
	title = {A "{Rationalized}" {Arcsine} {Transform}},
	volume = {28},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jshr.2803.455},
	doi = {10.1044/jshr.2803.455},
	abstract = {Arcsine or angular transformations have been used for many years to transform proportions to make them more suitable for statistical analysis. A problem with such transformations is that the arcsines do not bear any obvious relationship to the original proportions. For this reason, results expressed in arcsine units are difficult to interpret. In this paper a simple linear transformation of the arcsine transform is suggested. This transformation produces values that are numerically close to the original percentage values over most of the percentage range while retaining all of the desirable statistical properties of the arcsine transform.},
	language = {en},
	number = {3},
	urldate = {2021-02-26},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Studebaker, Gerald A.},
	month = sep,
	year = {1985},
	pages = {455--462},
	file = {Studebaker - 1985 - A Rationalized Arcsine Transform.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\G5DRCSBP\\Studebaker - 1985 - A Rationalized Arcsine Transform.pdf:application/pdf},
}

@article{sherbecoe_supplementary_2004,
	title = {Supplementary formulas and tables for calculating and interconverting speech recognition scores in transformed arcsine units},
	volume = {43},
	abstract = {Formulas that convert speech recognition scores, in percent or proportions, into units based on the arcsine transform have been described previously. This report reviews that work and presents various supplementary equations and tables for calculating and interconverting the proposed units. The relative merits of these data and their application to scores from closed-set tests are also discussed.},
	language = {en},
	number = {8},
	journal = {International Journal of Audiology},
	author = {Sherbecoe, Robert L and Studebaker, Gerald A},
	year = {2004},
	pages = {7},
	file = {Sherbecoe et Studebaker - Supplementary formulas and tables for calculating .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IU7ZW79N\\Sherbecoe et Studebaker - Supplementary formulas and tables for calculating .pdf:application/pdf},
}

@article{hagerman_method_2004,
	title = {A {Method} to {Measure} the {Effect} of {Noise} {Reduction} {Algorithms} {Using} {Simultaneous} {Speech} and {Noise}},
	volume = {90},
	number = {2},
	journal = {Acta Acustica united with Acustica},
	author = {Hagerman, Björn and Olofsson, Åke},
	month = mar,
	year = {2004},
	pages = {356--361},
}

@article{jorgensen_multi-resolution_2013,
	title = {A multi-resolution envelope-power based model for speech intelligibility},
	volume = {134},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4807563},
	doi = {10.1121/1.4807563},
	language = {en},
	number = {1},
	urldate = {2021-03-08},
	journal = {The Journal of the Acoustical Society of America},
	author = {Jørgensen, Søren and Ewert, Stephan D. and Dau, Torsten},
	month = jul,
	year = {2013},
	pages = {436--446},
	file = {Jørgensen et al. - 2013 - A multi-resolution envelope-power based model for .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3LF7KGW3\\Jørgensen et al. - 2013 - A multi-resolution envelope-power based model for .pdf:application/pdf},
}

@article{doclo_gsvd-based_2002,
	title = {{GSVD}-based optimal filtering for single and multimicrophone speech enhancement},
	volume = {50},
	issn = {1053-587X},
	url = {http://ieeexplore.ieee.org/document/1025586/},
	doi = {10.1109/TSP.2002.801937},
	abstract = {In this paper, a generalized singular value decomposition (GSVD) based algorithm is proposed for enhancing multimicrophone speech signals degraded by additive colored noise. This GSVD-based multimicrophone algorithm can be considered to be an extension of the single-microphone signal subspace algorithms for enhancing noisy speech signals and amounts to a specific optimal filtering problem when the desired response signal cannot be observed.},
	language = {en},
	number = {9},
	urldate = {2021-03-15},
	journal = {IEEE Transactions on Signal Processing},
	author = {Doclo, S. and Moonen, M.},
	month = sep,
	year = {2002},
	pages = {2230--2244},
	file = {Doclo et Moonen - 2002 - GSVD-based optimal filtering for single and multim.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AUYX6RIG\\Doclo et Moonen - 2002 - GSVD-based optimal filtering for single and multim.pdf:application/pdf},
}

@article{carlile_plastic_2014,
	title = {The plastic ear and perceptual relearning in auditory spatial perception},
	volume = {8},
	issn = {1662-453X},
	url = {http://journal.frontiersin.org/article/10.3389/fnins.2014.00237/abstract},
	doi = {10.3389/fnins.2014.00237},
	abstract = {The auditory system of adult listeners has been shown to accommodate to altered spectral cues to sound location which presumably provides the basis for recalibration to changes in the shape of the ear over a life time. Here we review the role of auditory and non-auditory inputs to the perception of sound location and consider a range of recent experiments looking at the role of non-auditory inputs in the process of accommodation to these altered spectral cues. A number of studies have used small ear molds to modify the spectral cues that result in signiﬁcant degradation in localization performance. Following chronic exposure (10–60 days) performance recovers to some extent and recent work has demonstrated that this occurs for both audio-visual and audio-only regions of space. This begs the questions as to the teacher signal for this remarkable functional plasticity in the adult nervous system. Following a brief review of inﬂuence of the motor state in auditory localization, we consider the potential role of auditory-motor learning in the perceptual recalibration of the spectral cues. Several recent studies have considered how multi-modal and sensory-motor feedback might inﬂuence accommodation to altered spectral cues produced by ear molds or through virtual auditory space stimulation using non-individualized spectral cues. The work with ear molds demonstrates that a relatively short period of training involving audio-motor feedback (5–10 days) signiﬁcantly improved both the rate and extent of accommodation to altered spectral cues. This has signiﬁcant implications not only for the mechanisms by which this complex sensory information is encoded to provide spatial cues but also for adaptive training to altered auditory inputs. The review concludes by considering the implications for rehabilitative training with hearing aids and cochlear prosthesis.},
	language = {en},
	urldate = {2021-03-15},
	journal = {Frontiers in Neuroscience},
	author = {Carlile, Simon},
	month = aug,
	year = {2014},
	file = {Carlile - 2014 - The plastic ear and perceptual relearning in audit.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZYYYBPTJ\\Carlile - 2014 - The plastic ear and perceptual relearning in audit.pdf:application/pdf},
}

@article{garcia-gomez_linear_2021,
	title = {Linear detector and neural networks in cascade for voice activity detection in hearing aids},
	volume = {175},
	issn = {0003682X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0003682X20309373},
	doi = {10.1016/j.apacoust.2020.107832},
	abstract = {Hearing loss is a common issue when people become older, resulting in problems such as depression, risk of dementia, and cognitive decline, among others. Hearing aids are computationally constrained devices that offer the possibility of solving this issue, thus improving people’s quality of life. A typical algorithm that should be implemented in these devices is Voice Activity Detection. In this work, cascade detectors are applied to reduce the computational cost while maintaining the same performance or to increase the performance while maintaining the same computational cost. This is achieved by a two-stage detector. In the ﬁrst stage, a linear system determines whether the detection can be easily carried out, or a second stage with a more complex neural-network-based detection is required. This way, some of the decisions are taken without using the complex detector. The results show that the system error can be reduced up to 8.5\% while using the same amount of resources. Moreover, the error is the lowest among the proposals that are affordably implemented in hearing aids.},
	language = {en},
	urldate = {2021-03-15},
	journal = {Applied Acoustics},
	author = {García-Gómez, Joaquín and Gil-Pita, Roberto and Aguilar-Ortega, Miguel and Utrilla-Manso, Manuel and Rosa-Zurera, Manuel and Mohino-Herranz, Inma},
	month = apr,
	year = {2021},
	keywords = {hearing aids},
	pages = {107832},
	file = {García-Gómez et al. - 2021 - Linear detector and neural networks in cascade for.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WDNCKRUP\\García-Gómez et al. - 2021 - Linear detector and neural networks in cascade for.pdf:application/pdf},
}

@article{graf_features_2015,
	title = {Features for voice activity detection: a comparative analysis},
	volume = {2015},
	issn = {1687-6180},
	shorttitle = {Features for voice activity detection},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-015-0277-z},
	doi = {10.1186/s13634-015-0277-z},
	abstract = {In many speech signal processing applications, voice activity detection (VAD) plays an essential role for separating an audio stream into time intervals that contain speech activity and time intervals where speech is absent. Many features that reflect the presence of speech were introduced in literature. However, to our knowledge, no extensive comparison has been provided yet. In this article, we therefore present a structured overview of several established VAD features that target at different properties of speech. We categorize the features with respect to properties that are exploited, such as power, harmonicity, or modulation, and evaluate the performance of some dedicated features. The importance of temporal context is discussed in relation to latency restrictions imposed by different applications. Our analyses allow for selecting promising VAD features and finding a reasonable trade-off between performance and complexity.},
	language = {en},
	number = {1},
	urldate = {2021-03-15},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Graf, Simon and Herbig, Tobias and Buck, Markus and Schmidt, Gerhard},
	month = dec,
	year = {2015},
	pages = {91},
	file = {Graf et al. - 2015 - Features for voice activity detection a comparati.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\G97F4FX2\\Graf et al. - 2015 - Features for voice activity detection a comparati.pdf:application/pdf},
}

@inproceedings{gil-pita_evolved_2017,
	address = {Naples, Italy},
	title = {Evolved frequency log-energy coefficients for voice activity detection in hearing aids},
	isbn = {978-1-5090-6034-4},
	url = {http://ieeexplore.ieee.org/document/8015620/},
	doi = {10.1109/FUZZ-IEEE.2017.8015620},
	abstract = {Eco-efﬁciency in hearing aids is an important issue, related to the maximization of the battery life. In order to minimize the power consumption, the embedded digital signal processor works at very low clock rates, constraining the implementation of signal processing techniques. The implemented algorithms can only use a small number of instructions per second and a small amount of memory. One of the main algorithms implemented in nowadays hearing aids is the voice activity detection algorithm, useful for several noise reduction and speech enhancement algorithms. The objective of this paper is the study of the implementation of voice activity detection algorithms in hearing aids using tailored fuzzy features, taking into account the optimization of the available resources.},
	language = {en},
	urldate = {2021-03-15},
	booktitle = {2017 {IEEE} {International} {Conference} on {Fuzzy} {Systems} ({FUZZ}-{IEEE})},
	publisher = {IEEE},
	author = {Gil-Pita, Roberto and Garcia-Gomez, Joaquin and Bautista-Duran, Marta and Combarro, Elias and Cocana-Fernandez, Alberto},
	month = jul,
	year = {2017},
	pages = {1--6},
	file = {Gil-Pita et al. - 2017 - Evolved frequency log-energy coefficients for voic.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RKJN3847\\Gil-Pita et al. - 2017 - Evolved frequency log-energy coefficients for voic.pdf:application/pdf},
}

@inproceedings{corey_hypothesis_2016,
	address = {Vietri sul Mare, Salerno, Italy},
	title = {A hypothesis testing approach for real-time multichannel speech separation using time-frequency masks},
	isbn = {978-1-5090-0746-2},
	url = {http://ieeexplore.ieee.org/document/7738827/},
	doi = {10.1109/MLSP.2016.7738827},
	abstract = {We propose a new approach to time-frequency mask generation for real-time multichannel speech separation. Whereas conventional approaches select the strongest source in each time-frequency bin, we perform a binary hypothesis test to determine whether a target source is present or not. We derive a generalized likelihood ratio test and extend it to underdetermined mixtures by aggregating the outputs of several tests with different interference models. This approach is justiﬁed by the nonstationarity and time-frequency disjointedness of speech signals. This computationally simple method is suitable for real-time source separation in resource-constrained and latency-critical applications.},
	language = {en},
	urldate = {2021-03-15},
	booktitle = {2016 {IEEE} 26th {International} {Workshop} on {Machine} {Learning} for {Signal} {Processing} ({MLSP})},
	publisher = {IEEE},
	author = {Corey, Ryan M. and Singer, Andrew C.},
	month = sep,
	year = {2016},
	pages = {1--6},
	file = {corey_mlsp2016_masks.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZV8AUZLL\\corey_mlsp2016_masks.pdf:application/pdf},
}

@article{alexander_effects_2015,
	title = {Effects of {WDRC} {Release} {Time} and {Number} of {Channels} on {Output} {SNR} and {Speech} {Recognition}},
	volume = {36},
	issn = {0196-0202},
	url = {https://journals.lww.com/00003446-201503000-00017},
	doi = {10.1097/AUD.0000000000000115},
	abstract = {Objectives: The purpose of this study was to investigate the joint effects that wide dynamic range compression (WDRC) release time (RT) and number of channels have on recognition of sentences in the presence of steady and modulated maskers at different signal-to-noise ratios (SNRs). How the different combinations of WDRC parameters affect output SNR and the role this plays in the observed findings were also investigated. Design: Twenty-four listeners with mild to moderate sensorineural hearing loss identified sentences mixed with steady or modulated maskers at three SNRs (−5, 0, and +5 dB) that had been processed using a hearing aid simulator with six combinations of RT (40 and 640 msec) and number of channels (4, 8, and 16). Compression parameters were set using the Desired Sensation Level v5.0a prescriptive fitting method. For each condition, amplified speech and masker levels and the resultant longterm output SNR were measured.
Results: Speech recognition with WDRC depended on the combination of RT and number of channels, with the greatest effects observed at 0 dB input SNR, in which mean speech recognition scores varied by 10 to 12\% across WDRC manipulations. Overall, effect sizes were generally small. Across both masker types and the three SNRs tested, the best speech recognition was obtained with eight channels, regardless of RT. Increased speech levels, which favor audibility, were associated with the short RT and with an increase in the number of channels. These same conditions also increased masker levels by an even greater amount, for a net decrease in the longterm output SNR. Changes in long-term SNR across WDRC conditions were found to be strongly associated with changes in the temporal envelope shape as quantified by the Envelope Difference Index; however, neither of these factors fully explained the observed differences in speech recognition.
Conclusions: A primary finding of this study was that the number of channels had a modest effect when analyzed at each level of RT, with results suggesting that selecting eight channels for a given RT might be the safest choice. Effects were smaller for RT, with results suggesting that short RT was slightly better when only 4 channels were used and that long RT was better when 16 channels were used. Individual differences in how listeners were influenced by audibility, output SNR, temporal distortion, and spectral distortion may have contributed to the size of the effects found in this study. Because only general suppositions could made for how each of these factors may have influenced the overall results of this study, future research would benefit from exploring the predictive value of these and other factors in selecting the processing parameters that maximize speech recognition for individuals.},
	language = {en},
	number = {2},
	urldate = {2021-03-15},
	journal = {Ear \& Hearing},
	author = {Alexander, Joshua M. and Masterson, Katie},
	month = mar,
	year = {2015},
	pages = {e35--e49},
	file = {Alexander et Masterson - 2015 - Effects of WDRC Release Time and Number of Channel.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UQV9WTJY\\Alexander et Masterson - 2015 - Effects of WDRC Release Time and Number of Channel.pdf:application/pdf},
}

@inproceedings{mauler_optimal_2007,
	title = {On {Optimal} {Estimation} of {Compressed} {Speech} for {Hearing} {Aids}},
	abstract = {When noise reduction (NR) and dynamic compression (CP) systems are concatenated in a hearing aid or in a cochlear implant we observe undesired interaction effects like the degradation of the global SNR. A reason for this might be that the optimization of the NR algorithm is performed with respect to the uncompressed clean speech only. In this contribution we propose an alternative approach which integrates the CP task in the derivation of the NR algorithm. By this we get novel MMSE and MAP optimal estimators for the compressed clean speech. An analysis of the behavior of the proposed solutions reveals that the differences to a serial concatenation of NR and CP are in general small. In case of the widely used MMSE log spectral amplitude (LSA) estimator [1] we show that the combined optimization is identical to a serial concatenation.},
	language = {en},
	booktitle = {Interspeech},
	author = {Mauler, Dirk and Nagathil, Anil M and Martin, Rainer},
	month = aug,
	year = {2007},
	pages = {4},
	file = {i07_0826.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9F7ZZDLY\\i07_0826.pdf:application/pdf;Mauler et al. - On Optimal Estimation of Compressed Speech for Hea.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4P6D4SGT\\Mauler et al. - On Optimal Estimation of Compressed Speech for Hea.pdf:application/pdf},
}

@inproceedings{jaffer_maximum_1988,
	address = {New York, NY, USA},
	title = {Maximum likelihood direction finding of stochastic sources: a separable solution},
	shorttitle = {Maximum likelihood direction finding of stochastic sources},
	url = {http://ieeexplore.ieee.org/document/197258/},
	doi = {10.1109/ICASSP.1988.197258},
	abstract = {A novel method is presented for maxir,,m likelihood direction finding of stochastic sources which may be correlated. It is shown that the maximum likelihood estimates of the angle parameters and unknown source covariance matrix may be obtained in a separable form, i.e. the angle parameters are obtained by maximizing a function of only the angle parameters. The source covariance matrix estimate is then obtained by an explicit formula. This results in a significant reduction of the dimensionality of the optimization problem to be solved compared to previous approaches based on direct maximization over all parameters. Computer simulation results are presented to demonstrate the performance of the proposed method.},
	language = {en},
	urldate = {2021-03-12},
	booktitle = {{ICASSP}-88., {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Jaffer, A.G.},
	year = {1988},
	pages = {2893--2896},
	file = {Jaffer - 1988 - Maximum likelihood direction finding of stochastic.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JNDZ27WD\\Jaffer - 1988 - Maximum likelihood direction finding of stochastic.pdf:application/pdf},
}

@article{wong_efficacy_2018,
	title = {Efficacy of a {Hearing} {Aid} {Noise} {Reduction} {Function}},
	volume = {22},
	issn = {2331-2165, 2331-2165},
	url = {http://journals.sagepub.com/doi/10.1177/2331216518782839},
	doi = {10.1177/2331216518782839},
	abstract = {Noise reduction systems have been implemented in hearing aids to improve signal-to-noise ratio and listening comfort. The aim of this study was to evaluate the efficacy of hearing aid noise reduction for Mandarin speakers. The results showed a significant improvement in acceptable noise levels and speech reception thresholds with noise reduction turned on. Sound quality ratings also suggested that most listeners preferred having noise reduction turned on for listening effort, listening comfort, speech clarity, and overall sound quality. These results suggest that the noise reduction system used in this study might improve sentence perception in steady-state noise, noise tolerance, and sound quality, although not all listeners preferred aggressive noise reduction. However, due to large interindividual variation, clinical application of the results should be on an individual basis.},
	language = {en},
	urldate = {2021-03-16},
	journal = {Trends in Hearing},
	author = {Wong, Lena L. N. and Chen, Yuan and Wang, Qianran and Kuehnel, Volker},
	month = jan,
	year = {2018},
	pages = {14},
	file = {Wong et al. - 2018 - Efficacy of a Hearing Aid Noise Reduction Function.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\53GQCCV5\\Wong et al. - 2018 - Efficacy of a Hearing Aid Noise Reduction Function.pdf:application/pdf},
}

@article{chung_effective_2007,
	title = {Effective compression and noise reduction configurations for hearing protectors},
	volume = {121},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2409859},
	doi = {10.1121/1.2409859},
	language = {en},
	number = {2},
	urldate = {2021-03-16},
	journal = {The Journal of the Acoustical Society of America},
	author = {Chung, King},
	month = feb,
	year = {2007},
	pages = {1090--1101},
	file = {Chung - 2007 - Effective compression and noise reduction configur.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\X3R37FII\\Chung - 2007 - Effective compression and noise reduction configur.pdf:application/pdf},
}

@article{anderson_acoustic_2009,
	title = {The {Acoustic} and {Peceptual} {Effects} of {Series} and {Parallel} {Processing}},
	volume = {2009},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/2009/619805},
	doi = {10.1155/2009/619805},
	language = {en},
	number = {1},
	urldate = {2021-03-16},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Anderson, Melinda C. and Arehart, Kathryn H. and Kates, James M.},
	month = dec,
	year = {2009},
	pages = {1--20},
	file = {Anderson et al. - 2009 - The Acoustic and Peceptual Effects of Series and P.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YDY4GH62\\Anderson et al. - 2009 - The Acoustic and Peceptual Effects of Series and P.pdf:application/pdf},
}

@article{moore_comparison_1988,
	title = {A comparison of four methods of implementing automatic gain control ({AGC}) in hearing aids},
	volume = {22},
	issn = {0300-5364},
	url = {http://www.tandfonline.com/doi/full/10.3109/03005368809077803},
	doi = {10.3109/03005368809077803},
	language = {en},
	number = {2},
	urldate = {2021-03-16},
	journal = {British Journal of Audiology},
	author = {Moore, Brian C. J. and Glasberg, Brian R.},
	month = jan,
	year = {1988},
	pages = {93--104},
	file = {Moore et Glasberg - 1988 - A comparison of four methods of implementing autom.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\877FZCPW\\Moore et Glasberg - 1988 - A comparison of four methods of implementing autom.pdf:application/pdf},
}

@article{chen_can_2021,
	title = {Can {Dual} {Compression} {Offer} {Better} {Mandarin} {Speech} {Intelligibility} and {Sound} {Quality} {Than} {Fast}-{Acting} {Compression}?},
	volume = {25},
	issn = {2331-2165, 2331-2165},
	url = {http://journals.sagepub.com/doi/10.1177/2331216521997610},
	doi = {10.1177/2331216521997610},
	abstract = {The aim of this study was to evaluate the efficacy of dual compression for Mandarin-speaking hearing aid users. Dual compression combines fast and slow compressors operating simultaneously across all frequency channels. The study participants were 31 hearing aid users with symmetrical moderate-to-severe hearing loss, with a mean age of 67 years. A new pair of 20-channel behind-the-ear hearing aids (i.e., Phonak Bolero B90-P) was used during the testing. The results revealed a significant improvement in speech reception thresholds in noise when switching from fast-acting compression to dual compression. The sound quality ratings revealed that most listeners preferred dual compression to fast-acting compression for listening effort, listening comfort, speech clarity, and overall sound quality at þ4 dB signal-to-noise ratio. These results are consistent with predictions based on the theoretical understanding of dual and fast-acting compression. However, whether these results can be generalized to other languages or other dual compression systems should be verified by future studies.},
	language = {en},
	urldate = {2021-03-16},
	journal = {Trends in Hearing},
	author = {Chen, Yuan and Wong, Lena L. N. and Kuehnel, Volker and Qian, Jinyu and Voss, Solveig Christina and Shangqiguo, Wang},
	month = jan,
	year = {2021},
	pages = {13},
	file = {2331216521997610.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IQNX6LV8\\2331216521997610.pdf:application/pdf;Chen et al. - 2021 - Can Dual Compression Offer Better Mandarin Speech .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\SCM3QUGG\\Chen et al. - 2021 - Can Dual Compression Offer Better Mandarin Speech .pdf:application/pdf},
}

@article{chong_critical_2018,
	title = {A critical review of hearing-aid single-microphone noise-reduction studies in adults and children},
	volume = {13},
	issn = {1748-3107, 1748-3115},
	url = {https://www.tandfonline.com/doi/full/10.1080/17483107.2017.1392619},
	doi = {10.1080/17483107.2017.1392619},
	abstract = {Purpose: Single-microphone noise reduction (SMNR) is implemented in hearing aids to suppress background noise. The purpose of this article was to provide a critical review of peer-reviewed studies in adults and children with sensorineural hearing loss who were fitted with hearing aids incorporating SMNR.
Method: Articles published between 2000 and 2016 were searched in PUBMED and EBSCO databases.
Results: Thirty-two articles were included in the final review. Most studies with adult participants showed that SMNR has no effect on speech intelligibility. Positive results were reported for acceptance of background noise, preference, and listening effort. Studies of school-aged children were consistent with the findings of adult studies. No study with infants or young children of under 5 years old was found. Recent studies on noise-reduction systems not yet available in wearable hearing aids have documented benefits of noise reduction on memory for speech processing for older adults.
Conclusions: This evidence supports the use of SMNR for adults and school-aged children when the aim is to improve listening comfort or reduce listening effort. Future research should test SMNR with infants and children who are younger than 5 years of age. Further development, testing, and clinical trials should be carried out on algorithms not yet available in wearable hearing aids. Testing higher cognitive level for speech processing and learning of novel sounds or words could show benefits of advanced signal processing features. These approaches should be expanded to other populations such as children and younger adults.},
	language = {en},
	number = {6},
	urldate = {2021-03-16},
	journal = {Disability and Rehabilitation: Assistive Technology},
	author = {Chong, Foong Yen and Jenstad, Lorienne M.},
	month = aug,
	year = {2018},
	pages = {600--608},
	file = {Chong et Jenstad - 2018 - A critical review of hearing-aid single-microphone.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KSA8TSTM\\Chong et Jenstad - 2018 - A critical review of hearing-aid single-microphone.pdf:application/pdf},
}

@article{corey_modeling_2020,
	title = {Modeling the effects of dynamic range compression on signals in noise},
	url = {http://arxiv.org/abs/2012.03860},
	abstract = {Hearing aids use dynamic range compression (DRC), a form of automatic gain control, to make quiet sounds louder and loud sounds quieter. Compression can improve listening comfort, but it can also cause distortion in noisy environments. It has been widely reported that DRC performs poorly in noise, but there has been little mathematical analysis of these distortion eﬀects. This work introduces a mathematical model to study the behavior of DRC in noise. Using statistical assumptions about the signal envelopes, we deﬁne an eﬀective compression function that models the compression applied to one signal in the presence of another. This framework is used to prove results about DRC that have been previously observed experimentally: that when DRC is applied to a mixture of signals, uncorrelated signal envelopes become negatively correlated; that the eﬀective compression applied to each sound in a mixture is weaker than it would have been for the signal alone; and that compression can reduce the long-term signal-to-noise ratio in certain conditions. These theoretical results are supported by software experiments using recorded speech signals.},
	language = {en},
	urldate = {2021-03-16},
	journal = {arXiv:2012.03860 [cs, eess]},
	author = {Corey, Ryan M. and Singer, Andrew C.},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.03860},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Corey et Singer - 2020 - Modeling the effects of dynamic range compression .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YQ649ADT\\Corey et Singer - 2020 - Modeling the effects of dynamic range compression .pdf:application/pdf},
}

@article{brons_acoustical_2015,
	title = {Acoustical and {Perceptual} {Comparison} of {Noise} {Reduction} and {Compression} in {Hearing} {Aids}},
	volume = {58},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/2015_JSLHR-H-14-0347},
	doi = {10.1044/2015_JSLHR-H-14-0347},
	abstract = {Purpose
              Noise reduction and dynamic-range compression are generally applied together in hearing aids but may have opposite effects on amplification. This study evaluated the acoustical and perceptual effects of separate and combined processing of noise reduction and compression.
            
            
              Design
              Recordings of the output of 4 hearing aids for speech in babble noise at +4 dB signal-to-noise ratio were used in 3 experiments: (a) acoustical measurements to determine the influence of processing on speech and noise levels; (b) perceptual measurements to determine the detectability of processing differences for 16 listeners with hearing impairment; and (c) perceptual measurements to determine the effect of processing on speech intelligibility, noise annoyance, speech naturalness, and overall preference.
            
            
              Results
              Noise reduction and compression processing differed between hearing aids. The combined processing (noise reduction with compression) most strongly reduced noise and speech levels. The combined processing was detectably different between hearing aids, but compression processing alone was not. The combined processing did not influence speech intelligibility. Preference for combined processing was lower than previously observed for noise reduction without compression.
            
            
              Conclusions
              Differences in processing between hearing aids are perceptually salient. The effect of compression should be taken into account during the development and evaluation of hearing aid noise reduction.},
	language = {en},
	number = {4},
	urldate = {2021-03-16},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Brons, Inge and Houben, Rolph and Dreschler, Wouter A.},
	month = aug,
	year = {2015},
	pages = {1363--1376},
	file = {Brons et al. - 2015 - Acoustical and Perceptual Comparison of Noise Redu.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\DHY5J7DU\\Brons et al. - 2015 - Acoustical and Perceptual Comparison of Noise Redu.pdf:application/pdf},
}

@article{mcaulay_speech_1980,
	title = {Speech enhancement using a soft-decision noise suppression filter},
	volume = {28},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1163394/},
	doi = {10.1109/TASSP.1980.1163394},
	language = {en},
	number = {2},
	urldate = {2021-03-16},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {McAulay, R. and Malpass, M.},
	month = apr,
	year = {1980},
	pages = {137--145},
	file = {McAulay et Malpass - 1980 - Speech enhancement using a soft-decision noise sup.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KV7D7SP8\\McAulay et Malpass - 1980 - Speech enhancement using a soft-decision noise sup.pdf:application/pdf},
}

@article{ephraim_speech_1985,
	title = {Speech enhancement using a minimum mean-square error log-spectral amplitude estimator},
	volume = {33},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1164550/},
	doi = {10.1109/TASSP.1985.1164550},
	language = {en},
	number = {2},
	urldate = {2021-03-16},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Ephraim, Y. and Malah, D.},
	month = apr,
	year = {1985},
	pages = {443--445},
	file = {Ephraim et Malah - 1985 - Speech enhancement using a minimum mean-square err.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3PUUJS65\\Ephraim et Malah - 1985 - Speech enhancement using a minimum mean-square err.pdf:application/pdf},
}

@misc{denk_hearpiece_2020,
	title = {The {Hearpiece} database of individual transfer functions of an openly available in-the-ear earpiece for hearing device research},
	copyright = {Creative Commons Attribution Share Alike 4.0 International, Open Access},
	url = {https://zenodo.org/record/3733191},
	doi = {10.5281/ZENODO.3733191},
	abstract = {We present a database of acoustic transfer functions of the Hearpiece, an openly available multi-microphone multi-driver in-the-ear earpiece for hearing device research. The database includes HRTFs for 87 incidence directions as well as responses of the drivers, all measured at the four microphones of the Hearpiece as well as the eardrum in the occluded and open ear. The transfer functions were measured in both ears of 25 human subjects and a KEMAR with anthropometric ears for five reinsertions of the device. We describe the measurements of the database and analyse derived acoustic parameters of the device. All regarded transfer functions are subject to differences between subjects as well as variations due to reinsertion into the same ear. Also, the results show that KEMAR measurements represent a median human ear well for all assessed transfer functions. The database is a rich basis for development, evaluation and robustness analysis of multiple hearing device algorithms and applications.},
	language = {en},
	urldate = {2021-03-19},
	publisher = {Zenodo},
	author = {Denk, Florian and Kollmeier, Birger},
	month = apr,
	year = {2020},
	keywords = {HRTF, Earphones, Feedback Paths, Hearables, Hearing Aids, Hearing Protectors, open Master Hearing Aid},
	file = {Denk et Kollmeier - 2020 - The Hearpiece database of individual transfer func.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MM6VFDUX\\Denk et Kollmeier - 2020 - The Hearpiece database of individual transfer func.pdf:application/pdf},
}

@article{denk_adapting_2018,
	title = {Adapting {Hearing} {Devices} to the {Individual} {Ear} {Acoustics}: {Database} and {Target} {Response} {Correction} {Functions} for {Various} {Device} {Styles}},
	volume = {22},
	issn = {2331-2165, 2331-2165},
	shorttitle = {Adapting {Hearing} {Devices} to the {Individual} {Ear} {Acoustics}},
	url = {http://journals.sagepub.com/doi/10.1177/2331216518779313},
	doi = {10.1177/2331216518779313},
	abstract = {To achieve a natural sound quality when listening through hearing devices, the sound pressure at the eardrum should replicate that of the open ear, modified only by an insertion gain if desired. A target approximating this reference condition can be computed by applying an appropriate correction function to the pressure observed at the device microphone. Such Target Response Correction Functions (TRCF) can be defined based on the directionally dependent relative transfer function between the location of the hearing device microphone and the eardrum of the open ear. However, it is unclear how exactly the TRCF should be derived, and how large the benefit of individual, versus generic, correction is. We present measurements of Head-Related Transfer Functions (HRTF) at the eardrum and at 9 microphone locations of a comprehensive set of 5 hearing device styles, including 91 incidence directions, and recorded in 16 subjects and 2 dummy heads. Based on these HRTFs, individualized and generic TRCF were computed for frontal (referred to as free-field) and diffuse-field sound incidence. Spectral deviations between the computed target and listening with the open ear were evaluated using an auditory model and virtual acoustic scenes. Results indicate that a correction for diffuse-field incidence should be preferred over the free field, and individual correction functions result in notably reduced spectral deviations to open-ear listening, as compared with generic correction functions. These outcomes depend substantially on the specific device style. The HRTF database and derived TRCFs are publicly available.},
	language = {en},
	urldate = {2021-03-19},
	journal = {Trends in Hearing},
	author = {Denk, Florian and Ernst, Stephan M. A. and Ewert, Stephan D. and Kollmeier, Birger},
	month = jan,
	year = {2018},
	pages = {1--19},
	file = {Denk et al. - 2018 - Adapting Hearing Devices to the Individual Ear Aco.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WXBRATDW\\Denk et al. - 2018 - Adapting Hearing Devices to the Individual Ear Aco.pdf:application/pdf},
}

@article{adiloglu_binaural_2015,
	title = {A {Binaural} {Steering} {Beamformer} {System} for {Enhancing} a {Moving} {Speech} {Source}},
	volume = {19},
	issn = {2331-2165, 2331-2165},
	url = {http://journals.sagepub.com/doi/10.1177/2331216515618903},
	doi = {10.1177/2331216515618903},
	abstract = {In many daily life communication situations, several sound sources are simultaneously active. While normal-hearing listeners can easily distinguish the target sound source from interfering sound sources—as long as target and interferers are spatially or spectrally separated—and concentrate on the target, hearing-impaired listeners and cochlear implant users have difficulties in making such a distinction. In this article, we propose a binaural approach composed of a spatial filter controlled by a direction-of-arrival estimator to track and enhance a moving target sound. This approach was implemented on a real-time signal processing platform enabling experiments with test subjects in situ. To evaluate the proposed method, a data set of sound signals with a single moving sound source in an anechoic diffuse noise environment was generated using virtual acoustics. The proposed steering method was compared with a fixed (nonsteering) method that enhances sound from the frontal direction in an objective evaluation and subjective experiments using this database. In both cases, the obtained results indicated a significant improvement in speech intelligibility and quality compared with the unprocessed signal. Furthermore, the proposed method outperformed the nonsteering method.},
	language = {en},
	urldate = {2021-03-19},
	journal = {Trends in Hearing},
	author = {Adiloğlu, Kamil and Kayser, Hendrik and Baumgärtel, Regina M. and Rennebeck, Sanja and Dietz, Mathias and Hohmann, Volker},
	month = dec,
	year = {2015},
	pages = {233121651561890},
	file = {Adiloğlu et al. - 2015 - A Binaural Steering Beamformer System for Enhancin.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\R7XV48LK\\Adiloğlu et al. - 2015 - A Binaural Steering Beamformer System for Enhancin.pdf:application/pdf},
}

@inproceedings{patel_frequency-based_2019,
	address = {San Diego, California},
	title = {Frequency-based multi-band adaptive compression for hearing aid application},
	url = {http://asa.scitation.org/doi/abs/10.1121/2.0001247},
	doi = {10.1121/2.0001247},
	language = {en},
	urldate = {2021-03-29},
	booktitle = {Proceedings of {Meetings} on {Acoustics}},
	author = {Patel, Kashyap and Panahi, Issa M.S.},
	year = {2019},
	pages = {055004},
	file = {Patel et Panahi - 2019 - Frequency-based multi-band adaptive compression fo.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZTCJ26XN\\Patel et Panahi - 2019 - Frequency-based multi-band adaptive compression fo.pdf:application/pdf},
}

@article{souden_integrated_2011,
	title = {An {Integrated} {Solution} for {Online} {Multichannel} {Noise} {Tracking} and {Reduction}},
	volume = {19},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5719158/},
	doi = {10.1109/TASL.2011.2118205},
	abstract = {Noise statistics estimation is a paramount issue in the design of reliable noise-reduction algorithms. Although signiﬁcant efforts have been devoted to this problem in the literature, most developed methods so far have focused on the single-channel case. When multiple microphones are used, it is important that the data from all the sensors are optimally combined to achieve judicious updates of the noise statistics and the noise-reduction ﬁlter. This contribution is devoted to the development of a practical approach to multichannel noise tracking and reduction. We combine the multichannel speech presence probability (MC-SPP) that we proposed in an earlier contribution with an alternative formulation of the minima-controlled recursive averaging (MCRA) technique that we generalize from the single-channel to the multichannel case. To demonstrate the effectiveness of the proposed MC-SPP and multichannel noise estimator, we integrate them into three variants of the multichannel noise reduction Wiener ﬁlter. Experimental results show the advantages of the proposed solution.},
	language = {en},
	number = {7},
	urldate = {2021-03-29},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Souden, Mehrez and Chen, Jingdong and Benesty, Jacob and Affes, Sofiène},
	month = sep,
	year = {2011},
	pages = {2159--2169},
	file = {Souden et al. - 2011 - An Integrated Solution for Online Multichannel Noi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AB2HCLEG\\Souden et al. - 2011 - An Integrated Solution for Online Multichannel Noi.pdf:application/pdf},
}

@article{andersen_refinement_2018,
	title = {Refinement and validation of the binaural short time objective intelligibility measure for spatially diverse conditions},
	volume = {102},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639317302947},
	doi = {10.1016/j.specom.2018.06.001},
	language = {en},
	urldate = {2021-03-29},
	journal = {Speech Communication},
	author = {Andersen, Asger Heidemann and de Haan, Jan Mark and Tan, Zheng-Hua and Jensen, Jesper},
	month = sep,
	year = {2018},
	pages = {1--13},
	file = {Andersen et al. - 2018 - Refinement and validation of the binaural short ti.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NR2WZ8D8\\Andersen et al. - 2018 - Refinement and validation of the binaural short ti.pdf:application/pdf},
}

@article{french_factors_1947,
	title = {Factors {Governing} the {Intelligibility} of {Speech} {Sounds}},
	volume = {90},
	language = {en},
	number = {19},
	journal = {The Journal of the Acoustical Society of America},
	author = {French, N R and Steinberg, J C},
	year = {1947},
	pages = {31},
	file = {French et Steinberg - Factors Governing the Intelligibility of Speech So.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\A4PKYXAR\\French et Steinberg - Factors Governing the Intelligibility of Speech So.pdf:application/pdf},
}

@article{goldsworthy_analysis_2004,
	title = {Analysis of speech-based speech transmission index methods with implications for nonlinear operations},
	volume = {116},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1804628},
	doi = {10.1121/1.1804628},
	language = {en},
	number = {6},
	urldate = {2021-03-29},
	journal = {The Journal of the Acoustical Society of America},
	author = {Goldsworthy, Ray L. and Greenberg, Julie E.},
	month = dec,
	year = {2004},
	pages = {3679--3689},
	file = {Goldsworthy et Greenberg - 2004 - Analysis of speech-based speech transmission index.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7Y4XVXYB\\Goldsworthy et Greenberg - 2004 - Analysis of speech-based speech transmission index.pdf:application/pdf},
}

@article{hawley_speech_1999,
	title = {Speech intelligibility and localization in a multi-source environment},
	volume = {6},
	language = {en},
	number = {105},
	journal = {The Journal of the Acoustical Society of America},
	author = {Hawley, Monica L and Litovsky, Ruth Y and Colburn, H Steven},
	year = {1999},
	pages = {13},
	file = {Hawley et al. - Speech intelligibility and localization in a multi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EXJCTQNI\\Hawley et al. - Speech intelligibility and localization in a multi.pdf:application/pdf},
}

@inproceedings{taal_short-time_2010,
	address = {Dallas, TX, USA},
	title = {A short-time objective intelligibility measure for time-frequency weighted noisy speech},
	isbn = {978-1-4244-4295-9},
	url = {http://ieeexplore.ieee.org/document/5495701/},
	doi = {10.1109/ICASSP.2010.5495701},
	abstract = {Existing objective speech-intelligibility measures are suitable for several types of degradation, however, it turns out that they are less appropriate for methods where noisy speech is processed by a timefrequency (TF) weighting, e.g., noise reduction and speech separation. In this paper, we present an objective intelligibility measure, which shows high correlation (rho=0.95) with the intelligibility of both noisy, and TF-weighted noisy speech. The proposed method shows signiﬁcantly better performance than three other, more sophisticated, objective measures. Furthermore, it is based on an intermediate intelligibility measure for short-time (approximately 400 ms) TF-regions, and uses a simple DFT-based TF-decomposition. In addition, a free Matlab implementation is provided.},
	language = {en},
	urldate = {2021-03-29},
	booktitle = {2010 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Taal, Cees H. and Hendriks, Richard C. and Heusdens, Richard and Jensen, Jesper},
	year = {2010},
	pages = {4214--4217},
	file = {Taal et al. - 2010 - A short-time objective intelligibility measure for.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7HBUY5E6\\Taal et al. - 2010 - A short-time objective intelligibility measure for.pdf:application/pdf},
}

@article{moore_simulation_1993,
	title = {Simulation of the effects of loudness recruitment and threshold elevation on the intelligibility of speech in quiet and in a background of speech},
	volume = {94},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.407478},
	doi = {10.1121/1.407478},
	language = {en},
	number = {4},
	urldate = {2021-03-26},
	journal = {The Journal of the Acoustical Society of America},
	author = {Moore, Brian C. J. and Glasberg, Brian R.},
	month = oct,
	year = {1993},
	pages = {2050--2062},
	file = {Moore et Glasberg - 1993 - Simulation of the effects of loudness recruitment .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5KLV8RCU\\Moore et Glasberg - 1993 - Simulation of the effects of loudness recruitment .pdf:application/pdf},
}

@inproceedings{thiergart_informed_2013,
	title = {An {Informed} {MMSE} {Filter} based on {Multiple} {Instantaneous} {Direction}-of-{Arrival} {Estimates}},
	abstract = {Sound acquisition in noisy and reverberant conditions where the acoustic scene changes rapidly remains a challenging task. In this work, we consider the problem of obtaining a desired, arbitrary spatial response for at most L sound sources being simultaneously active per time-frequency instant. We propose a minimum meansquared error spatial ﬁlter that adapts quickly to changes in the acoustic scene by incorporating instantaneous parametric information on the sound ﬁeld. In addition, an estimator for the power spectral densities of the L sources is developed that exhibits a sufﬁciently high temporal and spectral resolution to achieve both dereverberation and noise reduction. Simulation results demonstrate that a strong attenuation of undesired noise and interfering components can be achieved with a tolerable amount of signal distortion.},
	language = {en},
	booktitle = {Proc. 21st {Eur}. {Signal} {Process}. {Conf}. ({EUSIPCO} 13)},
	author = {Thiergart, Oliver and Taseska, Maja and Habets, Emanuel A P},
	year = {2013},
	pages = {5},
	file = {Thiergart et al. - AN INFORMED MMSE FILTER BASED ON MULTIPLE INSTANTA.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9PL6NUI4\\Thiergart et al. - AN INFORMED MMSE FILTER BASED ON MULTIPLE INSTANTA.pdf:application/pdf},
}

@article{roy_esprit-estimation_1989,
	title = {{ESPRIT}-estimation of signal parameters via rotational invariance techniques},
	volume = {37},
	issn = {00963518},
	url = {http://ieeexplore.ieee.org/document/32276/},
	doi = {10.1109/29.32276},
	number = {7},
	urldate = {2021-03-26},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Roy, R. and Kailath, T.},
	month = jul,
	year = {1989},
	pages = {984--995},
	file = {roy1989.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QHKMC36S\\roy1989.pdf:application/pdf},
}

@article{middleton_simultaneous_1968,
	title = {Simultaneous optimum detection and estimation of signals in noise},
	volume = {14},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1054139/},
	doi = {10.1109/TIT.1968.1054139},
	language = {en},
	number = {3},
	urldate = {2021-03-25},
	journal = {IEEE Transactions on Information Theory},
	author = {Middleton, D. and Esposito, R.},
	month = may,
	year = {1968},
	pages = {434--444},
	file = {Middleton et Esposito - 1968 - Simultaneous optimum detection and estimation of s.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LJN6D77R\\Middleton et Esposito - 1968 - Simultaneous optimum detection and estimation of s.pdf:application/pdf},
}

@article{stone_syllabic_1992,
	title = {Syllabic compression: {Effective} compression ratios for signals modulated at different rates},
	volume = {26},
	issn = {0300-5364},
	shorttitle = {Syllabic compression},
	url = {http://www.tandfonline.com/doi/full/10.3109/03005369209076659},
	doi = {10.3109/03005369209076659},
	language = {en},
	number = {6},
	urldate = {2021-03-25},
	journal = {British Journal of Audiology},
	author = {Stone, Michael A. and Moore, Brian C. J.},
	month = jan,
	year = {1992},
	pages = {351--361},
	file = {Stone et Moore - 1992 - Syllabic compression Effective compression ratios.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IGQ7G2RS\\Stone et Moore - 1992 - Syllabic compression Effective compression ratios.pdf:application/pdf},
}

@book{van_trees_optimum_2004,
	title = {Optimum {Array} {Processing}: {Part} 4 of {Detection}, {Estimation}, and {Modulation} {Theory}},
	isbn = {978-0-471-22110-4 978-0-471-09390-9},
	shorttitle = {Optimum {Array} {Processing}},
	language = {en},
	publisher = {John Wiley \& Sons Incorporated.},
	author = {Van Trees, Harry L},
	year = {2004},
	note = {OCLC: 1229606273},
	file = {Van Trees - Optimum Array Processing Part 4 of Detection, Est.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WYHVXKZE\\Van Trees - Optimum Array Processing Part 4 of Detection, Est.pdf:application/pdf},
}

@book{knorr-cetina_epistemic_2003,
	address = {Cambridge, Mass.},
	edition = {3. print},
	title = {Epistemic cultures: how the sciences make knowledge},
	isbn = {978-0-674-25893-8 978-0-674-25894-5},
	shorttitle = {Epistemic cultures},
	language = {en},
	publisher = {Harvard Univ. Press},
	author = {Knorr-Cetina, Karin},
	year = {2003},
	note = {OCLC: 254506278},
	file = {Knorr-Cetina - 2003 - Epistemic cultures how the sciences make knowledg.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UWU8QA5Q\\Knorr-Cetina - 2003 - Epistemic cultures how the sciences make knowledg.pdf:application/pdf},
}

@misc{ansi_ansi_1997,
	title = {{ANSI} {S3}.5: {SII}—{Speech} intelligibility index standard},
	publisher = {American National Standards Institute, New York},
	author = {ANSI},
	year = {1997},
}

@article{durlach_equalization_1963,
	title = {Equalization and {Cancellation} {Theory} of {Binaural} {Masking}‐{Level} {Differences}},
	volume = {35},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1918675},
	doi = {10.1121/1.1918675},
	language = {en},
	number = {8},
	urldate = {2021-03-29},
	journal = {The Journal of the Acoustical Society of America},
	author = {Durlach, N. I.},
	month = aug,
	year = {1963},
	pages = {1206--1218},
	file = {Durlach - 1963 - Equalization and Cancellation Theory of Binaural M.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6YJC5Y3Z\\Durlach - 1963 - Equalization and Cancellation Theory of Binaural M.pdf:application/pdf},
}

@article{ludvigsen_evaluation_1993,
	title = {Evaluation of a noise reduction method: {Comparison} between observed scores and scores predicted from {STI}},
	volume = {38},
	journal = {Scandinavian Audiology},
	author = {Ludvigsen, Carl and Elberling, Claus and Keidser, Gitte},
	year = {1993},
}

@article{hartmann_externalization_1996,
	title = {On the {Externalization} of {Sound} {Images}},
	volume = {99},
	number = {6},
	journal = {The Journal of the Acoustical Society of America},
	author = {Hartmann, William M. and Wittenberg, Andrew},
	year = {1996},
	pages = {3678--3688},
	file = {hw9601.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3CS8G622\\hw9601.pdf:application/pdf},
}

@article{durlach_externalization_1992,
	title = {On the {Externalization} of {Auditory} {Images}},
	volume = {1},
	issn = {1054-7460},
	url = {https://direct.mit.edu/pvar/article/1/2/251-257/58771},
	doi = {10.1162/pres.1992.1.2.251},
	language = {en},
	number = {2},
	urldate = {2021-04-12},
	journal = {Presence: Teleoperators and Virtual Environments},
	author = {Durlach, N. I. and Rigopulos, A. and Pang, X. D. and Woods, W. S. and Kulkarni, A. and Colburn, H. S. and Wenzel, E. M.},
	month = jan,
	year = {1992},
	pages = {251--257},
	file = {Durlach et al. - 1992 - On the Externalization of Auditory Images.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GS59J9KW\\Durlach et al. - 1992 - On the Externalization of Auditory Images.pdf:application/pdf},
}

@article{gerlach_survey_2021,
	title = {A {Survey} on {Application} {Specific} {Processor} {Architectures} for {Digital} {Hearing} {Aids}},
	issn = {1939-8018, 1939-8115},
	url = {http://link.springer.com/10.1007/s11265-021-01648-0},
	doi = {10.1007/s11265-021-01648-0},
	abstract = {On the one hand, processors for hearing aids are highly specialized for audio processing, on the other hand they have to meet challenging hardware restrictions. This paper aims to provide an overview of the requirements, architectures, and implementations of these processors. Special attention is given to the increasingly common application-specific instructionset processors (ASIPs). The main focus of this paper lies on hardware-related aspects such as the processor architecture, the interfaces, the application specific integrated circuit (ASIC) technology, and the operating conditions. The different hearing aid implementations are compared in terms of power consumption, silicon area, and computing performance for the algorithms used. Challenges for the design of future hearing aid processors are discussed based on current trends and developments.},
	language = {en},
	urldate = {2021-04-08},
	journal = {Journal of Signal Processing Systems},
	author = {Gerlach, Lukas and Payá-Vayá, Guillermo and Blume, Holger},
	month = mar,
	year = {2021},
	pages = {1--16},
	file = {Gerlach et al. - 2021 - A Survey on Application Specific Processor Archite.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\Y434JMZ3\\Gerlach et al. - 2021 - A Survey on Application Specific Processor Archite.pdf:application/pdf},
}

@article{skottun_ability_2001,
	title = {The ability of inferior colliculus neurons to signal differences in interaural delay},
	volume = {98},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.241513998},
	doi = {10.1073/pnas.241513998},
	language = {en},
	number = {24},
	urldate = {2021-04-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Skottun, B. C. and Shackleton, T. M. and Arnott, R. H. and Palmer, A. R.},
	month = nov,
	year = {2001},
	pages = {14050--14054},
	file = {Skottun et al. - 2001 - The ability of inferior colliculus neurons to sign.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6MPK5K5S\\Skottun et al. - 2001 - The ability of inferior colliculus neurons to sign.pdf:application/pdf},
}

@article{hagerman_sentences_1982,
	title = {Sentences for {Testing} {Speech}-{Intelligibility} in {Noise}},
	volume = {11},
	journal = {Scandinavian Audiology},
	author = {Hagerman, Björn},
	year = {1982},
	keywords = {B format, ambisonics, sparsity},
	file = {hagerman1982.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\U8MCIDZB\\hagerman1982.pdf:application/pdf},
}

@article{kidd_stimulus_2010,
	title = {Stimulus factors influencing spatial release from speech-on-speech masking},
	volume = {128},
	issn = {0001-4966},
	url = {http://scitation.aip.org/content/asa/journal/jasa/128/4/10.1121/1.3478781},
	doi = {10.1121/1.3478781},
	language = {en},
	number = {4},
	urldate = {2021-03-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kidd, Gerald and Mason, Christine R. and Best, Virginia and Marrone, Nicole},
	month = oct,
	year = {2010},
	pages = {1965--1978},
	file = {Kidd et al. - 2010 - Stimulus factors influencing spatial release from .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7VNEDIPR\\Kidd et al. - 2010 - Stimulus factors influencing spatial release from .pdf:application/pdf},
}

@article{habets_generating_2008,
	title = {Generating nonstationary multisensor signals under a spatial coherence constraint},
	volume = {124},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2987429},
	doi = {10.1121/1.2987429},
	language = {en},
	number = {5},
	urldate = {2021-03-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Habets, Emanuël A. P. and Cohen, Israel and Gannot, Sharon},
	month = nov,
	year = {2008},
	pages = {2911--2917},
	file = {Habets et al. - 2008 - Generating nonstationary multisensor signals under.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EL3R9ER2\\Habets et al. - 2008 - Generating nonstationary multisensor signals under.pdf:application/pdf},
}

@article{gosling_binaural_2020,
	title = {Binaural {LCMV} {Beamforming} {With} {Partial} {Noise} {Estimation}},
	volume = {28},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/9244114/},
	doi = {10.1109/TASLP.2020.3034526},
	abstract = {Besides reducing undesired sources, i.e., interfering sources and background noise, another important objective of a binaural beamforming algorithm is to preserve the spatial impression of the acoustic scene, which can be achieved by preserving the binaural cues of all sound sources. While the binaural minimum variance distortionless response (BMVDR) beamformer provides a good noise reduction performance and preserves the binaural cues of the desired source, it does not allow to control the reduction of the interfering sources and distorts the binaural cues of the interfering sources and the background noise. Hence, several extensions have been proposed. First, the binaural linearly constrained minimum variance (BLCMV) beamformer uses additional constraints, enabling to control the reduction of the interfering sources while preserving their binaural cues. Second, the BMVDR with partial noise estimation (BMVDR-N) mixes the output signals of the BMVDR with the noisy reference microphone signals, enabling to control the binaural cues of the background noise. Aiming at merging the advantages of both extensions, in this paper we propose the BLCMV with partial noise estimation (BLCMV-N). We show that the output signals of the BLCMV-N can be interpreted as a mixture between the noisy reference microphone signals and the output signals of a BLCMV using an adjusted interference scaling parameter. We provide a theoretical comparison between the BMVDR, the BLCMV, the BMVDR-N and the proposed BLCMV-N in terms of noise and interference reduction performance and binaural cue preservation. Experimental results using recorded signals as well as the results of a perceptual listening test show that the BLCMV-N is able to preserve the binaural cues of an interfering source (like the BLCMV), while enabling to trade off between noise reduction performance and binaural cue preservation of the background noise (like the BMVDR-N).},
	language = {en},
	urldate = {2021-03-30},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Gößling, Nico and Hadad, Elior and Gannot, Sharon and Doclo, Simon},
	year = {2020},
	pages = {2942--2955},
	file = {Gosling et al. - 2020 - Binaural LCMV Beamforming With Partial Noise Estim.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2S7N6AJT\\Gosling et al. - 2020 - Binaural LCMV Beamforming With Partial Noise Estim.pdf:application/pdf},
}

@article{arweiler_influence_2011,
	title = {The influence of spectral characteristics of early reflections on speech intelligibility},
	volume = {130},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.3609258},
	doi = {10.1121/1.3609258},
	language = {en},
	number = {2},
	urldate = {2021-03-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Arweiler, Iris and Buchholz, Jörg M.},
	month = aug,
	year = {2011},
	pages = {996--1005},
	file = {Arweiler et Buchholz - 2011 - The influence of spectral characteristics of early.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5WXHB5Q9\\Arweiler et Buchholz - 2011 - The influence of spectral characteristics of early.pdf:application/pdf},
}

@article{gosling_perceptual_2020,
	title = {Perceptual {Evaluation} of {Binaural} {MVDR}-{Based} {Algorithms} to {Preserve} the {Interaural} {Coherence} of {Diffuse} {Noise} {Fields}},
	volume = {24},
	issn = {2331-2165, 2331-2165},
	url = {http://journals.sagepub.com/doi/10.1177/2331216520919573},
	doi = {10.1177/2331216520919573},
	abstract = {Besides improving speech intelligibility in background noise, another important objective of noise reduction algorithms for binaural hearing devices is preserving the spatial impression for the listener. In this study, we evaluate the performance of several recently proposed noise reduction algorithms based on the binaural minimum-variance-distortionless-response (MVDR) beamformer, which trade-off between noise reduction performance and preservation of the interaural coherence (IC) for diffuse noise fields. Aiming at a perceptually optimized result, this trade-off is determined based on the IC discrimination ability of the human auditory system. The algorithms are evaluated with normal-hearing participants for an anechoic scenario and a reverberant cafeteria scenario, in terms of both speech intelligibility using a matrix sentence test and spatial quality using a MUlti Stimulus test with Hidden Reference and Anchor (MUSHRA). The results show that all the binaural noise reduction algorithms are able to improve speech intelligibility compared with the unprocessed microphone signals, where partially preserving the IC of the diffuse noise field leads to a significant improvement in perceived spatial quality compared with the binaural MVDR beamformer while hardly affecting speech intelligibility.},
	language = {en},
	urldate = {2021-03-30},
	journal = {Trends in Hearing},
	author = {Gößling, Nico and Marquardt, Daniel and Doclo, Simon},
	month = jan,
	year = {2020},
	pages = {1--18},
	file = {gosling2020.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9A5ELM99\\gosling2020.pdf:application/pdf;Gößling et al. - 2020 - Perceptual Evaluation of Binaural MVDR-Based Algor.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CD63RGHF\\Gößling et al. - 2020 - Perceptual Evaluation of Binaural MVDR-Based Algor.pdf:application/pdf},
}

@article{gates_presbycusis_2005,
	title = {Presbycusis},
	volume = {366},
	doi = {10.1016/S0140-6736(05)67423-5},
	number = {9491},
	journal = {The Lancet},
	author = {Gates, George A and Mills, John H},
	year = {2005},
	pages = {1111--1120},
	file = {gates2005.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\ZXSWH9PH\\gates2005.pdf:application/pdf},
}

@article{allen_contributions_2008,
	title = {Contributions of talker characteristics and spatial location to auditory streaming},
	volume = {123},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2831774},
	doi = {10.1121/1.2831774},
	language = {en},
	number = {3},
	urldate = {2021-03-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Allen, Kachina and Carlile, Simon and Alais, David},
	month = mar,
	year = {2008},
	pages = {1562--1570},
	file = {Allen et al. - 2008 - Contributions of talker characteristics and spatia.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\35QFJYYL\\Allen et al. - 2008 - Contributions of talker characteristics and spatia.pdf:application/pdf},
}

@article{brown_north_2010,
	title = {The {North} {American} {Listening} in {Spatialized} {Noise}—{Sentences} {Test} ({NA} {LiSN}-{S}): {Normative} {Data} and {Test}-{Retest} {Reliability} {Studies} for {Adolescents} and {Young} {Adults}},
	volume = {21},
	issn = {1050-0545, 2157-3107},
	shorttitle = {The {North} {American} {Listening} in {Spatialized} {Noise}—{Sentences} {Test} ({NA} {LiSN}-{S})},
	url = {http://www.thieme-connect.de/DOI/DOI?10.3766/jaaa.21.10.3},
	doi = {10.3766/jaaa.21.10.3},
	abstract = {Background: The Listening in Spatialized Noise – Sentences test (LISN-S; Cameron \& Dillon, 2009) was originally developed to assess auditory stream segregation skills in children aged 6 to 11 years with suspected central auditory processing disorder. The LISN-S creates a three-dimensional auditory environment under headphones. A simple repetition-response protocol is used to assess a listener’s speech reception threshold (SRT) for target sentences presented in competing speech maskers. Performance is measured as the improvement in SRT in dB gained when either pitch, spatial, or both pitch and spatial cues are incorporated in the maskers. A North American-accented version of the LiSN-S (NA LiSN-S) is available for use in the United States of America and Canada.},
	language = {en},
	number = {10},
	urldate = {2021-03-30},
	journal = {Journal of the American Academy of Audiology},
	author = {Brown, David K. and Cameron, Sharon and Martin, Jeffrey S. and Watson, Charlene and Dillon, Harvey},
	month = nov,
	year = {2010},
	pages = {629--641},
	file = {Brown et al. - 2010 - The North American Listening in Spatialized Noise—.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JD6XLVPD\\Brown et al. - 2010 - The North American Listening in Spatialized Noise—.pdf:application/pdf},
}

@misc{international_telecommunications_union-recommendation_method_2003,
	title = {Method for the subjective assessment of intermediate quality level of coding systems},
	language = {en},
	author = {{International Telecommunications Union-Recommendation} and {BS.1534-1.}},
	year = {2003},
	file = {RECOMMENDATION ITU-R BS.1534-1 - Method for the su.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EYCKNI3G\\RECOMMENDATION ITU-R BS.1534-1 - Method for the su.pdf:application/pdf},
}

@article{werner_improved_2021,
	title = {Improved spatialization performance for joint speech dereverberation and noise reduction in binaural hearing aids},
	volume = {68},
	issn = {17468094},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1746809421003116},
	doi = {10.1016/j.bspc.2021.102714},
	abstract = {Objective: In this work we propose a unified solution for joint speech dereverberation, noise reduction and spatial preservation of either diffuse noise fields or single-point noise sources in binaural hearing aid applications, which is based on the multichannel Wiener filter (MWF) with preservation of the interaural coherence (MWF-IC) method.
Methods: The original interaural coherence (IC) auxiliary penalty cost function in the MWF-IC, which comprises information about both interfering noise and late reverberation component, is modified to consider the inter­ ference noise information only. It provides a new regularization form for preserving both interaural time dif­ ference (ITD) and IC binaural cues of the residual noise. This approach is theoretically supported by recent findings about the relationship between IC and ITD binaural cues for point acoustic sources.
Results: Extensive simulation results with objective measures, as well as psychoacoustic experiments with normal hearing volunteers indicate that the proposed method outperforms the preservation of spatial localization cues of its original version, keeping approximately the same performance in terms of noise-reduction and speechdereverberation.
Conclusion: The proposed scheme expands the MWF-IC original spatial cue preservation capability from only diffuse noise fields to either diffuse noise fields or a single-point noise source. Significance: This is the first MWF-based speech dereverberation and noise reduction method for binaural hearing aid that provides perceptually relevant preservation of acoustic scenarios comprised of one speech source and either a single-point noise source or a diffuse noise field.},
	language = {en},
	urldate = {2021-05-26},
	journal = {Biomedical Signal Processing and Control},
	author = {Werner, Johnny and Costa, Márcio Holsbach},
	month = jul,
	year = {2021},
	pages = {102714},
	file = {Werner et Costa - 2021 - Improved spatialization performance for joint spee.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WARUY7BI\\Werner et Costa - 2021 - Improved spatialization performance for joint spee.pdf:application/pdf},
}

@article{leng_compromise_2021,
	title = {On the compromise between noise reduction and speech/noise spatial information preservation in binaural speech enhancement},
	volume = {149},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/10.1121/10.0004854},
	doi = {10.1121/10.0004854},
	language = {en},
	number = {5},
	urldate = {2021-05-26},
	journal = {The Journal of the Acoustical Society of America},
	author = {Leng, Xin and Chen, Jingdong and Benesty, Jacob},
	month = may,
	year = {2021},
	pages = {3151--3162},
	file = {Leng et al. - 2021 - On the compromise between noise reduction and spee.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2T3N7WNA\\Leng et al. - 2021 - On the compromise between noise reduction and spee.pdf:application/pdf},
}

@article{brand_efficient_2002,
	title = {Efficient adaptive procedures for threshold and concurrent slope estimates for psychophysics and speech intelligibility tests},
	volume = {111},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1479152},
	doi = {10.1121/1.1479152},
	language = {en},
	number = {6},
	urldate = {2021-06-01},
	journal = {The Journal of the Acoustical Society of America},
	author = {Brand, Thomas and Kollmeier, Birger},
	month = jun,
	year = {2002},
	pages = {2801--2810},
	file = {Brand et Kollmeier - 2002 - Efficient adaptive procedures for threshold and co.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4ZHVBDV9\\Brand et Kollmeier - 2002 - Efficient adaptive procedures for threshold and co.pdf:application/pdf},
}

@article{bisgaard_standard_2010,
	title = {Standard {Audiograms} for the {IEC} 60118-15 {Measurement} {Procedure}},
	volume = {14},
	issn = {1084-7138},
	url = {http://journals.sagepub.com/doi/10.1177/1084713810379609},
	doi = {10.1177/1084713810379609},
	abstract = {For the characterization of hearing aids, a new test method has been defined in the new International Electrotechnical Commission (IEC) standard 60118-15. For this characterization, the hearing aid will be set to actual user settings as programmed by standard fitting software from the hearing aid manufacturer. To limit the variation of programming outcomes, 10 standard audiograms, which cover the entire range of audiograms met in clinical practice, have been defined. This article describes how the set of standard audiograms has been developed. This set of standard audiogram has been derived by a vector quantization analysis method on a database of 28,244 audiograms. Using this analysis method, sets of typical audiograms have been obtained of sizes 12 and 60. It turned out that the smaller set could not be used for selecting audiograms as sloping audiograms were absent. Therefore, the larger set has been analyzed to provide seven standard audiograms for flat and moderately sloping hearing loss and three standard audiograms for steep hearing loss.},
	language = {en},
	number = {2},
	urldate = {2021-06-01},
	journal = {Trends in Amplification},
	author = {Bisgaard, Nikolai and Vlaming, Marcel S. M. G. and Dahlquist, Martin},
	month = jun,
	year = {2010},
	pages = {113--120},
	file = {Bisgaard et al. - 2010 - Standard Audiograms for the IEC 60118-15 Measureme.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7FDRWHN7\\Bisgaard et al. - 2010 - Standard Audiograms for the IEC 60118-15 Measureme.pdf:application/pdf},
}

@article{kollmeier_sentence_2016,
	title = {Sentence {Recognition} {Prediction} for {Hearing}-impaired {Listeners} in {Stationary} and {Fluctuation} {Noise} {With} {FADE}: {Empowering} the {Attenuation} and {Distortion} {Concept} by {Plomp} {With} a {Quantitative} {Processing} {Model}},
	volume = {20},
	issn = {2331-2165, 2331-2165},
	shorttitle = {Sentence {Recognition} {Prediction} for {Hearing}-impaired {Listeners} in {Stationary} and {Fluctuation} {Noise} {With} {FADE}},
	url = {http://journals.sagepub.com/doi/10.1177/2331216516655795},
	doi = {10.1177/2331216516655795},
	abstract = {To characterize the individual patient’s hearing impairment as obtained with the matrix sentence recognition test, a simulation Framework for Auditory Discrimination Experiments (FADE) is extended here using the Attenuation and Distortion (AþD) approach by Plomp as a blueprint for setting the individual processing parameters. FADE has been shown to predict the outcome of both speech recognition tests and psychoacoustic experiments based on simulations using an automatic speech recognition system requiring only few assumptions. It builds on the closed-set matrix sentence recognition test which is advantageous for testing individual speech recognition in a way comparable across languages. Individual predictions of speech recognition thresholds in stationary and in fluctuating noise were derived using the audiogram and an estimate of the internal level uncertainty for modeling the individual Plomp curves fitted to the data with the Attenuation (A-) and Distortion (D-) parameters of the Plomp approach. The “typical” audiogram shapes from Bisgaard et al with or without a “typical” level uncertainty and the individual data were used for individual predictions. As a result, the individualization of the level uncertainty was found to be more important than the exact shape of the individual audiogram to accurately model the outcome of the German Matrix test in stationary or fluctuating noise for listeners with hearing impairment. The prediction accuracy of the individualized approach also outperforms the (modified) Speech Intelligibility Index approach which is based on the individual threshold data only.},
	language = {en},
	urldate = {2021-07-08},
	journal = {Trends in Hearing},
	author = {Kollmeier, Birger and Schädler, Marc René and Warzybok, Anna and Meyer, Bernd T. and Brand, Thomas},
	month = jan,
	year = {2016},
	pages = {17},
	file = {Kollmeier et al. - 2016 - Sentence Recognition Prediction for Hearing-impair.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CV4MIN7V\\Kollmeier et al. - 2016 - Sentence Recognition Prediction for Hearing-impair.pdf:application/pdf},
}

@article{jia_multiple_2018,
	title = {Multiple {Speech} {Source} {Separation} {Using} {Inter}-{Channel} {Correlation} and {Relaxed} {Sparsity}},
	volume = {8},
	doi = {10.3390/app8010123},
	number = {123},
	journal = {Applied Sciences},
	author = {Jia, Maoshen and Sun, Jundai and Zheng, Xiguang},
	month = jan,
	year = {2018},
	pages = {23},
	file = {jia2018.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\E6MWTKK8\\jia2018.pdf:application/pdf},
}

@article{plomp_negative_1988,
	title = {The negative effect of amplitude compression in multichannel hearing aids in the light of the modulation‐transfer function},
	volume = {83},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.396363},
	doi = {10.1121/1.396363},
	language = {en},
	number = {6},
	urldate = {2021-09-02},
	journal = {The Journal of the Acoustical Society of America},
	author = {Plomp, Reinier},
	month = jun,
	year = {1988},
	pages = {2322--2327},
	file = {Plomp - 1988 - The negative effect of amplitude compression in mu.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\I7DU7Y2R\\Plomp - 1988 - The negative effect of amplitude compression in mu.pdf:application/pdf},
}

@incollection{edwards_hearing_2004,
	address = {New York},
	title = {Hearing {Aids} and {Hearing} {Impairment}},
	volume = {18},
	isbn = {978-0-387-00590-4},
	url = {http://link.springer.com/10.1007/0-387-21575-1_7},
	language = {en},
	urldate = {2021-09-02},
	booktitle = {Speech {Processing} in the {Auditory} {System}},
	publisher = {Springer-Verlag},
	author = {Edwards, Brent},
	year = {2004},
	doi = {10.1007/0-387-21575-1_7},
	note = {Series Title: Springer Handbook of Auditory Research},
	pages = {339--421},
	file = {Edwards - 2004 - Hearing Aids and Hearing Impairment.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7NPN8EGZ\\Edwards - 2004 - Hearing Aids and Hearing Impairment.pdf:application/pdf},
}

@book{greenberg_speech_2004,
	address = {New York},
	series = {Springer handbook of auditory research},
	title = {Speech processing in the auditory system},
	isbn = {978-0-387-00590-4},
	language = {en},
	number = {v. 18},
	publisher = {Springer},
	editor = {Greenberg, Steven},
	year = {2004},
	keywords = {Speech perception, Audiometry, Auditory pathways, Handbooks, manuals, etc, Hearing, Speech processing systems},
	file = {Greenberg - 2004 - Speech processing in the auditory system.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\SKXB2TLT\\Greenberg - 2004 - Speech processing in the auditory system.pdf:application/pdf},
}

@inproceedings{llave_speech_2021,
	address = {Tokyo, Japan},
	title = {On the {Speech} {Sparsity} for {Computational} {Efﬁciency} and {Noise} {Reduction} in {Hearing} {Aids}},
	abstract = {Beamforming techniques are widely used in hearing aids to improve the signal-to-noise ratio. In a multi-speaker scenario, it is common to assume that the speech signals associated with each speaker do not overlap in the time-frequency domain. This so-called W-disjoint orthogonality assumption allows us to reduce the complexity of the beamforming algorithm. However, its validity decreases in presence of more than two speakers. In this study, we propose a beamforming algorithm relying on a less restrictive assumption regarding the sparsity of speech signals in the time-frequency domain. Its implications over the noise reduction performance and the computational complexity are discussed and compared with the Linearly Constrained Minimum Variance (LCMV) and the Minimum Variance Distortionless Response (MVDR) beamformers. We show that the proposed algorithm improves the noise reduction performance and reduces the computational cost compared to the LCMV beamformer without increasing the artifacts amount unlike the MVDR beamformer.},
	language = {en},
	booktitle = {13th {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	publisher = {IEEE},
	author = {Llave, Adrien and Leglaive, Simon},
	year = {2021},
	pages = {5},
	file = {Llave et Leglaive - On the Speech Sparsity for Computational Efﬁciency.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RWMRH6PC\\Llave et Leglaive - On the Speech Sparsity for Computational Efﬁciency.pdf:application/pdf},
}

@inproceedings{porter_optimal_1984,
	address = {San Diego, CA, USA},
	title = {Optimal {Estimators} for {Spectral} {Restoration} of {Noisy} {Speech}},
	language = {en},
	booktitle = {{ICASSP} '84. {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Porter, Jack E. and Boll, Steve F.},
	month = mar,
	year = {1984},
	pages = {4},
	file = {Diego - Jack E. Porter and Steven F. Boll.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YGLWJG62\\Diego - Jack E. Porter and Steven F. Boll.pdf:application/pdf},
}

@inproceedings{lotter_noise_2003,
	address = {Kyoto, Japan},
	title = {Noise {Reduction} by {Maximum} a {Posteriori} {Spectral} {Amplitude} {Estimation} with {Supergaussian} {Speech} {Modeling}},
	abstract = {This contribution presents a spectral amplitude estimator for acoustical background noise suppression based on maximum a posteriori estimation and supergaussian statistical modeling of the speech DFT coefﬁcients. The probability density function of the speech spectral amplitude is modeled with a simple parametric function, which allows a high approximation accuracy for Laplace or Gamma distributed real and imaginary parts of the speech DFT coefﬁcients. Based on the approximation, a computationally efﬁcient maximum a posteriori speech estimator is derived, which outperforms the Ephraim-Malah algorithm in a single channel noise reduction framework.},
	language = {en},
	booktitle = {International {Workshop} on {Acoustic} {Echo} and {Noise} {Control}},
	publisher = {IEEE},
	author = {Lotter, Thomas and Vary, Peter},
	month = sep,
	year = {2003},
	pages = {4},
	file = {Lotter et Vary - NOISE REDUCTION BY MAXIMUM A POSTERIORI SPECTRAL A.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UYA9IBFT\\Lotter et Vary - NOISE REDUCTION BY MAXIMUM A POSTERIORI SPECTRAL A.pdf:application/pdf},
}

@inproceedings{vary_noise_2004,
	address = {Vienna, Austria},
	title = {Noise {Reduction} by {Joint} {Maximum} a {Posteriori} {Spectral} {Amplitude} and {Phase} {Estimation} {With} {Super}-{Gaussian} {Speech} {Modelling}},
	abstract = {For acoustical background noise reduction a computationally efﬁcient joint MAP estimator with a super-Gaussian speech model is presented. Compared to a recently introduced MAP estimator the new joint MAP estimator allows an optimal adjustment of the underlying statistical model to the real PDF of the speech spectral amplitude. The computationally efﬁcient estimator outperforms the Ephraim-Malah estimator and the recently proposed MAP estimator in a single microphone noise reduction framework due to the more accurate statistical model.},
	language = {en},
	booktitle = {2004 12th {European} {Signal} {Processing} {Conference}},
	publisher = {IEEE},
	author = {Vary, Peter and Lotter, Thomas},
	month = sep,
	year = {2004},
	pages = {4},
	file = {Vary et Lotter - Noise Reduction by Joint Maximum a Posteriori Spec.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GCJ76VCJ\\Vary et Lotter - Noise Reduction by Joint Maximum a Posteriori Spec.pdf:application/pdf},
}

@inproceedings{martin_speech_2002,
	address = {Orlando, FL, USA},
	title = {Speech {Enhancement} {Using} {MMSE} {Short} {Time} {Spectral} {Estimation} with {Gamma} {Distributed} {Speech} {Priors}},
	doi = {10.1109/ICASSP.2002.5743702},
	abstract = {In this paper we consider optimal estimators for speoch enhance­ ment in the Discrete Fourier Transform. (OFT) domain. We present an analytical solution for estimating complex OFT coeficfients in the MMSE sense when the clean spece b OFT coefficients are Gam­ ma distributed and the OFT coefficients of the noise are Gaussian or Laplace distributed. Compared to the state-of-the-art Wiener or MMSE short time amplitude estimators the new estimators de­ liver improved signai-to-noise ratios. When the noise model is a Laplacian density the enhanced speech shows less annoying ran­ dom fluctuations in the residual noise than for a Gaussian density.},
	language = {en},
	booktitle = {2002 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Martin, Rainer},
	year = {2002},
	pages = {4},
	file = {Martin - SPEECH ENHANCEMENT USING MMSE SHORT TIME SPECTRAL .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PBN3AQP3\\Martin - SPEECH ENHANCEMENT USING MMSE SHORT TIME SPECTRAL .pdf:application/pdf},
}

@article{voss_evaluating_2021,
	title = {Evaluating the benefit of hearing aids with motion-based beamformer adaptation in a real-world setup},
	issn = {1499-2027, 1708-8186},
	url = {https://www.tandfonline.com/doi/full/10.1080/14992027.2021.1948120},
	doi = {10.1080/14992027.2021.1948120},
	abstract = {Objective: Conventional directional hearing aid microphone technology may obstruct listening intentions when the talker and listener walk side by side. The purpose of the current study was to evaluate hearing aids that use a motion sensor to address listening needs during walking. Design: Each participant completed two walks in randomised order, one walk with each of two hearing aid programs: (1) conventional beamformer adaptation that activated an adaptive, multiband beamformer in loud environments and (2) motion-based beamformer adaptation that activated a pinna-mimicking microphone setting when walking was detected. Participants walked along a pre-defined track and completed tasks assessing speech understanding and environmental awareness. Study Sample: Participants were 22 older adults with moderate-to-severe hearing loss and experience using hearing aids.
Results: More participants preferred the motion-based than conventional beamformer adaptation for speech understanding, environmental awareness, overall listening, and sound quality (p {\textless} 0.05). Measures of speech understanding (p {\textless} 0.01) and localisation of sound stimuli (p {\textless} 0.05) were significantly better with motion-based than conventional beamformer adaptation.
Conclusions: The results suggest that hearing aid users can benefit from beamforming that uses motion sensor input to adapt the signal processing according to the user’s activity. The real-world setup of this study had limitations.},
	language = {en},
	urldate = {2021-09-09},
	journal = {International Journal of Audiology},
	author = {Voss, Solveig C. and Pichora-Fuller, M. Kathleen and Ishida, Ieda and Pereira, April and Seiter, Julia and El Guindi, Nadim and Kuehnel, Volker and Qian, Jinyu},
	month = aug,
	year = {2021},
	pages = {1--13},
	file = {Voss et al. - 2021 - Evaluating the benefit of hearing aids with motion.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\96RMFDAU\\Voss et al. - 2021 - Evaluating the benefit of hearing aids with motion.pdf:application/pdf},
}

@article{kuk_reconsidering_2003,
	title = {Reconsidering the {Concept} of the {Aided} {Threshold} for {Nonlinear} {Hearing} {Aids}},
	volume = {7},
	issn = {1084-7138},
	url = {http://journals.sagepub.com/doi/10.1177/108471380300700302},
	doi = {10.1177/108471380300700302},
	abstract = {The aided threshold (and functional gain) has been discussed in the context of linear hearing aids since the early 1960s. The use of nonlinear hearing aids, however, could change the meaningfulness of this verification tool because of their unique characteristics. The interpretation of the aided threshold (and functional gain) as it pertains to linear and nonlinear hearing aids is reviewed. Also discussed are the ideas of an optimal aided threshold, factors that may affect its magnitude, and a comparison between functional gain and insertion gain measures. Finally, how to improve the accuracy of the aided thresholds (and functional gain) through the use of in-situ unaided threshold measurements is discussed.},
	language = {en},
	number = {3},
	urldate = {2021-09-14},
	journal = {Trends in Amplification},
	author = {Kuk, Francis and Ludvigsen, Carl},
	month = jun,
	year = {2003},
	pages = {77--97},
	file = {Kuk et Ludvigsen - 2003 - Reconsidering the Concept of the Aided Threshold f.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\T3LLX7GH\\Kuk et Ludvigsen - 2003 - Reconsidering the Concept of the Aided Threshold f.pdf:application/pdf},
}

@article{zimpfer_impact_2014,
	title = {Impact of hearing protection devices on sound localization performance},
	volume = {8},
	abstract = {Hearing Protection Devices (HPDs) can protect the ear against loud potentially damaging sounds while allowing lower-level sounds such as speech to be perceived. However, the impact of these devices on the ability to localize sound sources is not well known. To address this question, we propose two different methods: one behavioral and one dealing with acoustical measurements. For the behavioral method, sound localization performance was measured with, and without, HPDs on 20 listeners. Five HPDs, including both passive (non-linear attenuation) and three active (talk-through) systems were evaluated. The results showed a signiﬁcant increase in localization errors, especially front-back and up-down confusions relative to the “naked ear” test condition for all of the systems tested, especially for the talk-through headphone system. For the acoustic measurement method, Head-Related Transfer Functions (HRTFs) were measured on an artiﬁcial head both without, and with the HPDs in place. The effects of the HPDs on the spectral cues for the localization of different sound sources in the horizontal plane were analyzed. Alterations of the Interaural Spectral Difference (ISD) cues were identiﬁed, which could explain the observed increase in front-back confusions caused by the talk-through headphone protectors.},
	language = {en},
	journal = {Frontiers in Neuroscience},
	author = {Zimpfer, Véronique},
	year = {2014},
	pages = {10},
	file = {Zimpfer - Impact of hearing protection devices on sound loca.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\4G7BJENP\\Zimpfer - Impact of hearing protection devices on sound loca.pdf:application/pdf},
}

@article{serizel_speech_2013,
	title = {A speech distortion weighting based approach to integrated active noise control and noise reduction in hearing aids},
	volume = {93},
	issn = {01651684},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165168413000856},
	doi = {10.1016/j.sigpro.2013.03.010},
	language = {en},
	number = {9},
	urldate = {2021-09-15},
	journal = {Signal Processing},
	author = {Serizel, Romain and Moonen, Marc and Wouters, Jan and Jensen, Søren Holdt},
	month = sep,
	year = {2013},
	pages = {2440--2452},
	file = {Serizel et al. - 2013 - A speech distortion weighting based approach to in.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MEBS3ULI\\Serizel et al. - 2013 - A speech distortion weighting based approach to in.pdf:application/pdf},
}

@article{serizel_zone--quiet_2012,
	title = {A {Zone}-of-{Quiet} {Based} {Approach} to {Integrated} {Active} {Noise} {Control} and {Noise} {Reduction} for {Speech} {Enhancement} in {Hearing} {Aids}},
	volume = {20},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/6151812/},
	doi = {10.1109/TASL.2012.2187193},
	abstract = {This paper focuses on speech enhancement in hearing aids and presents an integrated approach to active noise control and noise reduction which is based on an optimisation over a zone-of-quiet generated by the active noise control. A basic integrated active noise control and noise reduction scheme has been introduced previously to tackle secondary path effects and effects of noise leakage through an open ﬁtting. This scheme however, only takes the sound pressure at the ear canal microphone into account. For an integrated active noise control and noise reduction scheme to be efﬁcient, it is desired to achieve active noise control at the eardrum which in practice is away from the ear canal microphone. In some cases it can also be desired to achieve noise control over a zone not limited to a single point. Two different schemes are presented. The ﬁrst scheme is based on a mean squared error criterion expressed at a RP away from the ear canal microphone and the second scheme is based on an average mean squared error criterion over a desired zone-of-quiet. They are both compared experimentally with the original scheme for both active noise control and integrated active noise control and noise reduction, respectively. The remotepoint approach then allows to restore the performance of the original scheme at the desired remote point while the zone-ofquiet approach allows to increase performance up to 3dB on the desired zone-of-quiet.},
	language = {en},
	number = {6},
	urldate = {2021-09-15},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Serizel, Romain and Moonen, Marc and Wouters, Jan and Jensen, Søren Holdt},
	month = aug,
	year = {2012},
	pages = {1685--1697},
	file = {Serizel et al. - 2012 - A Zone-of-Quiet Based Approach to Integrated Activ.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\786MDA47\\Serizel et al. - 2012 - A Zone-of-Quiet Based Approach to Integrated Activ.pdf:application/pdf},
}

@article{serizel_binaural_2013,
	title = {Binaural {Integrated} {Active} {Noise} {Control} and {Noise} {Reduction} in {Hearing} {Aids}},
	volume = {21},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/6384710/},
	doi = {10.1109/TASL.2012.2234111},
	abstract = {This paper presents a binaural approach to integrated active noise control and noise reduction in hearing aids and aims at demonstrating that a binaural setup indeed provides signiﬁcant advantages in terms of the number of noise sources that can be compensated for and in terms of the causality margins.},
	language = {en},
	number = {5},
	urldate = {2021-09-15},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Serizel, Romain and Moonen, Marc and Wouters, Jan and Jensen, Søren Holdt},
	month = may,
	year = {2013},
	pages = {1113--1118},
	file = {Serizel et al. - 2013 - Binaural Integrated Active Noise Control and Noise.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\R7BSPMCT\\Serizel et al. - 2013 - Binaural Integrated Active Noise Control and Noise.pdf:application/pdf},
}

@article{lin_source_2020,
	title = {Source separation in ecoacoustics: a roadmap towards versatile soundscape information retrieval},
	volume = {6},
	issn = {2056-3485, 2056-3485},
	shorttitle = {Source separation in ecoacoustics},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/rse2.141},
	doi = {10.1002/rse2.141},
	abstract = {A comprehensive assessment of ecosystem dynamics requires the monitoring of biological, physical and social changes. Changes that cannot be observed visually may be trackable acoustically through soundscape analysis. Soundscapes vary greatly depending on geophysical events, biodiversity and human activities. However, retrieving source-speciﬁc information from geophony, biophony and anthropophony remains a challenging task, due to interference by simultaneous sound sources. Audio source separation is a technique that aims to recover individual sound sources when only mixtures are accessible. Here, we review techniques of monoaural audio source separation with the fundamental theories and assumptions behind them. Depending on the availability of prior information about the source signals, the task can be approached as a blind source separation or a model-based source separation. Most blind source separation techniques depend on assumptions about the behaviour of the source signals, and their performance may deteriorate when the assumptions fail. Model-based techniques generally do not require speciﬁc assumptions, and the models are directly learned from labelled data. With the recent advances of deep learning, the model-based techniques can yield state-of-the-art separation performance, accordingly facilitate content-based audio information retrieval. Source separation techniques have been adopted in several ecoacoustic applications to evaluate the contributions from biodiversity and anthropogenic disturbance to soundscape dynamics. They can also be employed as nonlinear ﬁlters to improve the recognition of bioacoustic signals. To effectively retrieve ecological information from soundscapes, source separation is a crucial tool. We believe that the future integrations of ecological hypotheses and deep learning can realize a high-performance source separation for ecoacoustics, and accordingly improve soundscape-based ecosystem monitoring. Therefore, we outline a roadmap for applying source separation to assist in soundscape information retrieval and hope to promote cross-disciplinary collaboration.},
	language = {en},
	number = {3},
	urldate = {2021-09-15},
	journal = {Remote Sensing in Ecology and Conservation},
	author = {Lin, Tzu‐Hao and Tsao, Yu},
	editor = {Pettorelli, Nathalie},
	month = sep,
	year = {2020},
	pages = {236--247},
	file = {Lin et Tsao - 2020 - Source separation in ecoacoustics a roadmap towar.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AU86F33E\\Lin et Tsao - 2020 - Source separation in ecoacoustics a roadmap towar.pdf:application/pdf},
}

@article{desjonqueres_monitoring_2020,
	title = {Monitoring the acoustic activity of an aquatic insect population in relation to temperature, vegetation and noise},
	volume = {65},
	issn = {0046-5070, 1365-2427},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/fwb.13171},
	doi = {10.1111/fwb.13171},
	language = {en},
	number = {1},
	urldate = {2021-09-15},
	journal = {Freshwater Biology},
	author = {Desjonquères, Camille and Rybak, Fanny and Ulloa, Juan Sebastian and Kempf, Alexandre and Bar Hen, Avner and Sueur, Jérôme},
	month = jan,
	year = {2020},
	pages = {107--116},
	file = {Desjonquères et al. - 2020 - Monitoring the acoustic activity of an aquatic ins.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\SNUIID7Z\\Desjonquères et al. - 2020 - Monitoring the acoustic activity of an aquatic ins.pdf:application/pdf},
}

@article{aubin_cocktailparty_1998,
	title = {Cocktail–party effect in king penguin colonies},
	volume = {265},
	issn = {0962-8452, 1471-2954},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspb.1998.0486},
	doi = {10.1098/rspb.1998.0486},
	language = {en},
	number = {1406},
	urldate = {2021-09-15},
	journal = {Proceedings of the Royal Society of London. Series B: Biological Sciences},
	author = {Aubin, Thierry and Jouventin, Pierre},
	month = sep,
	year = {1998},
	pages = {1665--1673},
	file = {Aubin et Jouventin - 1998 - Cocktail–party effect in king penguin colonies.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\IT69XZQP\\Aubin et Jouventin - 1998 - Cocktail–party effect in king penguin colonies.pdf:application/pdf},
}

@article{sumitani_non-invasive_2021,
	title = {Non-{Invasive} {Monitoring} of the {Spatio}-{Temporal} {Dynamics} of {Vocalizations} among {Songbirds} in a {Semi} {Free}-{Flight} {Environment} {Using} {Robot} {Audition} {Techniques}},
	volume = {2},
	issn = {2673-6004},
	url = {https://www.mdpi.com/2673-6004/2/2/12},
	doi = {10.3390/birds2020012},
	abstract = {To understand the social interactions among songbirds, extracting the timing, position, and acoustic properties of their vocalizations is essential. We propose a framework for automatic and ﬁne-scale extraction of spatial-spectral-temporal patterns of bird vocalizations in a densely populated environment. For this purpose, we used robot audition techniques to integrate information (i.e., the timing, direction of arrival, and separated sound of localized sources) from multiple microphone arrays (array of arrays) deployed in an environment, which is non-invasive. As a proof of concept of this framework, we examined the ability of the method to extract active vocalizations of multiple Zebra Finches in an outdoor mesh tent as a realistic situation in which they could ﬂy and vocalize freely. We found that localization results of vocalizations reﬂected the arrangements of landmark spots in the environment such as nests or perches and some vocalizations were localized at nonlandmark positions. We also classiﬁed their vocalizations as either songs or calls by using a simple method based on the tempo and length of the separated sounds, as an example of the use of the information obtained from the framework. Our proposed approach has great potential to understand their social interactions and the semantics or functions of their vocalizations considering the spatial relationships, although detailed understanding of the interaction would require analysis of more long-term recordings.},
	language = {en},
	number = {2},
	urldate = {2021-09-15},
	journal = {Birds},
	author = {Sumitani, Shinji and Suzuki, Reiji and Arita, Takaya and Nakadai, Kazuhiro and Okuno, Hiroshi G.},
	month = apr,
	year = {2021},
	pages = {158--172},
	file = {Sumitani et al. - 2021 - Non-Invasive Monitoring of the Spatio-Temporal Dyn.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CHGY44DG\\Sumitani et al. - 2021 - Non-Invasive Monitoring of the Spatio-Temporal Dyn.pdf:application/pdf},
}

@incollection{virtanen_computational_2018,
	address = {Cham},
	title = {Computational {Bioacoustic} {Scene} {Analysis}},
	isbn = {978-3-319-63449-4 978-3-319-63450-0},
	url = {http://link.springer.com/10.1007/978-3-319-63450-0_11},
	abstract = {The analysis of natural and animal sound makes a demonstrable contribution to important challenges in conservation, animal behaviour, and evolution. And now bioacoustics has entered its big data era. Thus automation is important, as is scalability in many cases to very large amounts of audio data and to realtime processing. This chapter will focus on the data science and the computational methods that can enable this. Computational bioacoustics has some commonalities with wider audio scene analysis, as well as with speech processing and other disciplines. However, the tasks required and the speciﬁc characteristics of bioacoustic data require new and adapted techniques. This chapter will survey the tasks and the methods of computational bioacoustics, and will place particular emphasis on existing work and future prospects which address scalable analysis. We will mostly focus on airborne sound; there has also been much work on freshwater and marine bioacoustics, and a small amount on solid-borne sounds.},
	language = {en},
	urldate = {2021-09-15},
	booktitle = {Computational {Analysis} of {Sound} {Scenes} and {Events}},
	publisher = {Springer International Publishing},
	author = {Stowell, Dan},
	editor = {Virtanen, Tuomas and Plumbley, Mark D. and Ellis, Dan},
	year = {2018},
	doi = {10.1007/978-3-319-63450-0_11},
	pages = {303--333},
	file = {Stowell - 2018 - Computational Bioacoustic Scene Analysis.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VRCX9B3C\\Stowell - 2018 - Computational Bioacoustic Scene Analysis.pdf:application/pdf},
}

@article{sockalingam_binaural_2009,
	title = {Binaural hearing aid communication shown to improve sound quality and localization},
	volume = {62},
	journal = {Hearing Journal},
	author = {Sockalingam, R. and Holmberg, Marcus and Eneroth, K. and Shulte, M.},
	year = {2009},
	pages = {46--47},
}

@article{jensen_harmonic_2020,
	title = {Harmonic beamformers for speech enhancement and dereverberation in the time domain},
	volume = {116},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639318303856},
	doi = {10.1016/j.specom.2019.11.003},
	language = {en},
	urldate = {2021-11-09},
	journal = {Speech Communication},
	author = {Jensen, J.R. and Karimian-Azari, S. and Christensen, M.G. and Benesty, J.},
	month = jan,
	year = {2020},
	pages = {1--11},
	file = {Jensen et al. - 2020 - Harmonic beamformers for speech enhancement and de.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KGAMX6FE\\Jensen et al. - 2020 - Harmonic beamformers for speech enhancement and de.pdf:application/pdf},
}

@article{das_linear_2020,
	title = {Linear versus deep learning methods for noisy speech separation for {EEG}-informed attention decoding},
	volume = {17},
	abstract = {Objective: A hearing aid’s noise reduction algorithm cannot infer to which speaker the user intends to listen to. Auditory attention decoding (AAD) algorithms allow to infer this information from neural signals, which leads to the concept of neuro-steered hearing aids. We aim to evaluate and demonstrate the feasibility of AAD-supported speech enhancement in challenging noisy conditions based on electroencephalography (EEG) recordings. Approach: The AAD performance with a linear versus a deep neural network (DNN) based speaker separation was evaluated for same-gender speaker mixtures using 3 diﬀerent speaker positions and 3 diﬀerent noise conditions. Main results: AAD results based on the linear approach were found to be at least on par and sometimes even better than pure DNN-based approaches in terms of AAD accuracy in all tested conditions. However, when using the DNN to support a linear data-driven beamformer, a performance improvement over the purely linear approach was obtained in the most challenging scenarios. The use of multiple microphones was also found to improve speaker separation and AAD performance over single-microphone systems. Signiﬁcance: Recent proof-of-concept studies in this context each focus on a diﬀerent method in a diﬀerent experimental setting, which makes it hard to compare them. Furthermore, they are tested in highly idealized experimental conditions, which are still far from a realistic hearing aid setting. This work provides a systematic comparison of a linear and non-linear neuro-steered speech enhancement model, as well as a more realistic validation in challenging conditions.},
	language = {en},
	number = {4},
	journal = {Journal of Neural Engineering},
	author = {Das, Neetha and Zegers, Jeroen and Bertrand, Alexander},
	year = {2020},
	pages = {046039},
	file = {Das et al. - Linear versus deep learning methods for noisy spee.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CWTUEAFD\\Das et al. - Linear versus deep learning methods for noisy spee.pdf:application/pdf},
}

@article{van_eyndhoven_eeg-informed_2017,
	title = {{EEG}-{Informed} {Attended} {Speaker} {Extraction} {From} {Recorded} {Speech} {Mixtures} {With} {Application} in {Neuro}-{Steered} {Hearing} {Prostheses}},
	volume = {64},
	issn = {0018-9294, 1558-2531},
	url = {http://ieeexplore.ieee.org/document/7505982/},
	doi = {10.1109/TBME.2016.2587382},
	abstract = {Objective: We aim to extract and denoise the attended speaker in a noisy, two-speaker acoustic scenario, relying on microphone array recordings from a binaural hearing aid, which are complemented with electroencephalography (EEG) recordings to infer the speaker of interest. Methods: In this study, we propose a modular processing ﬂow that ﬁrst extracts the two speech envelopes from the microphone recordings, then selects the attended speech envelope based on the EEG, and ﬁnally uses this envelope to inform a multi-channel speech separation and denoising algorithm. Results: Strong suppression of interfering (unattended) speech and background noise is achieved, while the attended speech is preserved. Furthermore, EEG-based auditory attention detection (AAD) is shown to be robust to the use of noisy speech signals. Conclusions: Our results show that AAD-based speaker extraction from microphone array recordings is feasible and robust, even in noisy acoustic environments, and without access to the clean speech signals to perform EEG-based AAD. Signiﬁcance: Current research on AAD always assumes the availability of the clean speech signals, which limits the applicability in real settings. We have extended this research to detect the attended speaker even when only microphone recordings with noisy speech mixtures are available. This is an enabling ingredient for new braincomputer interfaces and eﬀective ﬁltering schemes in neuro-steered hearing prostheses. Here, we provide a ﬁrst proof of concept for EEG-informed attended speaker extraction and denoising.},
	language = {en},
	number = {5},
	urldate = {2021-11-09},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Van Eyndhoven, Simon and Francart, Tom and Bertrand, Alexander},
	month = may,
	year = {2017},
	pages = {1045--1056},
	file = {Van Eyndhoven et al. - 2017 - EEG-Informed Attended Speaker Extraction From Reco.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NG67N3Y9\\Van Eyndhoven et al. - 2017 - EEG-Informed Attended Speaker Extraction From Reco.pdf:application/pdf},
}

@article{mesgarani_selective_2012,
	title = {Selective cortical representation of attended speaker in multi-talker speech perception},
	volume = {485},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature11020},
	doi = {10.1038/nature11020},
	abstract = {Humans possess a remarkable ability to attend to a single speaker’s voice in a multi-talker background1–3. How the auditory system manages to extract intelligible speech under such acoustically complex and adverse listening conditions is not known, and, indeed, it is not clear how attended speech is internally represented4,5. Here, using multi-electrode surface recordings from the cortex of subjects engaged in a listening task with two simultaneous speakers, we demonstrate that population responses in non-primary human auditory cortex encode critical features of attended speech: speech spectrograms reconstructed based on cortical responses to the mixture of speakers reveal the salient spectral and temporal features of the attended speaker, as if subjects were listening to that speaker alone. A simple classifier trained solely on examples of single speakers can decode both attended words and speaker identity. We find that task performance is well predicted by a rapid increase in attention-modulated neural selectivity across both single-electrode and population-level cortical responses. These findings demonstrate that the cortical representation of speech does not merely reflect the external acoustic environment, but instead gives rise to the perceptual aspects relevant for the listener’s intended goal.},
	language = {en},
	number = {7397},
	urldate = {2021-11-09},
	journal = {Nature},
	author = {Mesgarani, Nima and Chang, Edward F.},
	month = may,
	year = {2012},
	pages = {233--236},
	file = {Mesgarani et Chang - 2012 - Selective cortical representation of attended spea.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\E8LKCKG9\\Mesgarani et Chang - 2012 - Selective cortical representation of attended spea.pdf:application/pdf},
}

@article{rennies_benefit_2018,
	title = {Benefit of binaural listening as revealed by speech intelligibility and listening effort},
	volume = {144},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.5057114},
	doi = {10.1121/1.5057114},
	language = {en},
	number = {4},
	urldate = {2021-11-10},
	journal = {The Journal of the Acoustical Society of America},
	author = {Rennies, Jan and Kidd, Gerald},
	month = oct,
	year = {2018},
	pages = {2147--2159},
	file = {Rennies et Kidd - 2018 - Benefit of binaural listening as revealed by speec.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TELADARG\\Rennies et Kidd - 2018 - Benefit of binaural listening as revealed by speec.pdf:application/pdf},
}

@article{andeol_spatial_2017,
	title = {The {Spatial} {Release} of {Cognitive} {Load} in {Cocktail} {Party} {Is} {Determined} by the {Relative} {Levels} of the {Talkers}},
	volume = {18},
	issn = {1525-3961, 1438-7573},
	url = {http://link.springer.com/10.1007/s10162-016-0611-7},
	doi = {10.1007/s10162-016-0611-7},
	abstract = {In a multi-talker situation, spatial separation between talkers reduces cognitive processing load: this is the Bspatial release of cognitive load{\textasciicircum}. The present study investigated the role played by the relative levels of the talkers on this spatial release of cognitive load. During the experiment, participants had to report the speech emitted by a target talker in the presence of a concurrent masker talker. The spatial separation (0° and 120° angular distance in azimuth) and the relative levels of the talkers (adverse, intermediate, and favorable target-tomasker ratio) were manipulated. The cognitive load was assessed with a prefrontal functional nearinfrared spectroscopy. Data from 14 young normalhearing listeners revealed that the target-to-masker ratio had a direct impact on the spatial release of cognitive load. Spatial separation significantly reduced the prefrontal activity only for the intermediate target-to-masker ratio and had no effect on prefrontal activity for the favorable and the adverse target-to-masker ratios. Therefore, the relative levels of the talkers might be a key point to determine the spatial release of cognitive load and more specifically the prefrontal activity induced by spatial cues in multi-talker situations.},
	language = {en},
	number = {3},
	urldate = {2021-11-10},
	journal = {Journal of the Association for Research in Otolaryngology},
	author = {Andéol, Guillaume and Suied, Clara and Scannella, Sébastien and Dehais, Frédéric},
	month = jun,
	year = {2017},
	pages = {457--464},
	file = {Andéol et al. - 2017 - The Spatial Release of Cognitive Load in Cocktail .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NPM27MMM\\Andéol et al. - 2017 - The Spatial Release of Cognitive Load in Cocktail .pdf:application/pdf},
}

@article{gardner_historical_1968,
	title = {Historical {Background} of the {Haas} and/or {Precedence} {Effect}},
	volume = {43},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1910974},
	doi = {10.1121/1.1910974},
	language = {en},
	number = {6},
	urldate = {2021-11-26},
	journal = {The Journal of the Acoustical Society of America},
	author = {Gardner, Mark B.},
	month = jun,
	year = {1968},
	pages = {1243--1248},
	file = {Gardner - 1968 - Historical Background of the Haas andor Precedenc.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HT59429Q\\Gardner - 1968 - Historical Background of the Haas andor Precedenc.pdf:application/pdf},
}

@book{ehrgott_multicriteria_2005,
	address = {Berlin Heidelberg},
	edition = {2. ed},
	title = {Multicriteria optimization},
	isbn = {978-3-540-21398-7},
	shorttitle = {Multicriteria optimization},
	language = {en},
	publisher = {Springer},
	author = {Ehrgott, Matthias},
	year = {2005},
	file = {Ehrgott - 2005 - Multicriteria optimization 12 Tables.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\UHT34E4J\\Ehrgott - 2005 - Multicriteria optimization 12 Tables.pdf:application/pdf},
}

@article{llave_joint_2021,
	title = {Joint denoising and dynamic range compression in binaural hearing aids},
	journal = {Soumis à The Journal of the Acoustical Society of America},
	author = {Llave, Adrien and Leglaive, Simon},
	year = {2021},
	pages = {12},
}

@phdthesis{carlo_echo-aware_2020,
	address = {Rennes},
	title = {Echo-aware signal processing for audio scene analysis},
	url = {https://tel.archives-ouvertes.fr/tel-03133271v2},
	language = {en},
	school = {Université Rennes 1},
	author = {Carlo, Diego Di},
	month = dec,
	year = {2020},
	file = {Carlo - Echo-aware signal processing for audio scene analy.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BPST89GZ\\Carlo - Echo-aware signal processing for audio scene analy.pdf:application/pdf},
}

@phdthesis{guezenoc_binaural_2021,
	address = {Rennes},
	title = {Binaural {Synthesis} {Individualization} based on {Listener} {Perceptual} {Feedback}},
	language = {fr},
	school = {CentraleSupélec},
	author = {Guezenoc, Corentin},
	month = jun,
	year = {2021},
	file = {Guezenoc - Binaural Synthesis Individualization based on List.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\W3ZBF9X2\\Guezenoc - Binaural Synthesis Individualization based on List.pdf:application/pdf},
}

@inproceedings{zhang_low-delay_2021,
	title = {Low-{Delay} {Speech} {Enhancement} {Using} {Perceptually} {Motivated} {Target} and {Loss}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/zhang21t_interspeech.html},
	doi = {10.21437/Interspeech.2021-1410},
	abstract = {Speech enhancement approaches based on deep neural network have outperformed the traditional signal processing methods. This paper presents a low-delay speech enhancement method that employs a new perceptually motivated training target and loss function. The proposed approach can achieve similar speech enhancement performance compared to the state-of-theart approaches, but with significantly less latency and computational complexities. Judged by the MOS tests conducted by the INTERSPEECH 2021 Deep Noise Suppression Challenge organizer, the proposed method is ranked the 2nd place for Background Noise MOS, and the 6th place for overall MOS.},
	language = {en},
	urldate = {2021-12-01},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Zhang, Xu and Ren, Xinlei and Zheng, Xiguang and Chen, Lianwu and Zhang, Chen and Guo, Liang and Yu, Bing},
	month = aug,
	year = {2021},
	pages = {2826--2830},
	file = {Zhang et al. - 2021 - Low-Delay Speech Enhancement Using Perceptually Mo.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2NVBIIHP\\Zhang et al. - 2021 - Low-Delay Speech Enhancement Using Perceptually Mo.pdf:application/pdf},
}

@inproceedings{saddler_speech_2021,
	title = {Speech {Denoising} with {Auditory} {Models}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/saddler21_interspeech.html},
	doi = {10.21437/Interspeech.2021-1973},
	abstract = {Contemporary speech enhancement predominantly relies on audio transforms that are trained to reconstruct a clean speech waveform. The development of high-performing neural network sound recognition systems has raised the possibility of using deep feature representations as ‘perceptual’ losses with which to train denoising systems. We explored their utility by ﬁrst training deep neural networks to classify either spoken words or environmental sounds from audio. We then trained an audio transform to map noisy speech to an audio waveform that minimized the difference in the deep feature representations between the output audio and the corresponding clean audio. The resulting transforms removed noise substantially better than baseline methods trained to reconstruct clean waveforms, and also outperformed previous methods using deep feature losses. However, a similar beneﬁt was obtained simply by using losses derived from the ﬁlter bank inputs to the deep networks. The results show that deep features can guide speech enhancement, but suggest that they do not yet outperform simple alternatives that do not involve learned features.},
	language = {en},
	urldate = {2021-12-01},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Saddler, Mark R. and Francl, Andrew and Feather, Jenelle and Qian, Kaizhi and Zhang, Yang and McDermott, Josh H.},
	month = aug,
	year = {2021},
	pages = {2681--2685},
	file = {Saddler et al. - 2021 - Speech Denoising with Auditory Models.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HDHZJFD5\\Saddler et al. - 2021 - Speech Denoising with Auditory Models.pdf:application/pdf},
}

@inproceedings{han_binaural_2021,
	title = {Binaural {Speech} {Separation} of {Moving} {Speakers} {With} {Preserved} {Spatial} {Cues}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/han21e_interspeech.html},
	doi = {10.21437/Interspeech.2021-1372},
	abstract = {Binaural speech separation algorithms designed for augmented hearing technologies need to both improve the signal-to-noise ratio of individual speakers and preserve their perceived location in space. The majority of binaural speech separation methods assume nonmoving speakers. As a result, their application to real-world scenarios with freely moving speakers requires block-wise adaptation which relies on short-term contextual information and limits their performance. In this study, we propose an alternative approach for utterance-level source separation with moving speakers and in reverberant conditions. Our model makes use of spectral and spatial features of speakers in a larger context compared to the block-wise adaption methods. The model can implicitly track speakers within the utterance without the need for explicit tracking modules. Experimental results on simulated moving multitalker speech show that the proposed method can signiﬁcantly outperform block-wise adaptation methods in both separation performance and preserving the interaural cues across multiple conditions, which makes it suitable for real-world augmented hearing applications.},
	language = {en},
	urldate = {2021-12-01},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Han, Cong and Luo, Yi and Mesgarani, Nima},
	month = aug,
	year = {2021},
	pages = {3505--3509},
	file = {Han et al. - 2021 - Binaural Speech Separation of Moving Speakers With.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\W9W9KWRE\\Han et al. - 2021 - Binaural Speech Separation of Moving Speakers With.pdf:application/pdf},
}

@inproceedings{fontaine_alpha-stable_2021,
	title = {Alpha-{Stable} {Autoregressive} {Fast} {Multichannel} {Nonnegative} {Matrix} {Factorization} for {Joint} {Speech} {Enhancement} and {Dereverberation}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/fontaine21_interspeech.html},
	doi = {10.21437/Interspeech.2021-742},
	language = {en},
	urldate = {2021-12-01},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Fontaine, Mathieu and Sekiguchi, Kouhei and Nugraha, Aditya Arie and Bando, Yoshiaki and Yoshii, Kazuyoshi},
	month = aug,
	year = {2021},
	pages = {661--665},
	file = {Fontaine et al. - 2021 - Alpha-Stable Autoregressive Fast Multichannel Nonn.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7AQ7CZFV\\Fontaine et al. - 2021 - Alpha-Stable Autoregressive Fast Multichannel Nonn.pdf:application/pdf},
}

@article{ma_echofilter_2021,
	title = {{EchoFilter}: {End}-to-{End} {Neural} {Network} for {Acoustic} {Echo} {Cancellation}},
	shorttitle = {{EchoFilter}},
	url = {http://arxiv.org/abs/2105.14666},
	abstract = {Acoustic Echo Cancellation (AEC) whose aim is to suppress the echo originated from acoustic coupling between loudspeakers and microphones, plays a key role in voice interaction. Linear adaptive ﬁlter (AF) is always used for handling this problem. However, since there would be some severe effects in real scenarios, such nonlinear distortions, background noises, and microphone clipping, it would lead to considerable residual echo, giving poor performance in practice. In this paper, we propose an end-to-end network structure for echo cancellation, which is directly done on time-domain audio waveform. It is transformed to deep representation by temporal convolution, and modelled by Long Short-Term Memory (LSTM) for considering temporal property. Since time delay and severe reverberation may exist at the near-end with respect to the far-end, a local attention is employed for alignment. The network is trained using multitask learning by employing an auxiliary classiﬁcation network for double-talk detection. Experiments show the superiority of our proposed method in terms of the echo return loss enhancement (ERLE) for single-talk periods and the perceptual evaluation of speech quality (PESQ) score for double-talk periods in background noise and nonlinear distortion scenarios.},
	language = {en},
	urldate = {2021-12-07},
	journal = {arXiv:2105.14666 [cs, eess]},
	author = {Ma, Lu and Yang, Song and Gong, Yaguang and Wang, Xintian and Wu, Zhongqin},
	month = may,
	year = {2021},
	note = {arXiv: 2105.14666},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Ma et al. - 2021 - EchoFilter End-to-End Neural Network for Acoustic.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\T6HR6EKQ\\Ma et al. - 2021 - EchoFilter End-to-End Neural Network for Acoustic.pdf:application/pdf},
}

@article{segawa_extension_2021,
	title = {Extension of virtual microphone technique to multiple real microphones and investigation of the impact of phase and amplitude interpolation on speech enhancement},
	abstract = {A virtual microphone signal can artiﬁcially increase the number of channels of an observed signal, which leads to improved multi-channel speech enhancement performance. The conventional virtual microphone technique can only be applied to two real microphones, reducing its application to a limited range of situations. In this paper, we extend the virtual microphone technique to the interpolation of more than two real microphones. In the proposed method, the phase based on the plane wave model is linearly interpolated using an afﬁne coefﬁcients of the coordinates, and the amplitude is nonlinearly interpolated based on the β-divergence using coefﬁcients inversely proportional to the distance between the virtual and real microphones. The experimental results shows that the proposed method is effective in improving speech enhancement performance. Furthermore, we investigate the impact of phase and amplitude interpolation on speech enhancement performance with two or three real microphones. The experimental results indicated that the beneﬁts of phase interpolation based on plane wave model were limited, and that amplitude interpolation was important for improving speech enhancement performance.},
	language = {en},
	author = {Segawa, Hanako and Li, Li and Makino, Shoji and Yamada, Takeshi},
	year = {2021},
	pages = {6},
	file = {Segawa et al. - 2021 - Extension of virtual microphone technique to multi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KBDBKNGP\\Segawa et al. - 2021 - Extension of virtual microphone technique to multi.pdf:application/pdf},
}

@article{serizel_low-rank_2014,
	title = {Low-rank {Approximation} {Based} {Multichannel} {Wiener} {Filter} {Algorithms} for {Noise} {Reduction} with {Application} in {Cochlear} {Implants}},
	volume = {22},
	issn = {2329-9290, 2329-9304},
	url = {http://ieeexplore.ieee.org/document/6730918/},
	doi = {10.1109/TASLP.2014.2304240},
	abstract = {This paper presents low-rank approximation based multichannel Wiener ﬁlter algorithms for noise reduction in speech plus noise scenarios, with application in cochlear implants. In a single speech source scenario, the frequency-domain autocorrelation matrix of the speech signal is often assumed to be a rank-1 matrix, which then allows to derive different rank-1 approximation based noise reduction ﬁlters. In practice, however, the rank of the autocorrelation matrix of the speech signal is usually greater than one. Firstly, the link between the different rank-1 approximation based noise reduction ﬁlters and the original speech distortion weighted multichannel Wiener ﬁlter is investigated when the rank of the autocorrelation matrix of the speech signal is indeed greater than one. Secondly, in low input signal-to-noise-ratio scenarios, due to noise non-stationarity, the estimation of the autocorrelation matrix of the speech signal can be problematic and the noise reduction ﬁlters can deliver unpredictable noise reduction performance. An eigenvalue decomposition based ﬁlter and a generalized eigenvalue decomposition based ﬁlter are introduced that include a more robust rank-1, or more generally rank-R, approximation of the autocorrelation matrix of the speech signal. These noise reduction ﬁlters are demonstrated to deliver a better noise reduction performance especially in low input signal-to-noise-ratio scenarios. The ﬁlters are especially useful in cochlear implants, where more speech distortion and hence a more aggressive noise reduction can be tolerated.},
	language = {en},
	number = {4},
	urldate = {2022-01-04},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Serizel, Romain and Moonen, Marc and Van Dijk, Bas and Wouters, Jan},
	month = apr,
	year = {2014},
	pages = {785--799},
	file = {Serizel et al. - 2014 - Low-rank Approximation Based Multichannel Wiener F.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\P3K5GZWC\\Serizel et al. - 2014 - Low-rank Approximation Based Multichannel Wiener F.pdf:application/pdf},
}

@article{talmon_relative_2009,
	title = {Relative {Transfer} {Function} {Identification} {Using} {Convolutive} {Transfer} {Function} {Approximation}},
	volume = {17},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/4802172/},
	doi = {10.1109/TASL.2008.2009576},
	abstract = {In this paper, we present a relative transfer function (RTF) identiﬁcation method for speech sources in reverberant environments. The proposed method is based on the convolutive transfer function (CTF) approximation, which enables to represent a linear convolution in the time domain as a linear convolution in the short-time Fourier transform (STFT) domain. Unlike the restrictive and commonly used multiplicative transfer function (MTF) approximation, which becomes more accurate when the length of a time frame increases relative to the length of the impulse response, the CTF approximation enables representation of long impulse responses using short time frames. We develop an unbiased RTF estimator that exploits the nonstationarity and presence probability of the speech signal and derive an analytic expression for the estimator variance. Experimental results show that the proposed method is advantageous compared to common RTF identiﬁcation methods in various acoustic environments, especially when identifying long RTFs typical to real rooms.},
	language = {en},
	number = {4},
	urldate = {2022-01-04},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Talmon, R. and Cohen, I. and Gannot, S.},
	month = may,
	year = {2009},
	pages = {546--555},
	file = {Talmon et al. - 2009 - Relative Transfer Function Identification Using Co.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BRAV7VPE\\Talmon et al. - 2009 - Relative Transfer Function Identification Using Co.pdf:application/pdf},
}

@article{cohen_relative_2004,
	title = {Relative {Transfer} {Function} {Identification} {Using} {Speech} {Signals}},
	volume = {12},
	issn = {1063-6676},
	url = {http://ieeexplore.ieee.org/document/1323081/},
	doi = {10.1109/TSA.2004.832975},
	abstract = {An important component of a multichannel hands-free communication system is the identiﬁcation of the relative transfer function between sensors in response to a desired source signal. In this paper, a robust system identiﬁcation approach adapted to speech signals is proposed. A weighted least-squares optimization criterion is introduced, which considers the uncertainty of the desired signal presence in the observed signals. An asymptotically unbiased estimate for the system’s transfer function is derived, and a corresponding recursive online implementation is presented. We show that compared to a competing nonstationarity-based method, a smaller error variance is achieved and generally shorter observation intervals are required. Furthermore, in the case of a time-varying system, faster convergence and higher reliability of the system identiﬁcation are obtained by using the proposed method than by using the nonstationarity-based method. Evaluation of the proposed system identiﬁcation approach is performed under various noise conditions, including simulated stationary and nonstationary white Gaussian noise, and car interior noise in real pseudo-stationary and nonstationary environments. The experimental results conﬁrm the advantages of proposed approach.},
	language = {en},
	number = {5},
	urldate = {2022-01-04},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Cohen, I.},
	month = sep,
	year = {2004},
	pages = {451--459},
	file = {Cohen - 2004 - Relative Transfer Function Identification Using Sp.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\74MHSRA3\\Cohen - 2004 - Relative Transfer Function Identification Using Sp.pdf:application/pdf},
}

@article{markovich_multichannel_2009,
	title = {Multichannel {Eigenspace} {Beamforming} in a {Reverberant} {Noisy} {Environment} {With} {Multiple} {Interfering} {Speech} {Signals}},
	volume = {17},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/5109760/},
	doi = {10.1109/TASL.2009.2016395},
	abstract = {In many practical environments we wish to extract several desired speech signals, which are contaminated by nonstationary and stationary interfering signals. The desired signals may also be subject to distortion imposed by the acoustic room impulse responses (RIRs). In this paper, a linearly constrained minimum variance (LCMV) beamformer is designed for extracting the desired signals from multimicrophone measurements. The beamformer satisﬁes two sets of linear constraints. One set is dedicated to maintaining the desired signals, while the other set is chosen to mitigate both the stationary and nonstationary interferences. Unlike classical beamformers, which approximate the RIRs as delay-only ﬁlters, we take into account the entire RIR [or its respective acoustic transfer function (ATF)]. The LCMV beamformer is then reformulated in a generalized sidelobe canceler (GSC) structure, consisting of a ﬁxed beamformer (FBF), blocking matrix (BM), and adaptive noise canceler (ANC). It is shown that for spatially white noise ﬁeld, the beamformer reduces to a FBF, satisfying the constraint sets, without power minimization. It is shown that the application of the adaptive ANC contributes to interference reduction, but only when the constraint sets are not completely satisﬁed. We show that relative transfer functions (RTFs), which relate the desired speech sources and the microphones, and a basis for the interference subspace sufﬁce for constructing the beamformer. The RTFs are estimated by applying the generalized eigenvalue decomposition (GEVD) procedure to the power spectral density (PSD) matrices of the received signals and the stationary noise. A basis for the interference subspace is estimated by collecting eigenvectors, calculated in segments where nonstationary interfering sources are active and the desired sources are inactive. The rank of the basis is then reduced by the application of the orthogonal triangular decomposition (QRD). This procedure relaxes the common requirement for nonoverlapping activity periods of the interference sources. A comprehensive experimental study in both simulated and real environments demonstrates the performance of the proposed beamformer.},
	language = {en},
	number = {6},
	urldate = {2022-01-04},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Markovich, Shmulik and Gannot, Sharon and Cohen, Israel},
	month = aug,
	year = {2009},
	pages = {1071--1086},
	file = {Markovich et al. - 2009 - Multichannel Eigenspace Beamforming in a Reverbera.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\M7YUUDM8\\Markovich et al. - 2009 - Multichannel Eigenspace Beamforming in a Reverbera.pdf:application/pdf},
}

@inproceedings{dolne_model-based_2021,
	title = {Model-based {Beamforming} for {Wearable} {Microphone} {Arr}},
	booktitle = {{EUSIPCO}},
	author = {d'Olne, Emilie and Moore, Alastair H. and Naylor, Patrick A.},
	year = {2021},
	file = {olne2021.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\KYNBE4BM\\olne2021.pdf:application/pdf},
}

@inproceedings{moore_binaural_2021,
	title = {A binaural {MVDR} beamformer for the 2021 {Clarity} {Enhancement} {Challenge}: {ELO}-{SPHERES} consortium system description},
	author = {Moore, Alastair H and Hafezi, Sina and Vos, Rebecca and Brookes, Mike and Naylor, Patrick A. and Huckvale, Mark and Rosen, Stuart and Green, Tim and Hilkhuysen, Gaston},
	year = {2021},
	file = {moore2021.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\W8NQZZ6G\\moore2021.pdf:application/pdf},
}

@inproceedings{sathyapriyan_sathyapriyan2021pdf_2021,
	title = {Sathyapriyan2021.pdf},
	booktitle = {{EUSIPCO}},
	author = {Sathyapriyan, Vasudha and C¸alıs, Meltin and Hendriks, Richard C.},
	year = {2021},
	file = {Sathyapriyan2021.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LXUXLEL4\\Sathyapriyan2021.pdf:application/pdf},
}

@phdthesis{le_bagousse_elaboration_2014,
	title = {Élaboration d'une méthode de test pour l'évaluation subjective de la qualité des sons spatialisés},
	school = {Université de Bretagne Occidentale},
	author = {Le Bagousse, Sarah},
	month = apr,
	year = {2014},
	file = {10.1.1.716.3412.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5RKNVU3C\\10.1.1.716.3412.pdf:application/pdf},
}

@article{corey_modeling_2021,
	title = {Modeling the effects of dynamic range compression on signals in noise},
	volume = {150},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/10.1121/10.0005314},
	doi = {10.1121/10.0005314},
	abstract = {Hearing aids use dynamic range compression (DRC), a form of automatic gain control, to make quiet sounds louder and loud sounds quieter. Compression can improve listening comfort, but it can also cause unwanted distortion in noisy environments. It has been widely reported that DRC performs poorly in noise, but there has been little mathematical analysis of these noise-induced distortion effects. This work introduces a mathematical model to study the behavior of DRC in noise. By making simplifying assumptions about the signal envelopes, we deﬁne an effective compression function that models the compression applied to one signal in the presence of another. Using the properties of concave functions, we prove results about DRC that have been previously observed experimentally: that the effective compression applied to each sound in a mixture is weaker than it would have been for the signal alone; that uncorrelated signal envelopes become negatively correlated when compressed as a mixture; and that compression can reduce the long-term signal-to-noise ratio in certain conditions. These theoretical results are supported by software experiments using recorded speech signals. VC 2021 Acoustical Society of America.},
	language = {en},
	number = {1},
	urldate = {2022-01-31},
	journal = {The Journal of the Acoustical Society of America},
	author = {Corey, Ryan M. and Singer, Andrew C.},
	month = jul,
	year = {2021},
	pages = {159--170},
	file = {Corey et Singer - 2021 - Modeling the effects of dynamic range compression .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\DY8SATJN\\Corey et Singer - 2021 - Modeling the effects of dynamic range compression .pdf:application/pdf},
}

@inproceedings{carbajal_multiple-input_2018,
	address = {Calgary, AB},
	title = {Multiple-{Input} {Neural} {Network}-{Based} {Residual} {Echo} {Suppression}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8461476/},
	doi = {10.1109/ICASSP.2018.8461476},
	abstract = {A residual echo suppressor (RES) aims to suppress the residual echo in the output of an acoustic echo canceler (AEC). Spectral-based RES approaches typically estimate the magnitude spectra of the near-end speech and the residual echo from a single input, that is either the far-end speech or the echo computed by the AEC, and derive the RES ﬁlter coefﬁcients accordingly. These single inputs do not always sufﬁce to discriminate the near-end speech from the remaining echo. In this paper, we propose a neural network-based approach that directly estimates the RES ﬁlter coefﬁcients from multiple inputs, including the AEC output, the far-end speech, and/or the echo computed by the AEC. We evaluate our system on real recordings of acoustic echo and near-end speech acquired in various situations with a smart speaker. We compare it to two single-input spectral-based approaches in terms of echo reduction and near-end speech distortion.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Carbajal, Guillaume and Serizel, Romain and Vincent, Emmanuel and Humbert, Eric},
	month = apr,
	year = {2018},
	pages = {231--235},
	file = {Carbajal et al. - 2018 - Multiple-Input Neural Network-Based Residual Echo .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\2YI4LGDR\\Carbajal et al. - 2018 - Multiple-Input Neural Network-Based Residual Echo .pdf:application/pdf},
}

@inproceedings{valin_low-complexity_2021,
	address = {Toronto, ON, Canada},
	title = {Low-{Complexity}, {Real}-{Time} {Joint} {Neural} {Echo} {Control} and {Speech} {Enhancement} {Based} {On} {PercepNet}},
	url = {http://arxiv.org/abs/2102.05245},
	abstract = {Speech enhancement algorithms based on deep learning have greatly surpassed their traditional counterparts and are now being considered for the task of removing acoustic echo from hands-free communication systems. This is a challenging problem due to both real-world constraints like loudspeaker non-linearities, and to limited compute capabilities in some communication systems. In this work, we propose a system combining a traditional acoustic echo canceller, and a low-complexity joint residual echo and noise suppressor based on a hybrid signal processing/deep neural network (DSP/DNN) approach. We show that the proposed system outperforms both traditional and other neural approaches, while requiring only 5.5\% CPU for real-time operation. We further show that the system can scale to even lower complexity levels.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Valin, Jean-Marc and Tenneti, Srikanth and Helwani, Karim and Isik, Umut and Krishnaswamy, Arvindh},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.05245},
	keywords = {AEC Challenge ICASSP, perceptnet},
	file = {Valin et al. - 2021 - Low-Complexity, Real-Time Joint Neural Echo Contro.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QLNW9E87\\Valin et al. - 2021 - Low-Complexity, Real-Time Joint Neural Echo Contro.pdf:application/pdf},
}

@inproceedings{westhausen_acoustic_2021,
	address = {Toronto, ON, Canada},
	title = {Acoustic echo cancellation with the dual-signal transformation {LSTM} network},
	url = {http://arxiv.org/abs/2010.14337},
	abstract = {This paper applies the dual-signal transformation LSTM network (DTLN) to the task of real-time acoustic echo cancellation (AEC). The DTLN combines a short-time Fourier transformation and a learned feature representation in a stacked network approach, which enables robust information processing in the time-frequency and in the time domain, which also includes phase information. The model is only trained on 60 h of real and synthetic echo scenarios. The training setup includes multi-lingual speech, data augmentation, additional noise and reverberation to create a model that should generalize well to a large variety of real-world conditions. The DTLN approach produces state-of-the-art performance on clean and noisy echo conditions reducing acoustic echo and additional noise robustly. The method outperforms the AEC-Challenge baseline by 0.30 in terms of Mean Opinion Score (MOS).},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Westhausen, Nils L. and Meyer, Bernd T.},
	year = {2021},
	note = {arXiv: 2010.14337},
	keywords = {AEC Challenge ICASSP},
	file = {Westhausen et Meyer - 2020 - Acoustic echo cancellation with the dual-signal tr.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PN7XRVY3\\Westhausen et Meyer - 2020 - Acoustic echo cancellation with the dual-signal tr.pdf:application/pdf},
}

@inproceedings{valin_perceptually-motivated_2020,
	title = {A {Perceptually}-{Motivated} {Approach} for {Low}-{Complexity}, {Real}-{Time} {Enhancement} of {Fullband} {Speech}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/valin20_interspeech.html},
	doi = {10.21437/Interspeech.2020-2730},
	abstract = {Over the past few years, speech enhancement methods based on deep learning have greatly surpassed traditional methods based on spectral subtraction and spectral estimation. Many of these new techniques operate directly in the the short-time Fourier transform (STFT) domain, resulting in a high computational complexity. In this work, we propose PercepNet, an efﬁcient approach that relies on human perception of speech by focusing on the spectral envelope and on the periodicity of the speech. We demonstrate high-quality, real-time enhancement of fullband (48 kHz) speech with less than 5\% of a CPU core.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Valin, Jean-Marc and Isik, Umut and Phansalkar, Neerad and Giri, Ritwik and Helwani, Karim and Krishnaswamy, Arvindh},
	month = oct,
	year = {2020},
	keywords = {perceptnet},
	pages = {2482--2486},
	file = {Valin et al. - 2020 - A Perceptually-Motivated Approach for Low-Complexi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\NGDT8LUL\\Valin et al. - 2020 - A Perceptually-Motivated Approach for Low-Complexi.pdf:application/pdf},
}

@inproceedings{wang_weighted_2021,
	address = {Toronto, ON, Canada},
	title = {Weighted {Recursive} {Least} {Square} {Filter} and {Neural} {Network} based {Residual} {Echo} {Suppression} for the {AEC}-{Challenge}},
	url = {http://arxiv.org/abs/2102.08551},
	abstract = {This paper presents a real-time Acoustic Echo Cancellation (AEC) algorithm submitted to the AEC-Challenge. The algorithm consists of three modules: Generalized CrossCorrelation with PHAse Transform (GCC-PHAT) based time delay compensation, weighted Recursive Least Square (wRLS) based linear adaptive ﬁltering and neural network based residual echo suppression. The wRLS ﬁlter is derived from a novel semi-blind source separation perspective. The neural network model predicts a Phase-Sensitive Mask (PSM) based on the aligned reference and the linear ﬁlter output. The algorithm achieved a mean subjective score of 4.00 and ranked 2nd in the AEC-Challenge.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Wang, Ziteng and Na, Yueyue and Liu, Zhang and Tian, Biao and Fu, Qiang},
	year = {2021},
	note = {arXiv: 2102.08551},
	keywords = {AEC Challenge ICASSP},
	file = {Wang et al. - 2021 - Weighted Recursive Least Square Filter and Neural .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\GI4D9MNA\\Wang et al. - 2021 - Weighted Recursive Least Square Filter and Neural .pdf:application/pdf},
}

@inproceedings{halimeh_combining_2021,
	address = {Toronto, ON, Canada},
	title = {Combining {Adaptive} {Filtering} {And} {Complex}-{Valued} {Deep} {Postfiltering} {For} {Acoustic} {Echo} {Cancellation}},
	isbn = {978-1-72817-605-5},
	url = {https://ieeexplore.ieee.org/document/9414868/},
	doi = {10.1109/ICASSP39728.2021.9414868},
	abstract = {In this contribution, we introduce a novel approach to noise-robust acoustic echo cancellation employing a complex-valued Deep Neural Network (DNN) for postﬁltering. In a ﬁrst step, early linear echo components are removed using a double-talk robust adaptive ﬁlter. The residual signal is subsequently processed by the proposed postﬁlter (PF). Due to its complex-valued nature, the PF allows to suppress unwanted signal components without introducing distortions to the near-end speaker. For training and evaluation, we exclusively use data from the ICASSP 2021 AEC challenge. Exploiting only a moderate amount of training data, we demonstrate the efﬁcacy of the proposed method. Speciﬁcally, we show that the PF (i) beneﬁts signiﬁcantly from a preceding linear adaptive ﬁlter and (ii) signiﬁcantly outperforms a conventional real-valued DNN-based PF.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Halimeh, Mhd Modar and Haubner, Thomas and Briegleb, Annika and Schmidt, Alexander and Kellermann, Walter},
	month = jun,
	year = {2021},
	pages = {121--125},
	file = {Halimeh et al. - 2021 - Combining Adaptive Filtering And Complex-Valued De.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TEN5BVCE\\Halimeh et al. - 2021 - Combining Adaptive Filtering And Complex-Valued De.pdf:application/pdf},
}

@inproceedings{ivry_deep_2021,
	address = {Toronto, ON, Canada},
	title = {Deep {Residual} {Echo} {Suppression} with {A} {Tunable} {Tradeoff} {Between} {Signal} {Distortion} and {Echo} {Suppression}},
	url = {http://arxiv.org/abs/2106.13531},
	doi = {10.1109/ICASSP39728.2021.9414958},
	abstract = {In this paper, we propose a residual echo suppression method using a UNet neural network that directly maps the outputs of a linear acoustic echo canceler to the desired signal in the spectral domain. This system embeds a design parameter that allows a tunable tradeoff between the desired-signal distortion and residual echo suppression in double-talk scenarios. The system employs 136 thousand parameters, and requires 1.6 Giga ﬂoating-point operations per second and 10 Mega-bytes of memory. The implementation satisﬁes both the timing requirements of the AEC challenge and the computational and memory limitations of on-device applications. Experiments are conducted with 161 h of data from the AEC challenge database and from real independent recordings. We demonstrate the performance of the proposed system in real-life conditions and compare it with two competing methods regarding echo suppression and desired-signal distortion, generalization to various environments, and robustness to high echo levels.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Ivry, Amir and Cohen, Israel and Berdugo, Baruch},
	year = {2021},
	note = {arXiv: 2106.13531},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	pages = {126--130},
	file = {Ivry et al. - 2021 - Deep Residual Echo Suppression with A Tunable Trad.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CME4NTSG\\Ivry et al. - 2021 - Deep Residual Echo Suppression with A Tunable Trad.pdf:application/pdf},
}

@inproceedings{sridhar_icassp_2020,
	title = {{ICASSP} 2021 {Acoustic} {Echo} {Cancellation} {Challenge}: {Datasets}, {Testing} {Framework}, and {Results}},
	shorttitle = {{ICASSP} 2021 {Acoustic} {Echo} {Cancellation} {Challenge}},
	url = {http://arxiv.org/abs/2009.04972},
	abstract = {The ICASSP 2021 Acoustic Echo Cancellation Challenge is intended to stimulate research in the area of acoustic echo cancellation (AEC), which is an important part of speech enhancement and still a top issue in audio communication and conferencing systems. Many recent AEC studies report good performance on synthetic datasets where the train and test samples come from the same underlying distribution. However, the AEC performance often degrades significantly on real recordings. Also, most of the conventional objective metrics such as echo return loss enhancement (ERLE) and perceptual evaluation of speech quality (PESQ) do not correlate well with subjective speech quality tests in the presence of background noise and reverberation found in realistic environments. In this challenge, we open source two large datasets to train AEC models under both single talk and double talk scenarios. These datasets consist of recordings from more than 2,500 real audio devices and human speakers in real environments, as well as a synthetic dataset. We open source two large test sets, and we open source an online subjective test framework for researchers to quickly test their results. The winners of this challenge will be selected based on the average Mean Opinion Score (MOS) achieved across all different single talk and double talk scenarios.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {{arXiv}:2009.04972 [cs, eess]},
	author = {Sridhar, Kusha and Cutler, Ross and Saabas, Ando and Parnamaa, Tanel and Loide, Markus and Gamper, Hannes and Braun, Sebastian and Aichner, Robert and Srinivasan, Sriram},
	month = oct,
	year = {2020},
	note = {arXiv: 2009.04972},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Sridhar et al. - 2020 - ICASSP 2021 Acoustic Echo Cancellation Challenge .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\G4CFDT4I\\Sridhar et al. - 2020 - ICASSP 2021 Acoustic Echo Cancellation Challenge .pdf:application/pdf},
}

@article{cutler_icassp_2022,
	title = {{ICASSP} 2022 {Acoustic} {Echo} {Cancellation} {Challenge}},
	url = {http://arxiv.org/abs/2202.13290},
	abstract = {The ICASSP 2022 Acoustic Echo Cancellation Challenge is intended to stimulate research in acoustic echo cancellation (AEC), which is an important area of speech enhancement and still a top issue in audio communication. This is the third AEC challenge and it is enhanced by including mobile scenarios, adding speech recognition word accuracy rate as a metric, and making the audio 48 kHz. We open source two large datasets to train AEC models under both single talk and double talk scenarios. These datasets consist of recordings from more than 10,000 real audio devices and human speakers in real environments, as well as a synthetic dataset. We open source an online subjective test framework and provide an online objective metric service for researchers to quickly test their results. The winners of this challenge were selected based on the average Mean Opinion Score (MOS) achieved across all scenarios and the word accuracy rate.},
	language = {en},
	urldate = {2022-04-04},
	journal = {arXiv:2202.13290 [cs, eess]},
	author = {Cutler, Ross and Saabas, Ando and Parnamaa, Tanel and Purin, Marju and Gamper, Hannes and Braun, Sebastian and Sørensen, Karsten and Aichner, Robert},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.13290},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Cutler et al. - 2022 - ICASSP 2022 Acoustic Echo Cancellation Challenge.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\R9JZQZ6M\\Cutler et al. - 2022 - ICASSP 2022 Acoustic Echo Cancellation Challenge.pdf:application/pdf},
}

@inproceedings{halimeh_efficient_2020,
	address = {Barcelona, Spain},
	title = {Efficient {Multichannel} {Nonlinear} {Acoustic} {Echo} {Cancellation} {Based} on a {Cooperative} {Strategy}},
	isbn = {978-1-5090-6631-5},
	url = {https://ieeexplore.ieee.org/document/9054541/},
	doi = {10.1109/ICASSP40776.2020.9054541},
	abstract = {While a common approach to address nonlinear distortions, emitted by multiple loudspeakers and observed by multiple microphones, is to use post-ﬁltering techniques, this paper proposes a cooperative strategy to rather model and then cancel such distortions. In this approach, the overall problem of modeling distortions emitted by a number of loudspeakers is divided into multiple simpler and easier tasks of estimating distortions emitted by subsets of loudspeakers. This approach allows also the exploitation of the physical conﬁguration of the loudspeakers and microphones to select certain microphone signals for estimating the nonlinearity of loudspeakers that contribute the predominant part of the acoustic echo to this microphone signal. The proposed strategy is realized using the elitist resampling particle ﬁlter and the Gaussian particle ﬁlter. Both variants are evaluated and compared to a linear approach using synthesized and real recordings.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Halimeh, Mhd Modar and Kellermann, Walter},
	month = may,
	year = {2020},
	pages = {461--465},
	file = {Halimeh et Kellermann - 2020 - Efficient Multichannel Nonlinear Acoustic Echo Can.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TFJA5FG9\\Halimeh et Kellermann - 2020 - Efficient Multichannel Nonlinear Acoustic Echo Can.pdf:application/pdf},
}

@inproceedings{chen_nonlinear_2020,
	title = {Nonlinear {Residual} {Echo} {Suppression} {Based} on {Multi}-stream {Conv}-{TasNet}},
	url = {http://arxiv.org/abs/2005.07631},
	abstract = {Acoustic echo cannot be entirely removed by linear adaptive ﬁlters due to the nonlinear relationship between the echo and farend signal. Usually a post processing module is required to further suppress the echo. In this paper, we propose a residual echo suppression method based on the modiﬁcation of fully convolutional time-domain audio separation network (Conv-TasNet). Both the residual signal of the linear acoustic echo cancellation system, and the output of the adaptive ﬁlter are adopted to form multiple streams for the Conv-TasNet, resulting in more effective echo suppression while keeping a lower latency of the whole system. Simulation results validate the efﬁcacy of the proposed method in both single-talk and double-talk situations.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {{arXiv}:2005.07631 [cs, eess, stat]},
	author = {Chen, Hongsheng and Xiang, Teng and Chen, Kai and Lu, Jing},
	month = may,
	year = {2020},
	note = {arXiv: 2005.07631},
	keywords = {Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Chen et al. - 2020 - Nonlinear Residual Echo Suppression Based on Multi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\W7RSNIN3\\Chen et al. - 2020 - Nonlinear Residual Echo Suppression Based on Multi.pdf:application/pdf},
}

@inproceedings{huemmer_elitist_2014,
	address = {Florence, Italy},
	title = {The elitist particle filter based on evolutionary strategies as novel approach for nonlinear acoustic echo cancellation},
	isbn = {978-1-4799-2893-4},
	url = {http://ieeexplore.ieee.org/document/6853810/},
	doi = {10.1109/ICASSP.2014.6853810},
	abstract = {In this article, we introduce a novel approach for nonlinear acoustic echo cancellation based on a combination of particle ﬁltering and evolutionary strategies. The nonlinear echo path is modeled as a state vector with non-Gaussian probability distribution and the relation to the observed signals and near-end interferences are captured by nonlinear functions. To estimate the probability distribution of the state vector and the model parameters, we apply the numerical sampling method of particle ﬁltering, where each set of particles represents different realizations of the nonlinear echo path. While the classical particle-ﬁlter approach is unsuitable for system identiﬁcation with large search spaces, we introduce a modiﬁed particle ﬁlter to select elitist particles based on long-term ﬁtness measures and to create new particles based on the approximated probability distribution of the state vector. The validity of the novel approach is experimentally veriﬁed with real recordings for a nonlinear echo path stemming from a commercial smartphone.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {2014 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Huemmer, Christian and Hofmann, Christian and Maas, Roland and Schwarz, Andreas and Kellermann, Walter},
	month = may,
	year = {2014},
	pages = {1315--1319},
	file = {Huemmer et al. - 2014 - The elitist particle filter based on evolutionary .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\F5Z9BT83\\Huemmer et al. - 2014 - The elitist particle filter based on evolutionary .pdf:application/pdf},
}

@inproceedings{gaultier_double-talk_2021,
	address = {Dublin, Ireland},
	title = {Double-{Talk} {Robust} {Acoustic} {Echo} {Cancellation} {Using} {Partition} {Block} {Frequency}-{Domain} {Adaptive} {Filtering}},
	isbn = {978-90-827970-6-0},
	url = {https://ieeexplore.ieee.org/document/9616039/},
	doi = {10.23919/EUSIPCO54536.2021.9616039},
	abstract = {This work aims at introducing a new acoustic echo cancellation algorithm robust to double-talk situation. We propose a method which combines partition-block frequency domain adaptive ﬁltering (PBFDAF) and best linear unbiased estimation (BLUE) through local signal characteristics recursive estimation. We report that our method outperforms an Acoustic Echo Cancellation (AEC) baseline using BLUE for Echo Return Loss Enhancement (ERLE) and show its usefulness when used in an automated speech recognition pipeline. Improvements are obtained for situations featuring either continuous or discontinuous echo signals.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {2021 29th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	publisher = {IEEE},
	author = {Gaultier, Clement and Guerin, Alexandre and Pallone, Gregory and Emerit, Marc},
	month = aug,
	year = {2021},
	pages = {171--175},
	file = {Gaultier et al. - 2021 - Double-Talk Robust Acoustic Echo Cancellation Usin.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YRFGAIAV\\Gaultier et al. - 2021 - Double-Talk Robust Acoustic Echo Cancellation Usin.pdf:application/pdf},
}

@article{schroter_clcnet_2020,
	title = {{CLCNet}: {Deep} learning-based {Noise} {Reduction} for {Hearing} {Aids} using {Complex} {Linear} {Coding}},
	shorttitle = {{CLCNet}},
	url = {http://arxiv.org/abs/2001.10218},
	abstract = {Noise reduction is an important part of modern hearing aids and is included in most commercially available devices. Deep learning-based state-of-the-art algorithms, however, either do not consider real-time and frequency resolution constrains or result in poor quality under very noisy conditions.},
	language = {en},
	urldate = {2022-04-04},
	journal = {arXiv:2001.10218 [cs, eess, stat]},
	author = {Schröter, Hendrik and Rosenkranz, Tobias and B., Alberto N. Escalante and Aubreville, Marc and Maier, Andreas},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.10218},
	keywords = {Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Schröter et al. - 2020 - CLCNet Deep learning-based Noise Reduction for He.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YR9JSVAL\\Schröter et al. - 2020 - CLCNet Deep learning-based Noise Reduction for He.pdf:application/pdf},
}

@inproceedings{schwarz_combined_2014,
	address = {Erlangen, Germany},
	title = {Combined {Nonlinear} {Echo} {Cancellation} and {Residual} {Echo} {Suppression}},
	abstract = {We describe a combined nonlinear acoustic echo cancellation and residual echo suppression system. The echo canceler uses parallel Hammerstein branches consisting of ﬁxed nonlinear basis functions and linear adaptive ﬁlters. The residual echo suppressor uses an Artiﬁcial Neural Network for modeling of the residual echo spectrum from spectral features computed from the far-end signal. We show that modeling nonlinear effects both in the echo canceler and in the echo suppressor leads to an increased performance of the combined system.},
	language = {en},
	booktitle = {11th {ITG} {Conference} on {Speech} {Communication}},
	author = {Schwarz, Andreas and Hofmann, Christian and Kellermann, Walter},
	year = {2014},
	pages = {4},
	file = {Schwarz et al. - 2014 - Combined Nonlinear Echo Cancellation and Residual .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\QWMVGAMJ\\Schwarz et al. - 2014 - Combined Nonlinear Echo Cancellation and Residual .pdf:application/pdf},
}

@inproceedings{schwarz_spectral_2013,
	address = {New Paltz, NY, USA},
	title = {Spectral feature-based nonlinear residual echo suppression},
	isbn = {978-1-4799-0972-8},
	url = {http://ieeexplore.ieee.org/document/6701825/},
	doi = {10.1109/WASPAA.2013.6701825},
	abstract = {We propose a method for nonlinear residual echo suppression that consists of extracting spectral features from the far-end signal, and using an artiﬁcial neural network to model the residual echo magnitude spectrum from these features. We compare the modeling accuracy achieved by realizations with different features and network topologies, evaluating the mean squared error of the estimated residual echo magnitude spectrum. We also present a low complexity real-time implementation combining an ofﬂine-trained network with online adaptation, and investigate its performance in terms of echo suppression and speech distortion for real mobile phone recordings.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {2013 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics}},
	publisher = {IEEE},
	author = {Schwarz, Andreas and Hofmann, Christian and Kellermann, Walter},
	month = oct,
	year = {2013},
	pages = {1--4},
	file = {Schwarz et al. - 2013 - Spectral feature-based nonlinear residual echo sup.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5W6DC6ER\\Schwarz et al. - 2013 - Spectral feature-based nonlinear residual echo sup.pdf:application/pdf},
}

@inproceedings{lee_dnn-based_2015,
	title = {{DNN}-{Based} {Residual} {Echo} {Suppression}},
	abstract = {Due to the limitations of power ampliﬁers or loudspeakers, the echo signals captured in the microphones are not in a linear relationship with the far-end signals even when the echo path is perfectly linear. The nonlinear components of the echo cannot be successfully removed by a linear acoustic echo canceller. Residual echo suppression (RES) is a technique to suppress the remained echo after acoustic echo suppression (AES). Conventional approaches compute RES gain using Wiener ﬁlter or spectral subtraction method based on the estimated statistics on related signals. In this paper, we propose a deep neural network (DNN)-based RES gain estimation based on both the far-end and the AES output signals in all frequency bins. A DNN architecture, which is suitable to model a complicated nonlinear mapping between high-dimensional vectors, is employed as a regression function from these signals to the optimal RES gain. The proposed method can suppress the residual components without any explicit double-talk detectors. The experimental results show that our proposed approach outperforms a conventional method in terms of the echo return loss enhancement (ERLE) for single-talk periods and the perceptual evaluation of speech quality (PESQ) score for double-talk periods.},
	language = {en},
	booktitle = {Interspeech 2015},
	author = {Lee, Chul Min and Shin, Jong Won and Kim, Nam Soo},
	year = {2015},
	pages = {5},
	file = {Lee et al. - DNN-Based Residual Echo Suppression.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MT4RQRKK\\Lee et al. - DNN-Based Residual Echo Suppression.pdf:application/pdf},
}

@article{chen_nonlinear_2021,
	title = {Nonlinear residual echo suppression based on dual-stream {DPRNN}},
	volume = {2021},
	issn = {1687-4722},
	url = {https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-021-00221-8},
	doi = {10.1186/s13636-021-00221-8},
	abstract = {The acoustic echo cannot be entirely removed by linear adaptive filters due to the nonlinear relationship between the echo and the far-end signal. Usually, a post-processing module is required to further suppress the echo. In this paper, we propose a residual echo suppression method based on the modification of dual-path recurrent neural network (DPRNN) to improve the quality of speech communication. Both the residual signal and the auxiliary signal, the far-end signal or the output of the adaptive filter, obtained from the linear acoustic echo cancelation are adopted to form a dual-stream for the DPRNN. We validate the efficacy of the proposed method in the notoriously difficult double-talk situations and discuss the impact of different auxiliary signals on performance. We also compare the performance of the time domain and the time-frequency domain processing. Furthermore, we propose an efficient and applicable way to deploy our method to off-the-shelf loudspeakers by fine-tuning the pre-trained model with little recorded-echo data.},
	language = {en},
	number = {1},
	urldate = {2022-04-04},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Chen, Hongsheng and Chen, Guoliang and Chen, Kai and Lu, Jing},
	month = dec,
	year = {2021},
	pages = {35},
	file = {Chen et al. - 2021 - Nonlinear residual echo suppression based on dual-.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9KVWHVLG\\Chen et al. - 2021 - Nonlinear residual echo suppression based on dual-.pdf:application/pdf},
}

@inproceedings{peng_acoustic_2021,
	title = {Acoustic {Echo} {Cancellation} {Using} {Deep} {Complex} {Neural} {Network} with {Nonlinear} {Magnitude} {Compression} and {Phase} {Information}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/peng21f_interspeech.html},
	doi = {10.21437/Interspeech.2021-2022},
	abstract = {This paper describes a two-stage acoustic echo cancellation (AEC) and suppression framework for the INTERSPEECH2021 AEC Challenge. In the ﬁrst stage, four parallel partitioned block frequency domain adaptive ﬁlters are used to cancel the linear echo components, where the far-end signal is delayed 0ms, 320ms, 640ms and 960ms for these four adaptive ﬁlters, respectively, thus a maximum 1280 ms time delay can be well handled in the blind test dataset. The error signal with minimum energy and its corresponding reference signal are chosen as the input for the second stage, where a gate complex convolutional recurrent neural network (GCCRN) is trained to further suppress the residual echo, late reverberation and environmental noise simultaneously. To improve the performance of GCCRN, we compress both the magnitude of the error signal and that of the far-end reference signal, and then the two compressed magnitudes are combined with the phase of the error signal to regenerate the complex spectra as the input features of GCCRN. Numerous experimental results show that the proposed framework is robust to the blind test dataset, and achieves a promising result with the P.808 evaluation.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Peng, Renhua and Cheng, Linjuan and Zheng, Chengshi and Li, Xiaodong},
	month = aug,
	year = {2021},
	keywords = {AEC Challenge interspeech},
	pages = {4768--4772},
	file = {Peng et al. - 2021 - Acoustic Echo Cancellation Using Deep Complex Neur.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\5PHLD8D6\\Peng et al. - 2021 - Acoustic Echo Cancellation Using Deep Complex Neur.pdf:application/pdf},
}

@inproceedings{zhang_f-t-lstm_2021,
	title = {F-{T}-{LSTM} based {Complex} {Network} for {Joint} {Acoustic} {Echo} {Cancellation} and {Speech} {Enhancement}},
	url = {http://arxiv.org/abs/2106.07577},
	doi = {10.21437/Interspeech.2021-1359},
	abstract = {With the increasing demand for audio communication and online conference, ensuring the robustness of Acoustic Echo Cancellation (AEC) under the complicated acoustic scenario including noise, reverberation and nonlinear distortion has become a top issue. Although there have been some traditional methods that consider nonlinear distortion, they are still inefﬁcient for echo suppression and the performance will be attenuated when noise is present. In this paper, we present a real-time AEC approach using complex neural network to better modeling the important phase information and frequency-time-LSTMs (F-TLSTM), which scan both frequency and time axis, for better temporal modeling. Moreover, we utilize modiﬁed SI-SNR as cost function to make the model to have better echo cancellation and noise suppression (NS) performance. With only 1.4M parameters, the proposed approach outperforms the AECchallenge baseline by 0.27 in terms of Mean Opinion Score (MOS).},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {Interspeech 2021},
	author = {Zhang, Shimin and Kong, Yuxiang and Lv, Shubo and Hu, Yanxin and Xie, Lei},
	month = aug,
	year = {2021},
	note = {arXiv: 2106.07577},
	keywords = {AEC Challenge interspeech},
	pages = {4758--4762},
	file = {Zhang et al. - 2021 - F-T-LSTM based Complex Network for Joint Acoustic .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VH8DIFLV\\Zhang et al. - 2021 - F-T-LSTM based Complex Network for Joint Acoustic .pdf:application/pdf},
}

@article{habets_joint_2008,
	title = {Joint {Dereverberation} and {Residual} {Echo} {Suppression} of {Speech} {Signals} in {Noisy} {Environments}},
	volume = {16},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/4637896/},
	doi = {10.1109/TASL.2008.2002071},
	abstract = {Hands-free devices are often used in a noisy and reverberant environment. Therefore, the received microphone signal does not only contain the desired near-end speech signal but also interferences such as room reverberation that is caused by the near-end source, background noise and a far-end echo signal that results from the acoustic coupling between the loudspeaker and the microphone. These interferences degrade the ﬁdelity and intelligibility of near-end speech. In the last two decades, postﬁlters have been developed that can be used in conjunction with a single microphone acoustic echo canceller to enhance the near-end speech. In previous works, spectral enhancement techniques have been used to suppress residual echo and background noise for single microphone acoustic echo cancellers. However, dereverberation of the near-end speech was not addressed in this context. Recently, practically feasible spectral enhancement techniques to suppress reverberation have emerged. In this paper, we derive a novel spectral variance estimator for the late reverberation of the near-end speech. Residual echo will be present at the output of the acoustic echo canceller when the acoustic echo path cannot be completely modeled by the adaptive ﬁlter. A spectral variance estimator for the so-called late residual echo that results from the deﬁcient length of the adaptive ﬁlter is derived. Both estimators are based on a statistical reverberation model. The model parameters depend on the reverberation time of the room, which can be obtained using the estimated acoustic echo path. A novel postﬁlter is developed which suppresses late reverberation of the near-end speech, residual echo and background noise, and maintains a constant residual background noise level. Experimental results demonstrate the beneﬁcial use of the developed system for reducing reverberation, residual echo, and background noise.},
	language = {en},
	number = {8},
	urldate = {2022-04-04},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Habets, E.A.P. and Gannot, S. and Cohen, I. and Sommen, P.},
	month = nov,
	year = {2008},
	pages = {1433--1451},
	file = {Habets et al. - 2008 - Joint Dereverberation and Residual Echo Suppressio.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\LRCD2EEC\\Habets et al. - 2008 - Joint Dereverberation and Residual Echo Suppressio.pdf:application/pdf},
}

@article{pastore_spatial_2022,
	title = {Spatial {Release} from {Masking} for {Tones} and {Noises} in a {Soundfield} under {Conditions} {Where} {Targets} and {Maskers} {Are} {Stationary} or {Moving}},
	volume = {12},
	issn = {2039-4349},
	url = {https://www.mdpi.com/2039-4349/12/2/13},
	doi = {10.3390/audiolres12020013},
	abstract = {Stationary visual targets often become far more salient when they move against an otherwise static background–the so-called “pop out” effect. In two experiments conducted over loudspeakers, we tested for a similar pop-out effect in the auditory domain. Tone-in-noise and noise-in-noise detection thresholds were measured using a 2-up, 1-down adaptive procedure under conditions where target and masker(s) were presented from the same or different locations and when the target was stationary or moved via amplitude-panning. In the ﬁrst experiment, target tones of 0.5 kHz and 4 kHz were tested, maskers (2–4, depending on the condition) were independent Gaussian noises, and all stimuli were 500-ms duration. In the second experiment, a single pink noise masker (0.3–12 kHz) was presented with a single target at one of four bandwidths (0.3–0.6 kHz, 3–6 kHz, 6–12 kHz, 0.3–12 kHz) under conditions where target and masker were presented from the same or different locations and where the target moved or not. The results of both experiments failed to show a decrease in detection thresholds resulting from movement of the target.},
	language = {en},
	number = {2},
	urldate = {2022-04-04},
	journal = {Audiology Research},
	author = {Pastore, M. Torben and Yost, William A.},
	month = feb,
	year = {2022},
	pages = {99--112},
	file = {Pastore et Yost - 2022 - Spatial Release from Masking for Tones and Noises .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\JLGQCAXQ\\Pastore et Yost - 2022 - Spatial Release from Masking for Tones and Noises .pdf:application/pdf},
}

@article{andersen_creating_2021,
	title = {Creating {Clarity} in {Noisy} {Environments} by {Using} {Deep} {Learning} in {Hearing} {Aids}},
	volume = {42},
	issn = {0734-0451, 1098-8955},
	url = {http://www.thieme-connect.de/DOI/DOI?10.1055/s-0041-1735134},
	doi = {10.1055/s-0041-1735134},
	abstract = {Hearing aids continue to acquire increasingly sophisticated sound-processing features beyond basic ampliﬁcation. On the one hand, these have the potential to add user beneﬁt and allow for personalization. On the other hand, if such features are to beneﬁt according to their potential, they require clinicians to be acquainted with both the underlying technologies and the speciﬁc ﬁtting handles made available by the individual hearing aid manufacturers. Ensuring beneﬁt from hearing aids in typical daily listening environments requires that the hearing aids handle sounds that interfere with communication, generically referred to as “noise.” With this aim, considerable efforts from both academia and industry have led to increasingly advanced algorithms that handle noise, typically using the principles of directional processing and postﬁltering. This article provides an overview of the techniques used for noise reduction in modern hearing aids. First, classical techniques are covered as they are used in modern hearing aids. The discussion then shifts to how deep learning, a subﬁeld of artiﬁcial intelligence, provides a radically different way of solving the noise problem. Finally, the results of several experiments are used to showcase the beneﬁts of recent algorithmic advances in terms of signal-to-noise ratio, speech intelligibility, selective attention, and listening effort.},
	language = {en},
	number = {03},
	urldate = {2022-04-04},
	journal = {Seminars in Hearing},
	author = {Andersen, Asger Heidemann and Santurette, Sébastien and Pedersen, Michael Syskind and Alickovic, Emina and Fiedler, Lorenz and Jensen, Jesper and Behrens, Thomas},
	month = aug,
	year = {2021},
	pages = {260--281},
	file = {Andersen et al. - 2021 - Creating Clarity in Noisy Environments by Using De.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RVF7I52Z\\Andersen et al. - 2021 - Creating Clarity in Noisy Environments by Using De.pdf:application/pdf},
}

@article{erickson_maximum_1991,
	title = {Maximum {Real}-{Ear} {Gain} of {In}-the-{Ear} {Hearing} {Aids}},
	volume = {34},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jshr.3402.351},
	doi = {10.1044/jshr.3402.351},
	abstract = {Three hearing aid manufacturers provided custom full-shell in-the-ear hearing aids for each of 3 hearing-impaired subjects. Each manufacturer was instructed that the hearing aids should provide the maximum possible acoustic gain within the limits of hearing aid shell size and available components. Coupler gain, insertion gain, and functional gain were measured for each hearing aid. Gain measures were made with the volume control at either the full-on setting or the highest setting possible before the onset of acoustical feedback. Full-on coupler gain curves were similar across all nine hearing aids. Individual differences in concha/ear canal size and in the fit of the hearing aids produced substantial variance in insertion gain across hearing aids. Peak insertion gain varied from 41 to 58 dB. If 10 dB reserve gain is allowed, the range of estimated peak use gain from these maximum-gain in-the-ear hearing aids is 31-48 dB.},
	language = {en},
	number = {2},
	urldate = {2022-04-04},
	journal = {Journal of Speech, Language, and Hearing Research},
	author = {Erickson, Faye N. and Tasell, Dianne J. Van},
	month = apr,
	year = {1991},
	pages = {351--359},
	file = {Erickson et Tasell - 1991 - Maximum Real-Ear Gain of In-the-Ear Hearing Aids.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\6W4NMS5F\\Erickson et Tasell - 1991 - Maximum Real-Ear Gain of In-the-Ear Hearing Aids.pdf:application/pdf},
}

@article{heeren_relation_2013,
	title = {Relation between loudness in categorical units and loudness in phons and sones},
	volume = {133},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4795217},
	doi = {10.1121/1.4795217},
	abstract = {Data are presented on the relation between loudness measured in categorical units (CUs) using a standardized loudness scaling method (ISO 16832, 2006) and loudness expressed as the classical standardized measures phon and sone. Based on loudness scaling of narrowband noise signals by 31 normal-hearing subjects, sound pressure levels eliciting the same categorical loudness were derived for various center frequencies. The results were comparable to the standardized equal-loudness level contours. A comparison between the loudness function in CUs at 1000 Hz and the standardized loudness function in sones indicates a cubic relation between the two loudness measures.},
	language = {en},
	number = {4},
	urldate = {2022-04-04},
	journal = {The Journal of the Acoustical Society of America},
	author = {Heeren, Wiebke and Hohmann, Volker and Appell, Jens E. and Verhey, Jesko L.},
	month = apr,
	year = {2013},
	pages = {EL314--EL319},
	file = {Heeren et al. - 2013 - Relation between loudness in categorical units and.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\3SNTXGY7\\Heeren et al. - 2013 - Relation between loudness in categorical units and.pdf:application/pdf},
}

@article{stelmachowicz_importance_2004,
	title = {The {Importance} of {High}-{Frequency} {Audibility} in the {Speech} and {Language} {Development} of {Children} {With} {Hearing} {Loss}},
	volume = {130},
	issn = {0886-4470},
	url = {http://archotol.jamanetwork.com/article.aspx?doi=10.1001/archotol.130.5.556},
	doi = {10.1001/archotol.130.5.556},
	language = {en},
	number = {5},
	urldate = {2022-04-04},
	journal = {Archives of Otolaryngology–Head \& Neck Surgery},
	author = {Stelmachowicz, Patricia G. and Pittman, Andrea L. and Hoover, Brenda M. and Lewis, Dawna E. and Moeller, Mary Pat},
	month = may,
	year = {2004},
	pages = {556},
	file = {Stelmachowicz et al. - 2004 - The Importance of High-Frequency Audibility in the.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MLRGPVCV\\Stelmachowicz et al. - 2004 - The Importance of High-Frequency Audibility in the.pdf:application/pdf},
}

@article{macrae_gain_1996,
	title = {Gain, frequency response, and maximum output requirementsfor hearing aids},
	abstract = {This article applies the gain and frequency response and maximum output selection procedures currently recommended by the National Acoustic Laboratories (NAL) of Australia to the audiograms of a representative group of adult and child clients of Australian Hearing Services (AHS) to specify the performance that is required of various types of hearing aids in order to ensure that they can provide adequate gain, frequency response, and maximum output levels for at least 90\% of the AHS client population . Cumulative frequency distributions of required 2-cc coupler gain slopes were calculated for each type of aid and used to design required frequency response variations . Coupler slope requirements in different octaves were found to be independent of one another . The required range of gain-maximum output combinations was determined for each type of aid.},
	language = {en},
	author = {Macrae, John H},
	year = {1996},
	pages = {14},
	file = {Macrae - 1996 - Gain, frequency response, and maximum output requi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XFFZMPBW\\Macrae - 1996 - Gain, frequency response, and maximum output requi.pdf:application/pdf},
}

@inproceedings{pfeifenberger_acoustic_2021,
	title = {Acoustic {Echo} {Cancellation} with {Cross}-{Domain} {Learning}},
	url = {https://www.isca-speech.org/archive/interspeech_2021/pfeifenberger21_interspeech.html},
	doi = {10.21437/Interspeech.2021-85},
	abstract = {This paper proposes the Cross-Domain Echo-Controller (CDEC), submitted to the Interspeech 2021 AEC-Challenge. The algorithm consists of three building blocks: (i) a TimeDelay Compensation (TDC) module, (ii) a frequency-domain block-based Acoustic Echo Canceler (AEC), and (iii) a TimeDomain Neural-Network (TD-NN) used as a post-processor. Our system achieves an overall MOS score of 3.80, while only using 2.1 million parameters at a system latency of 32ms.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {Interspeech 2021},
	publisher = {ISCA},
	author = {Pfeifenberger, Lukas and Zoehrer, Matthias and Pernkopf, Franz},
	month = aug,
	year = {2021},
	keywords = {AEC Challenge interspeech},
	pages = {4753--4757},
	file = {Pfeifenberger et al. - 2021 - Acoustic Echo Cancellation with Cross-Domain Learn.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XVJ2625U\\Pfeifenberger et al. - 2021 - Acoustic Echo Cancellation with Cross-Domain Learn.pdf:application/pdf},
}

@article{mack_deep_2020,
	title = {Deep {Filtering}: {Signal} {Extraction} and {Reconstruction} {Using} {Complex} {Time}-{Frequency} {Filters}},
	volume = {27},
	issn = {1070-9908, 1558-2361},
	shorttitle = {Deep {Filtering}},
	url = {https://ieeexplore.ieee.org/document/8911434/},
	doi = {10.1109/LSP.2019.2955818},
	abstract = {Signal extraction from a single-channel mixture with additional undesired signals is most commonly performed using time-frequency (TF) masks. Typically, the mask is estimated with a deep neural network (DNN), and element-wise applied to the complex mixture short-time Fourier transform (STFT) representation to perform the extraction. Ideal mask magnitudes are zero for solely undesired signals in a TF bin and undeﬁned for total destructive interference. Usually, masks have an upper bound to provide well-deﬁned DNN outputs at the cost of limited extraction capabilities. We propose to estimate with a DNN a complex TF ﬁlter for each mixture TF bin which maps an STFT area in the respective mixture to the desired TF bin to address destructive interference in mixture TF bins. The DNN is optimized by minimizing the error between the extracted and the ground-truth desired signal allowing to learn the TF ﬁlters without having to specify ground-truth TF ﬁlters. We compare our approach with complex and real-valued TF masks by separating speech from a variety of different sound and noise classes from the Google AudioSet corpus. We also process the mixture STFT with notch-ﬁlters and zero whole time-frames, to simulate packet-loss during transmission, to demonstrate the reconstruction capabilities of our approach. The proposed method outperformed the baselines, especially when notch-ﬁlters and time-frame zeroing were applied.},
	language = {en},
	urldate = {2022-04-04},
	journal = {IEEE Signal Processing Letters},
	author = {Mack, Wolfgang and Habets, Emanuel A. P.},
	year = {2020},
	pages = {61--65},
	file = {Mack et Habets - 2020 - Deep Filtering Signal Extraction and Reconstructi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\A2M3VLXN\\Mack et Habets - 2020 - Deep Filtering Signal Extraction and Reconstructi.pdf:application/pdf},
}

@inproceedings{kim_attention_2020,
	title = {Attention {Wave}-{U}-{Net} for {Acoustic} {Echo} {Cancellation}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/kim20e_interspeech.html},
	doi = {10.21437/Interspeech.2020-3200},
	abstract = {In this paper, a Wave-U-Net based acoustic echo cancellation (AEC) with an attention mechanism is proposed to jointly suppress acoustic echo and background noise. The proposed approach consists of the Wave-U-Net, an auxiliary encoder, and an attention network. In the proposed approach, the Wave-U-Net yields the estimated near-end speech from the mixture, the auxiliary encoder extracts the latent features of the far-end speech, among which the relevant features are provided to the Wave-UNet by using the attention mechanism. With the attention network, the echo can be effectively suppressed from the mixture. Experimental results on TIMIT dataset show that the proposed approach outperforms the existing methods in terms of the echo return loss enhancement (ERLE) for the single-talk period and the perceptual evaluation of speech quality (PESQ) score for the double-talk period. Furthermore, the robustness of the proposed approach against unseen noise condition is also validated from the experimental results.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Kim, Jung-Hee and Chang, Joon-Hyuk},
	month = oct,
	year = {2020},
	pages = {3969--3973},
	file = {Kim et Chang - 2020 - Attention Wave-U-Net for Acoustic Echo Cancellatio.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\RV2J39FX\\Kim et Chang - 2020 - Attention Wave-U-Net for Acoustic Echo Cancellatio.pdf:application/pdf},
}

@article{mack_deep_2020-1,
	title = {Deep {Filtering}: {Signal} {Extraction} and {Reconstruction} {Using} {Complex} {Time}-{Frequency} {Filters}},
	volume = {27},
	issn = {1070-9908, 1558-2361},
	shorttitle = {Deep {Filtering}},
	url = {https://ieeexplore.ieee.org/document/8911434/},
	doi = {10.1109/LSP.2019.2955818},
	abstract = {Signal extraction from a single-channel mixture with additional undesired signals is most commonly performed using time-frequency (TF) masks. Typically, the mask is estimated with a deep neural network (DNN), and element-wise applied to the complex mixture short-time Fourier transform (STFT) representation to perform the extraction. Ideal mask magnitudes are zero for solely undesired signals in a TF bin and undeﬁned for total destructive interference. Usually, masks have an upper bound to provide well-deﬁned DNN outputs at the cost of limited extraction capabilities. We propose to estimate with a DNN a complex TF ﬁlter for each mixture TF bin which maps an STFT area in the respective mixture to the desired TF bin to address destructive interference in mixture TF bins. The DNN is optimized by minimizing the error between the extracted and the ground-truth desired signal allowing to learn the TF ﬁlters without having to specify ground-truth TF ﬁlters. We compare our approach with complex and real-valued TF masks by separating speech from a variety of different sound and noise classes from the Google AudioSet corpus. We also process the mixture STFT with notch-ﬁlters and zero whole time-frames, to simulate packet-loss during transmission, to demonstrate the reconstruction capabilities of our approach. The proposed method outperformed the baselines, especially when notch-ﬁlters and time-frame zeroing were applied.},
	language = {en},
	urldate = {2022-04-04},
	journal = {IEEE Signal Processing Letters},
	author = {Mack, Wolfgang and Habets, Emanuel A. P.},
	year = {2020},
	pages = {61--65},
	file = {Mack et Habets - 2020 - Deep Filtering Signal Extraction and Reconstructi.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\PEYFYNMJ\\Mack et Habets - 2020 - Deep Filtering Signal Extraction and Reconstructi.pdf:application/pdf},
}

@inproceedings{zhang_deep-fsmn_2018,
	address = {Calgary, AB},
	title = {Deep-{FSMN} for {Large} {Vocabulary} {Continuous} {Speech} {Recognition}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8461404/},
	doi = {10.1109/ICASSP.2018.8461404},
	abstract = {In this paper, we present an improved feedforward sequential memory networks (FSMN) architecture, namely Deep-FSMN (DFSMN), by introducing skip connections between memory blocks in adjacent layers. These skip connections enable the information ﬂow across different layers and thus alleviate the gradient vanishing problem when building very deep structure. As a result, DFSMN signiﬁcantly beneﬁts from these skip connections and deep structure. We have compared the performance of DFSMN to BLSTM both with and without lower frame rate (LFR) on several large speech recognition tasks, including English and Mandarin. Experimental results shown that DFSMN can consistently outperform BLSTM with dramatic gain, especially trained with LFR using CD-Phone as modeling units. In the 20000 hours Fisher (FSH) task, the proposed DFSMN can achieve a word error rate of 9.4\% by purely using the cross-entropy criterion and decoding with a 3-gram language model, which achieves a 1.5\% absolute improvement compared to the BLSTM. In a 20000 hours Mandarin recognition task, the LFR trained DFSMN can achieve more than 20\% relative improvement compared to the LFR trained BLSTM. Moreover, we can easily design the lookahead ﬁlter order of the memory blocks in DFSMN to control the latency for real-time applications.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Zhang, Shiliang and Lei, Ming and Yan, Zhijie and Dai, Lirong},
	month = apr,
	year = {2018},
	pages = {5869--5873},
	file = {Zhang et al. - 2018 - Deep-FSMN for Large Vocabulary Continuous Speech R.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\V5CKAMF8\\Zhang et al. - 2018 - Deep-FSMN for Large Vocabulary Continuous Speech R.pdf:application/pdf},
}

@article{park_speech_2020,
	title = {Speech {Enhancement} for {Hearing} {Aids} with {Deep} {Learning} on {Environmental} {Noises}},
	journal = {Applied Sciences},
	author = {Park, Gyuseok and Cho, Woohyeong and Kim, Kyu-Sung and Lee, Sangmin},
	year = {2020},
	file = {10.3390@app10176077.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\D42VJP9A\\10.3390@app10176077.pdf:application/pdf},
}

@article{ladune_cool-chic_nodate,
	title = {{COOL}-{CHIC}: {Coordinate}-based {Low} {Complexity} {Hierarchical} {Image} {Codec}},
	abstract = {We introduce COOL-CHIC, a Coordinate-based Low Complexity Hierarchical Image Codec. It is a learned alternative to autoencoders with 629 parameters and 680 multiplications per decoded pixel. COOL-CHIC offers compression performance close to modern conventional MPEG codecs such as HEVC and is competitive with popular autoencoder-based systems. This method is inspired by Coordinate-based Neural Representations, where an image is represented as a learned function which maps pixel coordinates to RGB values. The parameters of the mapping function are then sent using entropy coding. At the receiver side, the compressed image is obtained by evaluating the mapping function for all pixel coordinates. COOL-CHIC implementation is made open-source1.},
	language = {en},
	author = {Ladune, Theo and Philippe, Pierrick and Henry, Felix and Clare, Gordon and Leguay, Thomas},
	file = {Ladune et al. - COOL-CHIC Coordinate-based Low Complexity Hierarc.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\V38DSMRC\\Ladune et al. - COOL-CHIC Coordinate-based Low Complexity Hierarc.pdf:application/pdf},
}

@inproceedings{tancik_learned_2021,
	address = {Nashville, TN, USA},
	title = {Learned {Initializations} for {Optimizing} {Coordinate}-{Based} {Neural} {Representations}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578751/},
	doi = {10.1109/CVPR46437.2021.00287},
	abstract = {Coordinate-based neural representations have shown signiﬁcant promise as an alternative to discrete, arraybased representations for complex low dimensional signals. However, optimizing a coordinate-based network from randomly initialized weights for each new signal is inefﬁcient. We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being represented (e.g., images of faces or 3D models of chairs). Despite requiring only a minor change in implementation, using these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better generalization when only partial observations of a given signal are available. We explore these beneﬁts across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations.},
	language = {en},
	urldate = {2023-10-20},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tancik, Matthew and Mildenhall, Ben and Wang, Terrance and Schmidt, Divi and Srinivasan, Pratul P. and Barron, Jonathan T. and Ng, Ren},
	month = jun,
	year = {2021},
	pages = {2845--2854},
	file = {Tancik et al. - 2021 - Learned Initializations for Optimizing Coordinate-.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\7IALUP6K\\Tancik et al. - 2021 - Learned Initializations for Optimizing Coordinate-.pdf:application/pdf},
}

@inproceedings{shin_deep_2022,
	address = {Singapore, Singapore},
	title = {Deep {Neural} {Network} ({DNN}) {Audio} {Coder} {Using} {A} {Perceptually} {Improved} {Training} {Method}},
	isbn = {978-1-66540-540-9},
	url = {https://ieeexplore.ieee.org/document/9747575/},
	doi = {10.1109/ICASSP43922.2022.9747575},
	abstract = {A new end-to-end audio coder based on a deep neural network (DNN) is proposed. To compensate for the perceptual distortion that occurred by quantization, the proposed coder is optimized to minimize distortions in both signal and perceptual domains. The distortion in the perceptual domain is measured using the psychoacoustic model (PAM), and a loss function is obtained through the two-stage compensation approach. Also, the scalar uniform quantization was approximated using a uniform stochastic noise, together with a compression-decompression scheme, which provides simpler but more stable learning without an additional penalty than the softmax quantizer. Test results showed that the proposed coder achieves more accurate noise-masking than the previous PAM-based method and better perceptual quality then the MP3 audio coder.},
	language = {en},
	urldate = {2023-10-20},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Shin, Seungmin and Byun, Joon and Park, Youngcheol and Sung, Jongmo and Beack, Seungkwon},
	month = may,
	year = {2022},
	pages = {871--875},
	file = {Shin et al. - 2022 - Deep Neural Network (DNN) Audio Coder Using A Perc.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\CQGRBSDP\\Shin et al. - 2022 - Deep Neural Network (DNN) Audio Coder Using A Perc.pdf:application/pdf},
}

@article{gu_recent_2018,
	title = {Recent {Advances} in {Convolutional} {Neural} {Networks}},
	volume = {77},
	url = {http://arxiv.org/abs/1512.07108},
	abstract = {In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among diﬀerent types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved stateof-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on diﬀerent aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.},
	language = {en},
	urldate = {2022-12-29},
	journal = {Pattern recognition},
	author = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Li and Wang, Gang and Cai, Jianfei and Chen, Tsuhan},
	year = {2018},
	note = {arXiv:1512.07108 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	pages = {354--377},
	file = {Gu et al. - 2017 - Recent Advances in Convolutional Neural Networks.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\K95SJEUN\\Gu et al. - 2017 - Recent Advances in Convolutional Neural Networks.pdf:application/pdf},
}

@article{lin_survey_2022,
	title = {A survey of transformers},
	volume = {3},
	issn = {26666510},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666651022000146},
	doi = {10.1016/j.aiopen.2022.10.001},
	abstract = {Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.},
	language = {en},
	urldate = {2022-11-15},
	journal = {AI Open},
	author = {Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
	year = {2022},
	pages = {111--132},
	file = {Lin et al. - 2022 - A survey of transformers.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WMFSJ2CL\\Lin et al. - 2022 - A survey of transformers.pdf:application/pdf},
}

@inproceedings{martinez-ramirez_automatic_2022,
	title = {Automatic music mixing with deep learning and out-of-domain data},
	url = {http://arxiv.org/abs/2208.11428},
	language = {en},
	urldate = {2022-11-17},
	booktitle = {23rd {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR}),},
	publisher = {arXiv},
	author = {Martínez-Ramírez, Marco A. and Liao, Wei-Hsiang and Fabbro, Giorgio and Uhlich, Stefan and Nagashima, Chihiro and Mitsufuji, Yuki},
	month = aug,
	year = {2022},
	note = {arXiv:2208.11428 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	file = {Martínez-Ramírez et al. - 2022 - Automatic music mixing with deep learning and out-.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EK5I9VXT\\Martínez-Ramírez et al. - 2022 - Automatic music mixing with deep learning and out-.pdf:application/pdf},
}

@article{hargreaves_structural_2012,
	title = {Structural segmentation of multitrack audio},
	volume = {20},
	number = {10},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Hargreaves, S. and Klapuri, A. and Sandler, M.},
	year = {2012},
	pages = {2637--2647},
}

@inproceedings{de_man_ten_2017,
	address = {Salford, UK},
	title = {Ten {Years} of {Automatic} {Mixing}},
	language = {en},
	booktitle = {Proceedings of the 3rd {Workshop} on {Intelligent} {Music} {Production}},
	author = {De Man, Brecht and Reiss, Joshua D. and Stables, Ryan},
	month = sep,
	year = {2017},
	pages = {5},
	file = {Stables - 2017 - Digital Media Technology Lab Birmingham City Unive.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\W66ZPS3K\\Stables - 2017 - Digital Media Technology Lab Birmingham City Unive.pdf:application/pdf},
}

@article{reiss_applications_2018,
	title = {Applications of {Cross}-{Adaptive} {Audio} {Effects}: {Automatic} {Mixing}, {Live} {Performance} and {Everything} in {Between}},
	volume = {5},
	issn = {2297-2668},
	shorttitle = {Applications of {Cross}-{Adaptive} {Audio} {Effects}},
	url = {https://www.frontiersin.org/article/10.3389/fdigh.2018.00017/full},
	doi = {10.3389/fdigh.2018.00017},
	language = {en},
	urldate = {2022-11-17},
	journal = {Frontiers in Digital Humanities},
	author = {Reiss, Joshua D. and Brandtsegg, Øyvind},
	month = jun,
	year = {2018},
	pages = {17},
	file = {Reiss et Brandtsegg - 2018 - Applications of Cross-Adaptive Audio Effects Auto.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\BIXFJNZ8\\Reiss et Brandtsegg - 2018 - Applications of Cross-Adaptive Audio Effects Auto.pdf:application/pdf},
}

@misc{chen_speech_2022,
	title = {Speech separation with large-scale self-supervised learning},
	url = {http://arxiv.org/abs/2211.05172},
	abstract = {Self-supervised learning (SSL) methods such as WavLM have shown promising speech separation (SS) results in small-scale simulation-based experiments. In this work, we extend the exploration of the SSL-based SS by massively scaling up both the pre-training data (more than 300K hours) and ﬁne-tuning data (10K hours). We also investigate various techniques to efﬁciently integrate the pre-trained model with the SS network under a limited computation budget, including a low frame rate SSL model training setup and a ﬁne-tuning scheme using only the part of the pre-trained model. Compared with a supervised baseline and the WavLM-based SS model using feature embeddings obtained with the previously released 94K hours trained WavLM, our proposed model obtains 15.9\% and 11.2\% of relative word error rate (WER) reductions, respectively, for a simulated far-ﬁeld speech mixture test set. For conversation transcription on real meeting recordings using continuous speech separation, the proposed model achieves 6.8\% and 10.6\% of relative WER reductions over the purely supervised baseline on AMI and ICSI evaluation sets, respectively, while reducing the computational cost by 38\%.},
	language = {en},
	urldate = {2022-11-16},
	publisher = {arXiv},
	author = {Chen, Zhuo and Kanda, Naoyuki and Wu, Jian and Wu, Yu and Wang, Xiaofei and Yoshioka, Takuya and Li, Jinyu and Sivasankaran, Sunit and Eskimez, Sefik Emre},
	month = nov,
	year = {2022},
	note = {arXiv:2211.05172 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language},
	file = {Chen et al. - 2022 - Speech separation with large-scale self-supervised.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\DISRSYXV\\Chen et al. - 2022 - Speech separation with large-scale self-supervised.pdf:application/pdf},
}

@misc{lavechin_brouhaha_2022,
	title = {Brouhaha: multi-task training for voice activity detection, speech-to-noise ratio, and {C50} room acoustics estimation},
	shorttitle = {Brouhaha},
	url = {http://arxiv.org/abs/2210.13248},
	abstract = {Most automatic speech processing systems are sensitive to the acoustic environment, with degraded performance when applied to noisy or reverberant speech. But how can one tell whether speech is noisy or reverberant? We propose Brouhaha, a pipeline to simulate audio segments recorded in noisy and reverberant conditions. We then use the simulated audio to jointly train the Brouhaha model for voice activity detection, signal-to-noise ratio estimation, and C50 room acoustics prediction. We show how the predicted SNR and C50 values can be used to investigate and help diagnose errors made by automatic speech processing tools (such as pyannote.audio for speaker diarization or OpenAI’s Whisper for automatic speech recognition). Both our pipeline and a pretrained model are open source and shared with the speech community.},
	language = {en},
	urldate = {2022-11-16},
	publisher = {arXiv},
	author = {Lavechin, Marvin and Métais, Marianne and Titeux, Hadrien and Boissonnet, Alodie and Copet, Jade and Rivière, Morgane and Bergelson, Elika and Cristia, Alejandrina and Dupoux, Emmanuel and Bredin, Hervé},
	month = oct,
	year = {2022},
	note = {arXiv:2210.13248 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Lavechin et al. - 2022 - Brouhaha multi-task training for voice activity d.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\HVTRZVPB\\Lavechin et al. - 2022 - Brouhaha multi-task training for voice activity d.pdf:application/pdf},
}

@article{birtchnell_automating_2018,
	title = {Automating the black art: {Creative} places for artificial intelligence in audio mastering},
	volume = {96},
	issn = {00167185},
	shorttitle = {Automating the black art},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0016718518302392},
	doi = {10.1016/j.geoforum.2018.08.005},
	abstract = {In this paper, we consider the impact of artificial intelligence (AI) in the creative economy of music production. One sector in particular, audio post-production, is experiencing rapid change due to AI and various other forms of automation. This spells major changes, now and in the future, for skills, employment and work. Many accounts on the role of machine automation in occupational instability-specifically, reductions in human employment-have focused on the manufacturing (assembly lines) and service (financial, legal and administration) sectors: so-called blue- and white-collar jobs. However, there are as yet only limited forays into the possible consequences of AI in the creative economy, in particular on 'no-collar jobs'. Creative occupations were previously understood to be immune from the disruptions of AI due to the high levels of intuition, affective knowledge, 'gut instinct', and other human 'assets' difficult to replicate by complex algorithms and intelligent machines. Drawing on empirical research on AI in audio post-production, this article contends that there are conflicting notions of the possible impacts of these new innovations on human expertise and digital skills. The article highlights change underway in this profession of audio mastering as workers in the creative industries collaborate and compete with AI-driven technological innovation.},
	language = {en},
	urldate = {2022-11-15},
	journal = {Geoforum},
	author = {Birtchnell, Thomas and Elliott, Anthony},
	month = nov,
	year = {2018},
	pages = {77--86},
	file = {Birtchnell et Elliott - 2018 - Automating the black art Creative places for arti.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\AQIUVCUG\\Birtchnell et Elliott - 2018 - Automating the black art Creative places for arti.pdf:application/pdf},
}

@article{de_man_perceptual_2017,
	title = {Perceptual evaluation and analysis of reverberation in multitrack music production},
	volume = {65},
	number = {1},
	journal = {Journal of the Audio Engineering Society},
	author = {De Man, B. and McNally, K. and Reiss, J. D.},
	year = {2017},
	pages = {108--116},
}

@inproceedings{wichern_comparison_2015,
	title = {Comparison of loudness features for automatic level adjustment in mixing},
	volume = {139},
	booktitle = {Audio {Engineering} {Society} {Convention} 139},
	publisher = {Audio Engineering Society},
	author = {Wichern, G. and Wishnick, A. and Lukin, A. and Robertson, H.},
	year = {2015},
}

@inproceedings{ford_mixviz_2015,
	title = {Mixviz: {A} tool to visualize masking in audio mixes},
	volume = {139},
	booktitle = {Audio {Engineering} {Society} {Convention} 139},
	publisher = {Audio Engineering Society},
	author = {Ford, J. and Cartwright, M. and Pardo, B.},
	year = {2015},
}

@inproceedings{sauer_recommending_2013,
	title = {Recommending audio mixing workflows},
	booktitle = {International {Conference} on {Case}-{Based} {Reasoning}},
	publisher = {Springer},
	author = {Sauer, C. and Roth-Berghofer, T. and Auricchio, N. and Proctor, S.},
	year = {2013},
	pages = {299--313},
}

@inproceedings{bittner_medleydb_2014,
	title = {{MedleyDB}: {A} {Multitrack} {Dataset} for {Annotation}-{Intensive} {MIR} {Research}},
	abstract = {We introduce MedleyDB: a dataset of annotated, royaltyfree multitrack recordings. The dataset was primarily developed to support research on melody extraction, addressing important shortcomings of existing collections. For each song we provide melody f0 annotations as well as instrument activations for evaluating automatic instrument recognition. The dataset is also useful for research on tasks that require access to the individual tracks of a song such as source separation and automatic mixing. In this paper we provide a detailed description of MedleyDB, including curation, annotation, and musical content. To gain insight into the new challenges presented by the dataset, we run a set of experiments using a state-of-the-art melody extraction algorithm and discuss the results. The dataset is shown to be considerably more challenging than the current test sets used in the MIREX evaluation campaign, thus opening new research avenues in melody extraction research.},
	language = {en},
	booktitle = {15th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Bittner, Rachel and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan},
	year = {2014},
	pages = {6},
	file = {Bittner et al. - MedleyDB A MULTITRACK DATASET FOR ANNOTATION-INTE.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MCS26HJ3\\Bittner et al. - MedleyDB A MULTITRACK DATASET FOR ANNOTATION-INTE.pdf:application/pdf},
}

@inproceedings{reed_perceptual_2000,
	title = {A perceptual assistant to do sound equalization},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	author = {Reed, D.},
	year = {2000},
	pages = {212--218},
}

@inproceedings{jillings_automatic_2017,
	title = {Automatic masking reduction in balance mixes using evolutionary computing},
	booktitle = {Audio {Engineering} {Society} {Convention} 143},
	publisher = {Audio Engineering Society},
	author = {Jillings, N. and Stables, R.},
	year = {2017},
}

@book{miranda_handbook_2021,
	address = {Cham},
	edition = {Springer},
	title = {Handbook of {Artificial} {Intelligence} for {Music}: {Foundations}, {Advanced} {Approaches}, and {Developments} for {Creativity}},
	isbn = {978-3-030-72115-2 978-3-030-72116-9},
	shorttitle = {Handbook of {Artificial} {Intelligence} for {Music}},
	url = {https://link.springer.com/10.1007/978-3-030-72116-9},
	language = {en},
	urldate = {2022-11-15},
	publisher = {Springer International Publishing},
	editor = {Miranda, Eduardo Reck},
	year = {2021},
	doi = {10.1007/978-3-030-72116-9},
	file = {Miranda - 2021 - Handbook of Artificial Intelligence for Music Fou.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\YCR8U6EL\\Miranda - 2021 - Handbook of Artificial Intelligence for Music Fou.pdf:application/pdf},
}

@book{owsinski_mixing_2013,
	title = {The mixing engineer’s handbook},
	publisher = {Nelson Education},
	author = {Owsinski, B.},
	year = {2013},
}

@inproceedings{schmidt_interactive_2003,
	title = {Interactive {Mixing} of {Game} {Audio}},
	volume = {115},
	booktitle = {Audio {Engineering} {Society} {Convention} 115},
	publisher = {Audio Engineering Society},
	author = {Schmidt, Brian},
	month = oct,
	year = {2003},
}

@article{reiss_applications_2018-1,
	title = {Applications of {Cross}-{Adaptive} {Audio} {Effects}: {Automatic} {Mixing}, {Live} {Performance} and {Everything} in {Between}},
	volume = {5},
	issn = {2297-2668},
	shorttitle = {Applications of {Cross}-{Adaptive} {Audio} {Effects}},
	url = {https://www.frontiersin.org/article/10.3389/fdigh.2018.00017/full},
	doi = {10.3389/fdigh.2018.00017},
	language = {en},
	urldate = {2022-11-15},
	journal = {Frontiers in Digital Humanities},
	author = {Reiss, Joshua D. and Brandtsegg, Øyvind},
	month = jun,
	year = {2018},
	pages = {17},
	file = {Reiss et Brandtsegg - 2018 - Applications of Cross-Adaptive Audio Effects Auto.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\XS2ZQUGT\\Reiss et Brandtsegg - 2018 - Applications of Cross-Adaptive Audio Effects Auto.pdf:application/pdf},
}

@article{perez_film_2018,
	title = {{FiLM}: {Visual} {Reasoning} with a {General} {Conditioning} {Layer}},
	volume = {32},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{FiLM}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11671},
	doi = {10.1609/aaai.v32i1.11671},
	abstract = {We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers inﬂuence neural network computation via a simple, feature-wise afﬁne transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning — answering image-related questions which require a multi-step, high-level process — a task which has proven difﬁcult for standard deep learning methods that do not explicitly model reasoning. Speciﬁcally, we show on visual reasoning tasks that FiLM layers 1) halve state-of-theart error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modiﬁcations, and 4) generalize well to challenging, new data from few examples or even zero-shot.},
	language = {en},
	number = {1},
	urldate = {2022-11-15},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
	month = apr,
	year = {2018},
	file = {Perez et al. - 2018 - FiLM Visual Reasoning with a General Conditioning.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\TE3NM5XJ\\Perez et al. - 2018 - FiLM Visual Reasoning with a General Conditioning.pdf:application/pdf},
}

@misc{chen_automatic_2022,
	title = {Automatic {DJ} {Transitions} with {Differentiable} {Audio} {Effects} and {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/2110.06525},
	abstract = {A central task of a Disc Jockey (DJ) is to create a mixset of music with seamless transitions between adjacent tracks. In this paper, we explore a data-driven approach that uses a generative adversarial network to create the song transition by learning from real-world DJ mixes. The generator uses two differentiable digital signal processing components, an equalizer (EQ) and a fader, to mix two tracks selected by a data generation pipeline. The generator has to set the parameters of the EQs and fader in such a way that the resulting mix resembles real mixes created by human DJ, as judged by the discriminator counterpart. Result of a listening test shows that the model can achieve competitive results compared with a number of baselines.},
	language = {en},
	urldate = {2022-11-15},
	publisher = {arXiv},
	author = {Chen, Bo-Yu and Hsu, Wei-Han and Liao, Wei-Hsiang and Ramírez, Marco A. Martínez and Mitsufuji, Yuki and Yang, Yi-Hsuan},
	month = feb,
	year = {2022},
	note = {arXiv:2110.06525 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Chen et al. - 2022 - Automatic DJ Transitions with Differentiable Audio.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\EFUPBR77\\Chen et al. - 2022 - Automatic DJ Transitions with Differentiable Audio.pdf:application/pdf},
}

@misc{steinmetz_automatic_2020,
	title = {Automatic multitrack mixing with a differentiable mixing console of neural audio effects},
	url = {http://arxiv.org/abs/2010.10291},
	abstract = {Applications of deep learning to automatic multitrack mixing are largely unexplored. This is partly due to the limited available data, coupled with the fact that such data is relatively unstructured and variable. To address these challenges, we propose a domain-inspired model with a strong inductive bias for the mixing task. We achieve this with the application of pre-trained sub-networks and weight sharing, as well as with a sum/difference stereo loss function. The proposed model can be trained with a limited number of examples, is permutation invariant with respect to the input ordering, and places no limit on the number of input sources. Furthermore, it produces human-readable mixing parameters, allowing users to manually adjust or reﬁne the generated mix. Results from a perceptual evaluation involving audio engineers indicate that our approach generates mixes that outperform baseline approaches. To the best of our knowledge, this work demonstrates the ﬁrst approach in learning multitrack mixing conventions from real-world data at the waveform level, without knowledge of the underlying mixing parameters.},
	language = {en},
	urldate = {2022-11-15},
	publisher = {arXiv},
	author = {Steinmetz, Christian J. and Pons, Jordi and Pascual, Santiago and Serrà, Joan},
	month = oct,
	year = {2020},
	note = {arXiv:2010.10291 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Steinmetz et al. - 2020 - Automatic multitrack mixing with a differentiable .pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\46NGEVD6\\Steinmetz et al. - 2020 - Automatic multitrack mixing with a differentiable .pdf:application/pdf},
}

@article{gutierrez-parera_interaural_2022,
	title = {Interaural time difference individualization in {HRTF} by scaling through anthropometric parameters},
	volume = {2022},
	issn = {1687-4722},
	url = {https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-022-00241-y},
	doi = {10.1186/s13636-022-00241-y},
	abstract = {Head-related transfer function (HRTF) individualization can improve the perception of binaural sound. The interaural time difference (ITD) of the HRTF is a relevant cue for sound localization, especially in azimuth. Therefore, individualization of the ITD is likely to result in better sound spatial localization. A study of ITD has been conducted from a perceptual point of view using data from individual HRTF measurements and subjective perceptual tests. Two anthropometric dimensions have been demonstrated in relation to the ITD, predicting the subjective behavior of various subjects in a perceptual test. With this information, a method is proposed to individualize the ITD of a generic HRTF set by adapting it with a scale factor, which is obtained by a linear regression formula dependent on the two previous anthropometric dimensions. The method has been validated with both objective measures and another perceptual test. In addition, practical regression formula coefficients are provided for fitting the ITD of the generic HRTFs of the widely used Brüel \& Kjær 4100 and Neumann KU100 binaural dummy heads.},
	language = {en},
	number = {1},
	urldate = {2022-05-18},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Gutierrez-Parera, Pablo and Lopez, Jose J. and Mora-Merchan, Javier M. and Larios, Diego F.},
	month = dec,
	year = {2022},
	pages = {9},
	file = {Gutierrez-Parera et al. - 2022 - Interaural time difference individualization in HR.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\WM432FAJ\\Gutierrez-Parera et al. - 2022 - Interaural time difference individualization in HR.pdf:application/pdf},
}

@article{koszewski_automatic_2023,
	title = {Automatic music signal mixing system based on one-dimensional {Wave}-{U}-{Net} autoencoders},
	volume = {2023},
	issn = {1687-4722},
	url = {https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-022-00266-3},
	doi = {10.1186/s13636-022-00266-3},
	abstract = {The purpose of this paper is to show a music mixing system that is capable of automatically mixing separate raw recordings with good quality regardless of the music genre. This work recalls selected methods for automatic audio mixing first. Then, a novel deep model based on one-dimensional Wave-U-Net autoencoders is proposed for automatic music mixing. The model is trained on a custom-prepared database. Mixes created using the proposed system are compared with amateur, state-of-the-art software, and professional mixes prepared by audio engineers. The results obtained prove that mixes created automatically by Wave-U-Net can objectively be evaluated as highly as mixes prepared professionally. This is also confirmed by the statistical analysis of the results of the conducted listening tests. Moreover, the results show a strong correlation between the experience of the listeners in mixing and the likelihood of a higher rating of the Wave-U-Net-based and professional mixes than the amateur ones or the mix prepared using state-of-the-art software. These results are also confirmed by the outcome of the similarity matrix-based analysis.},
	language = {en},
	number = {1},
	urldate = {2023-11-08},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Koszewski, Damian and Görne, Thomas and Korvel, Grazina and Kostek, Bozena},
	month = jan,
	year = {2023},
	pages = {1},
	file = {Koszewski et al. - 2023 - Automatic music signal mixing system based on one-.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\MSD668QM\\Koszewski et al. - 2023 - Automatic music signal mixing system based on one-.pdf:application/pdf},
}

@inproceedings{lee_blind_2023,
	address = {Rhodes Island, Greece},
	title = {Blind {Estimation} of {Audio} {Processing} {Graph}},
	isbn = {978-1-72816-327-7},
	url = {https://ieeexplore.ieee.org/document/10096581/},
	doi = {10.1109/ICASSP49357.2023.10096581},
	abstract = {Musicians and audio engineers sculpt and transform their sounds by connecting multiple processors, forming an audio processing graph. However, most deep-learning methods overlook this real-world practice and assume fixed graph settings. To bridge this gap, we develop a system that reconstructs the entire graph from a given reference audio. We first generate a realistic graph-reference pair dataset and train a simple blind estimation system composed of a convolutional reference encoder and a transformer-based graph decoder. We apply our model to singing voice effects and drum mixing estimation tasks. Evaluation results show that our method can reconstruct complex signal routings, including multi-band processing and sidechaining.},
	language = {en},
	urldate = {2023-11-08},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Lee, Sungho and Park, Jaehyun and Paik, Seungryeol and Lee, Kyogu},
	month = jun,
	year = {2023},
	pages = {1--5},
	file = {Lee et al. - 2023 - Blind Estimation of Audio Processing Graph.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\FJRZ2DDU\\Lee et al. - 2023 - Blind Estimation of Audio Processing Graph.pdf:application/pdf},
}

@misc{vanka_adoption_2023,
	title = {Adoption of {AI} {Technology} in the {Music} {Mixing} {Workflow}: {An} {Investigation}},
	shorttitle = {Adoption of {AI} {Technology} in the {Music} {Mixing} {Workflow}},
	url = {http://arxiv.org/abs/2304.03407},
	abstract = {The integration of artiﬁcial intelligence (AI) technology in the music industry is driving a signiﬁcant change in the way music is being composed, produced and mixed. This study investigates the current state of AI in the mixing workﬂows and its adoption by different user groups. Through semi-structured interviews, a questionnairebased study, and analyzing web forums, the study conﬁrms three user groups comprising amateurs, pro-ams, and professionals. Our ﬁndings show that while AI mixing tools can simplify the process and provide decent results for amateurs, pro-ams seek precise control and customization options, while professionals desire control and customization options in addition to assistive and collaborative technologies. The study provides strategies for designing effective AI mixing tools for different user groups and outlines future directions.},
	language = {en},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Vanka, Soumya Sai and Safi, Maryam and Rolland, Jean-Baptiste and Fazekas, George},
	month = sep,
	year = {2023},
	note = {arXiv:2304.03407 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {Vanka et al. - 2023 - Adoption of AI Technology in the Music Mixing Work.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\VQG2CYX6\\Vanka et al. - 2023 - Adoption of AI Technology in the Music Mixing Work.pdf:application/pdf},
}

@misc{duguay_collaborative_2023,
	title = {Collaborative {Song} {Dataset} ({CoSoD}): {An} annotated dataset of multi-artist collaborations in popular music},
	shorttitle = {Collaborative {Song} {Dataset} ({CoSoD})},
	url = {http://arxiv.org/abs/2307.05588},
	abstract = {The Collaborative Song Dataset (CoSoD) is a corpus of 331 multi-artist collaborations from the 2010–2019 Billboard “Hot 100” year-end charts. The corpus is annotated with formal sections, aspects of vocal production (including reverberation, layering, panning, and gender of the performers), and relevant metadata. CoSoD complements other popular music datasets by focusing exclusively on musical collaborations between independent acts. In addition to facilitating the study of song form and vocal production, CoSoD allows for the in-depth study of gender as it relates to various timbral, pitch, and formal parameters in musical collaborations. In this paper, we detail the contents of the dataset and outline the annotation process. We also present an experiment using CoSoD that examines how the use of reverberation, layering, and panning are related to the gender of the artist. In this experiment, we find that men’s voices are on average treated with less reverberation and occupy a more narrow position in the stereo mix than women’s voices.},
	language = {en},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Duguay, Michèle and Mancey, Kate and Devaney, Johanna},
	month = jul,
	year = {2023},
	note = {arXiv:2307.05588 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Duguay et al. - 2023 - Collaborative Song Dataset (CoSoD) An annotated d.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\9QWFQ2ZM\\Duguay et al. - 2023 - Collaborative Song Dataset (CoSoD) An annotated d.pdf:application/pdf},
}

@inproceedings{koo_music_2023,
	address = {Rhodes Island, Greece},
	title = {Music {Mixing} {Style} {Transfer}: {A} {Contrastive} {Learning} {Approach} to {Disentangle} {Audio} {Effects}},
	isbn = {978-1-72816-327-7},
	shorttitle = {Music {Mixing} {Style} {Transfer}},
	url = {https://ieeexplore.ieee.org/document/10096458/},
	doi = {10.1109/ICASSP49357.2023.10096458},
	abstract = {We propose an end-to-end music mixing style transfer system that converts the mixing style of an input multitrack to that of a reference song. This is achieved with an encoder pre-trained with a contrastive objective to extract only audio effects related information from a reference music recording. All our models are trained in a self-supervised manner from an already-processed wet multitrack dataset with an effective data preprocessing method that alleviates the data scarcity of obtaining unprocessed dry data. We analyze the proposed encoder for the disentanglement capability of audio effects and also validate its performance for mixing style transfer through both objective and subjective evaluations. From the results, we show the proposed system not only converts the mixing style of multitrack audio close to a reference but is also robust with mixture-wise style transfer upon using a music source separation model.},
	language = {en},
	urldate = {2023-11-08},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Koo, Junghyun and Martínez-Ramírez, Marco A. and Liao, Wei-Hsiang and Uhlich, Stefan and Lee, Kyogu and Mitsufuji, Yuki},
	month = jun,
	year = {2023},
	pages = {1--5},
	file = {Koo et al. - 2023 - Music Mixing Style Transfer A Contrastive Learnin.pdf:C\:\\Users\\Adrien\\Documents\\Zotero\\storage\\V5SCHYVJ\\Koo et al. - 2023 - Music Mixing Style Transfer A Contrastive Learnin.pdf:application/pdf},
}
