
@misc{steinmetz_automatic_2020,
	title = {Automatic multitrack mixing with a differentiable mixing console of neural audio effects},
	url = {http://arxiv.org/abs/2010.10291},
	abstract = {Applications of deep learning to automatic multitrack mixing are largely unexplored. This is partly due to the limited available data, coupled with the fact that such data is relatively unstructured and variable. To address these challenges, we propose a domain-inspired model with a strong inductive bias for the mixing task. We achieve this with the application of pre-trained sub-networks and weight sharing, as well as with a sum/difference stereo loss function. The proposed model can be trained with a limited number of examples, is permutation invariant with respect to the input ordering, and places no limit on the number of input sources. Furthermore, it produces human-readable mixing parameters, allowing users to manually adjust or reﬁne the generated mix. Results from a perceptual evaluation involving audio engineers indicate that our approach generates mixes that outperform baseline approaches. To the best of our knowledge, this work demonstrates the ﬁrst approach in learning multitrack mixing conventions from real-world data at the waveform level, without knowledge of the underlying mixing parameters.},
	language = {en},
	urldate = {2022-11-15},
	publisher = {arXiv},
	author = {Steinmetz, Christian J. and Pons, Jordi and Pascual, Santiago and Serrà, Joan},
	month = oct,
	year = {2020},
	note = {arXiv:2010.10291 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Steinmetz et al. - 2020 - Automatic multitrack mixing with a differentiable .pdf:C\:\\Users\\cbgd6932\\Zotero\\storage\\46NGEVD6\\Steinmetz et al. - 2020 - Automatic multitrack mixing with a differentiable .pdf:application/pdf},
}

@misc{chen_automatic_2022,
	title = {Automatic {DJ} {Transitions} with {Differentiable} {Audio} {Effects} and {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/2110.06525},
	abstract = {A central task of a Disc Jockey (DJ) is to create a mixset of music with seamless transitions between adjacent tracks. In this paper, we explore a data-driven approach that uses a generative adversarial network to create the song transition by learning from real-world DJ mixes. The generator uses two differentiable digital signal processing components, an equalizer (EQ) and a fader, to mix two tracks selected by a data generation pipeline. The generator has to set the parameters of the EQs and fader in such a way that the resulting mix resembles real mixes created by human DJ, as judged by the discriminator counterpart. Result of a listening test shows that the model can achieve competitive results compared with a number of baselines.},
	language = {en},
	urldate = {2022-11-15},
	publisher = {arXiv},
	author = {Chen, Bo-Yu and Hsu, Wei-Han and Liao, Wei-Hsiang and Ramírez, Marco A. Martínez and Mitsufuji, Yuki and Yang, Yi-Hsuan},
	month = feb,
	year = {2022},
	note = {arXiv:2110.06525 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Chen et al. - 2022 - Automatic DJ Transitions with Differentiable Audio.pdf:C\:\\Users\\cbgd6932\\Zotero\\storage\\EFUPBR77\\Chen et al. - 2022 - Automatic DJ Transitions with Differentiable Audio.pdf:application/pdf},
}

@book{miranda_handbook_2021,
	address = {Cham},
	edition = {Springer},
	title = {Handbook of {Artificial} {Intelligence} for {Music}: {Foundations}, {Advanced} {Approaches}, and {Developments} for {Creativity}},
	isbn = {978-3-030-72115-2 978-3-030-72116-9},
	shorttitle = {Handbook of {Artificial} {Intelligence} for {Music}},
	url = {https://link.springer.com/10.1007/978-3-030-72116-9},
	language = {en},
	urldate = {2022-11-15},
	publisher = {Springer International Publishing},
	editor = {Miranda, Eduardo Reck},
	year = {2021},
	doi = {10.1007/978-3-030-72116-9},
	file = {Miranda - 2021 - Handbook of Artificial Intelligence for Music Fou.pdf:C\:\\Users\\cbgd6932\\Zotero\\storage\\YCR8U6EL\\Miranda - 2021 - Handbook of Artificial Intelligence for Music Fou.pdf:application/pdf},
}

@article{perez_film_2018,
	title = {{FiLM}: {Visual} {Reasoning} with a {General} {Conditioning} {Layer}},
	volume = {32},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{FiLM}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11671},
	doi = {10.1609/aaai.v32i1.11671},
	abstract = {We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers inﬂuence neural network computation via a simple, feature-wise afﬁne transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning — answering image-related questions which require a multi-step, high-level process — a task which has proven difﬁcult for standard deep learning methods that do not explicitly model reasoning. Speciﬁcally, we show on visual reasoning tasks that FiLM layers 1) halve state-of-theart error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modiﬁcations, and 4) generalize well to challenging, new data from few examples or even zero-shot.},
	language = {en},
	number = {1},
	urldate = {2022-11-15},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
	month = apr,
	year = {2018},
	file = {Perez et al. - 2018 - FiLM Visual Reasoning with a General Conditioning.pdf:C\:\\Users\\cbgd6932\\Zotero\\storage\\TE3NM5XJ\\Perez et al. - 2018 - FiLM Visual Reasoning with a General Conditioning.pdf:application/pdf},
}

@article{reiss_applications_2018,
	title = {Applications of {Cross}-{Adaptive} {Audio} {Effects}: {Automatic} {Mixing}, {Live} {Performance} and {Everything} in {Between}},
	volume = {5},
	issn = {2297-2668},
	shorttitle = {Applications of {Cross}-{Adaptive} {Audio} {Effects}},
	url = {https://www.frontiersin.org/article/10.3389/fdigh.2018.00017/full},
	doi = {10.3389/fdigh.2018.00017},
	language = {en},
	urldate = {2022-11-15},
	journal = {Frontiers in Digital Humanities},
	author = {Reiss, Joshua D. and Brandtsegg, Oyvind},
	month = jun,
	year = {2018},
	pages = {17},
	file = {Reiss et Brandtsegg - 2018 - Applications of Cross-Adaptive Audio Effects Auto.pdf:C\:\\Users\\cbgd6932\\Zotero\\storage\\XS2ZQUGT\\Reiss et Brandtsegg - 2018 - Applications of Cross-Adaptive Audio Effects Auto.pdf:application/pdf},
}

@inproceedings{schmidt_interactive_2003,
	title = {Interactive {Mixing} of {Game} {Audio}},
	volume = {115},
	booktitle = {Audio {Engineering} {Society} {Convention} 115},
	publisher = {Audio Engineering Society},
	author = {Schmidt, Brian},
	month = oct,
	year = {2003},
}

@book{owsinski_mixing_2013,
	title = {The mixing engineer’s handbook},
	publisher = {Nelson Education},
	author = {Owsinski, B.},
	year = {2013},
}

@inproceedings{jillings_automatic_2017,
	title = {Automatic masking reduction in balance mixes using evolutionary computing},
	booktitle = {Audio {Engineering} {Society} {Convention} 143},
	publisher = {Audio Engineering Society},
	author = {Jillings, N. and Stables, R.},
	year = {2017},
}

@inproceedings{reed_perceptual_2000,
	title = {A perceptual assistant to do sound equalization},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	author = {Reed, D.},
	year = {2000},
	pages = {212--218},
}

@inproceedings{bittner_medleydb_2014,
	title = {{MedleyDB}: {A} {Multitrack} {Dataset} for {Annotation}-{Intensive} {MIR} {Research}},
	abstract = {We introduce MedleyDB: a dataset of annotated, royaltyfree multitrack recordings. The dataset was primarily developed to support research on melody extraction, addressing important shortcomings of existing collections. For each song we provide melody f0 annotations as well as instrument activations for evaluating automatic instrument recognition. The dataset is also useful for research on tasks that require access to the individual tracks of a song such as source separation and automatic mixing. In this paper we provide a detailed description of MedleyDB, including curation, annotation, and musical content. To gain insight into the new challenges presented by the dataset, we run a set of experiments using a state-of-the-art melody extraction algorithm and discuss the results. The dataset is shown to be considerably more challenging than the current test sets used in the MIREX evaluation campaign, thus opening new research avenues in melody extraction research.},
	language = {en},
	booktitle = {15th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Bittner, Rachel and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan},
	year = {2014},
	pages = {6},
	file = {Bittner et al. - MedleyDB A MULTITRACK DATASET FOR ANNOTATION-INTE.pdf:C\:\\Users\\cbgd6932\\Zotero\\storage\\MCS26HJ3\\Bittner et al. - MedleyDB A MULTITRACK DATASET FOR ANNOTATION-INTE.pdf:application/pdf},
}

@inproceedings{sauer_recommending_2013,
	title = {Recommending audio mixing workflows},
	booktitle = {International {Conference} on {Case}-{Based} {Reasoning}},
	publisher = {Springer},
	author = {Sauer, C. and Roth-Berghofer, T. and Auricchio, N. and Proctor, S.},
	year = {2013},
	pages = {299--313},
}

@inproceedings{wichern_comparison_2015,
	title = {Comparison of loudness features for automatic level adjustment in mixing},
	volume = {139},
	booktitle = {Audio {Engineering} {Society} {Convention} 139},
	publisher = {Audio Engineering Society},
	author = {Wichern, G. and Wishnick, A. and Lukin, A. and Robertson, H.},
	year = {2015},
}

@inproceedings{ford_mixviz_2015,
	title = {Mixviz: {A} tool to visualize masking in audio mixes},
	volume = {139},
	booktitle = {Audio {Engineering} {Society} {Convention} 139},
	publisher = {Audio Engineering Society},
	author = {Ford, J. and Cartwright, M. and Pardo, B.},
	year = {2015},
}

@article{de_man_perceptual_2017,
	title = {Perceptual evaluation and analysis of reverberation in multitrack music production},
	volume = {65},
	number = {1},
	journal = {Journal of the Audio Engineering Society},
	author = {De Man, B. and McNally, K. and Reiss, J. D.},
	year = {2017},
	pages = {108--116},
}

@article{birtchnell_automating_2018,
	title = {Automating the black art: {Creative} places for artificial intelligence in audio mastering},
	volume = {96},
	issn = {00167185},
	shorttitle = {Automating the black art},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0016718518302392},
	doi = {10.1016/j.geoforum.2018.08.005},
	abstract = {In this paper, we consider the impact of artificial intelligence (AI) in the creative economy of music production. One sector in particular, audio post-production, is experiencing rapid change due to AI and various other forms of automation. This spells major changes, now and in the future, for skills, employment and work. Many accounts on the role of machine automation in occupational instability-specifically, reductions in human employment-have focused on the manufacturing (assembly lines) and service (financial, legal and administration) sectors: so-called blue- and white-collar jobs. However, there are as yet only limited forays into the possible consequences of AI in the creative economy, in particular on 'no-collar jobs'. Creative occupations were previously understood to be immune from the disruptions of AI due to the high levels of intuition, affective knowledge, 'gut instinct', and other human 'assets' difficult to replicate by complex algorithms and intelligent machines. Drawing on empirical research on AI in audio post-production, this article contends that there are conflicting notions of the possible impacts of these new innovations on human expertise and digital skills. The article highlights change underway in this profession of audio mastering as workers in the creative industries collaborate and compete with AI-driven technological innovation.},
	language = {en},
	urldate = {2022-11-15},
	journal = {Geoforum},
	author = {Birtchnell, Thomas and Elliott, Anthony},
	month = nov,
	year = {2018},
	pages = {77--86},
	file = {Birtchnell et Elliott - 2018 - Automating the black art Creative places for arti.pdf:C\:\\Users\\cbgd6932\\Zotero\\storage\\AQIUVCUG\\Birtchnell et Elliott - 2018 - Automating the black art Creative places for arti.pdf:application/pdf},
}

@inproceedings{de_man_ten_2017,
	address = {Salford, UK},
	title = {Ten {Years} of {Automatic} {Mixing}},
	language = {en},
	booktitle = {Proceedings of the 3rd {Workshop} on {Intelligent} {Music} {Production}},
	author = {De Man, Brecht and Reiss, Joshua D. and Stables, Ryan},
	month = sep,
	year = {2017},
	pages = {5},
	file = {Stables - 2017 - Digital Media Technology Lab Birmingham City Unive.pdf:C\:\\Users\\cbgd6932\\Zotero\\storage\\W66ZPS3K\\Stables - 2017 - Digital Media Technology Lab Birmingham City Unive.pdf:application/pdf},
}

@article{hargreaves_structural_2012,
	title = {Structural segmentation of multitrack audio},
	volume = {20},
	number = {10},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Hargreaves, S. and Klapuri, A. and Sandler, M.},
	year = {2012},
	pages = {2637--2647},
}

@inproceedings{martinez-ramirez_automatic_2022,
	title = {Automatic music mixing with deep learning and out-of-domain data},
	url = {http://arxiv.org/abs/2208.11428},
	language = {en},
	urldate = {2022-11-17},
	booktitle = {23rd {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR}),},
	publisher = {arXiv},
	author = {Martínez-Ramírez, Marco A. and Liao, Wei-Hsiang and Fabbro, Giorgio and Uhlich, Stefan and Nagashima, Chihiro and Mitsufuji, Yuki},
	month = aug,
	year = {2022},
	note = {arXiv:2208.11428 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing, Computer Science - Machine Learning},
	file = {Martínez-Ramírez et al. - 2022 - Automatic music mixing with deep learning and out-.pdf:C\:\\Users\\cbgd6932\\Zotero\\storage\\EK5I9VXT\\Martínez-Ramírez et al. - 2022 - Automatic music mixing with deep learning and out-.pdf:application/pdf},
}

@article{lin_survey_2022,
	title = {A survey of transformers},
	volume = {3},
	issn = {26666510},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666651022000146},
	doi = {10.1016/j.aiopen.2022.10.001},
	abstract = {Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.},
	language = {en},
	urldate = {2022-11-15},
	journal = {AI Open},
	author = {Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
	year = {2022},
	pages = {111--132},
	file = {Lin et al. - 2022 - A survey of transformers.pdf:C\:\\Users\\cbgd6932\\Zotero\\storage\\WMFSJ2CL\\Lin et al. - 2022 - A survey of transformers.pdf:application/pdf},
}

@article{gu_recent_2018,
	title = {Recent {Advances} in {Convolutional} {Neural} {Networks}},
	volume = {77},
	url = {http://arxiv.org/abs/1512.07108},
	abstract = {In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among diﬀerent types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved stateof-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on diﬀerent aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.},
	language = {en},
	urldate = {2022-12-29},
	journal = {Pattern recognition},
	author = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Li and Wang, Gang and Cai, Jianfei and Chen, Tsuhan},
	year = {2018},
	note = {arXiv:1512.07108 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {354--377},
	file = {Gu et al. - 2017 - Recent Advances in Convolutional Neural Networks.pdf:C\:\\Users\\cbgd6932\\Zotero\\storage\\K95SJEUN\\Gu et al. - 2017 - Recent Advances in Convolutional Neural Networks.pdf:application/pdf},
}
